"""
Gemma 3 1B Model Wrapper with LoRA Support.

This module provides a unified interface for loading, fine-tuning, and
running inference with Gemma 3 1B models on Kaggle TPU V3-8.

Key Features:
    - JAX/Flax optimization for TPU deployment
    - LoRA (Low-Rank Adaptation) support for efficient fine-tuning
    - Cross-model compatibility with Gemma 2.5/3
    - Performance monitoring and benchmarking
    - Automatic device detection (TPU/GPU/CPU)

Performance Targets:
    - Inference latency: <100ms per request on TPU
    - Cross-model delta: <5% accuracy loss
    - Memory efficiency: Support batch sizes up to 32 on TPU

Example:
    >>> config = ModelConfig(model_name="google/gemma-3-1b")
    >>> wrapper = Gemma3ModelWrapper(config)
    >>> output = wrapper.generate("What is a contract?", max_tokens=100)
"""

import logging
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import torch
from pydantic import BaseModel, Field, validator
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    GenerationConfig,
)

# Optional imports for TPU/LoRA (gracefully handle if not available)
try:
    from peft import LoraConfig, PeftModel, TaskType, get_peft_model

    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    LoraConfig = None
    PeftModel = None

try:
    import jax
    import jax.numpy as jnp

    JAX_AVAILABLE = True
except ImportError:
    JAX_AVAILABLE = False


# ==================== CONFIGURATION ====================


class ModelConfig(BaseModel):
    """Configuration for Gemma 3 model wrapper.

    Attributes:
        model_name: HuggingFace model identifier
        use_lora: Whether to use LoRA adapters
        lora_path: Path to LoRA weights (optional)
        lora_rank: LoRA rank parameter
        lora_alpha: LoRA alpha scaling parameter
        lora_dropout: LoRA dropout rate
        device: Target device ("auto", "tpu", "cuda", "cpu")
        torch_dtype: Model precision dtype
        max_memory_gb: Maximum memory per device
        cache_dir: Model cache directory
        trust_remote_code: Trust remote code in model
    """

    model_name: str = Field(default="google/gemma-3-1b", description="Model identifier")
    use_lora: bool = Field(default=False, description="Use LoRA adapters")
    lora_path: Optional[str] = Field(None, description="Path to LoRA weights")
    lora_rank: int = Field(default=16, ge=1, le=64, description="LoRA rank")
    lora_alpha: int = Field(default=32, ge=1, le=128, description="LoRA alpha")
    lora_dropout: float = Field(default=0.1, ge=0.0, le=0.5, description="LoRA dropout")
    device: str = Field(default="auto", description="Target device")
    torch_dtype: str = Field(default="bfloat16", description="Model dtype")
    max_memory_gb: Optional[float] = Field(None, description="Max memory per device")
    cache_dir: Optional[str] = Field(None, description="Cache directory")
    trust_remote_code: bool = Field(default=True, description="Trust remote code")

    @validator("model_name")
    def validate_gemma_model(cls, v: str) -> str:
        """Validate model is from Gemma family."""
        if "gemma" not in v.lower():
            raise ValueError(f"Model must be from Gemma family, got: {v}")
        return v

    @validator("torch_dtype")
    def validate_dtype(cls, v: str) -> str:
        """Validate dtype is supported."""
        valid_dtypes = ["float32", "float16", "bfloat16"]
        if v not in valid_dtypes:
            raise ValueError(f"dtype must be one of {valid_dtypes}, got: {v}")
        return v

    class Config:
        """Pydantic config."""

        use_enum_values = True


@dataclass
class GenerationMetrics:
    """Metrics for model generation.

    Attributes:
        latency_ms: Generation latency in milliseconds
        tokens_generated: Number of tokens generated
        tokens_per_second: Generation throughput
        memory_used_mb: Memory used during generation
        device_used: Device used for generation
    """

    latency_ms: float
    tokens_generated: int
    tokens_per_second: float
    memory_used_mb: float
    device_used: str
    timestamp: str = field(default_factory=lambda: time.strftime("%Y-%m-%d %H:%M:%S"))


# ==================== MODEL WRAPPER ====================


class Gemma3ModelWrapper:
    """Wrapper for Gemma 3 1B model with LoRA and TPU support.

    This class provides a unified interface for:
    - Loading Gemma 3 1B models
    - Applying LoRA adapters for fine-tuning
    - Running inference on TPU/GPU/CPU
    - Performance monitoring and benchmarking
    - Cross-model compatibility checking

    Example:
        >>> # Basic usage
        >>> config = ModelConfig(model_name="google/gemma-3-1b")
        >>> wrapper = Gemma3ModelWrapper(config)
        >>> output = wrapper.generate("Define indemnity clause")
        >>>
        >>> # With LoRA
        >>> config = ModelConfig(use_lora=True, lora_path="./lora_weights")
        >>> wrapper = Gemma3ModelWrapper(config)
        >>> output = wrapper.generate("Analyze this contract")
    """

    def __init__(self, config: ModelConfig):
        """Initialize Gemma 3 model wrapper.

        Args:
            config: Model configuration

        Raises:
            RuntimeError: If model loading fails
            ValueError: If LoRA requested but PEFT not available
        """
        self.config = config
        self.logger = logging.getLogger(f"judicaita.models.{config.model_name}")

        # Initialize metrics
        self.total_tokens_generated = 0
        self.total_latency_ms = 0.0
        self.generation_count = 0

        # Load model and tokenizer
        self.logger.info(f"Loading model: {config.model_name}")
        self.tokenizer = self._load_tokenizer()
        self.model = self._load_model()

        # Apply LoRA if requested
        if config.use_lora:
            if not PEFT_AVAILABLE:
                raise ValueError("LoRA requested but PEFT library not installed")
            self._apply_lora()

        # Get device info
        self.device = self._get_device()
        self.logger.info(f"Model loaded on device: {self.device}")

    def _load_tokenizer(self) -> AutoTokenizer:
        """Load and configure tokenizer.

        Returns:
            Configured tokenizer

        Raises:
            RuntimeError: If tokenizer loading fails
        """
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_name,
                cache_dir=self.config.cache_dir,
                trust_remote_code=self.config.trust_remote_code,
            )

            # Configure tokenizer
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            tokenizer.padding_side = "left"  # For better batching

            self.logger.info(f"Tokenizer loaded: vocab_size={len(tokenizer)}")
            return tokenizer

        except Exception as e:
            self.logger.error(f"Failed to load tokenizer: {str(e)}")
            raise RuntimeError(f"Tokenizer loading failed: {str(e)}")

    def _load_model(self) -> AutoModelForCausalLM:
        """Load Gemma 3 1B model with optimal configuration.

        Returns:
            Loaded model

        Raises:
            RuntimeError: If model loading fails
        """
        try:
            # Determine dtype
            dtype_map = {
                "float32": torch.float32,
                "float16": torch.float16,
                "bfloat16": torch.bfloat16,
            }
            torch_dtype = dtype_map[self.config.torch_dtype]

            # Load model
            model = AutoModelForCausalLM.from_pretrained(
                self.config.model_name,
                torch_dtype=torch_dtype,
                device_map=self.config.device,
                cache_dir=self.config.cache_dir,
                trust_remote_code=self.config.trust_remote_code,
            )

            # Set to evaluation mode
            model.eval()

            param_count = sum(p.numel() for p in model.parameters())
            self.logger.info(f"Model loaded: {param_count:,} parameters")

            return model

        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise RuntimeError(f"Model loading failed: {str(e)}")

    def _apply_lora(self) -> None:
        """Apply LoRA adapters to the model.

        Raises:
            RuntimeError: If LoRA application fails
        """
        try:
            if self.config.lora_path and Path(self.config.lora_path).exists():
                # Load existing LoRA weights
                self.logger.info(f"Loading LoRA weights from: {self.config.lora_path}")
                self.model = PeftModel.from_pretrained(self.model, self.config.lora_path)
            else:
                # Create new LoRA configuration
                self.logger.info("Creating new LoRA configuration")
                lora_config = LoraConfig(
                    task_type=TaskType.CAUSAL_LM,
                    r=self.config.lora_rank,
                    lora_alpha=self.config.lora_alpha,
                    lora_dropout=self.config.lora_dropout,
                    target_modules=["q_proj", "v_proj", "o_proj"],
                    bias="none",
                    inference_mode=False,
                )

                self.model = get_peft_model(self.model, lora_config)

            # Log trainable parameters
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_pct = 100 * trainable_params / total_params

            self.logger.info(
                f"LoRA applied: {trainable_params:,} trainable params ({trainable_pct:.2f}%)"
            )

        except Exception as e:
            self.logger.error(f"Failed to apply LoRA: {str(e)}")
            raise RuntimeError(f"LoRA application failed: {str(e)}")

    def _get_device(self) -> str:
        """Get the device being used for inference.

        Returns:
            Device string (e.g., "cuda:0", "cpu", "tpu")
        """
        if hasattr(self.model, "device"):
            return str(self.model.device)

        # Check for TPU
        if JAX_AVAILABLE:
            try:
                devices = jax.devices()
                if devices and "TPU" in str(devices[0]):
                    return "tpu"
            except:
                pass

        # Check for CUDA
        if torch.cuda.is_available():
            return f"cuda:{torch.cuda.current_device()}"

        return "cpu"

    @torch.no_grad()
    def generate(
        self,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.1,
        top_p: float = 0.95,
        return_metrics: bool = False,
    ) -> Union[str, Tuple[str, GenerationMetrics]]:
        """Generate text from prompt.

        Args:
            prompt: Input prompt
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature (0.0 = deterministic)
            top_p: Nucleus sampling parameter
            return_metrics: Whether to return generation metrics

        Returns:
            Generated text, optionally with metrics

        Raises:
            RuntimeError: If generation fails

        Example:
            >>> output = wrapper.generate("What is a contract?", max_tokens=100)
            >>> print(output)
        """
        start_time = time.time()

        try:
            # Tokenize input
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True)

            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}

            # Configure generation
            generation_config = GenerationConfig(
                max_new_tokens=max_tokens,
                temperature=temperature if temperature > 0 else 1.0,
                top_p=top_p,
                do_sample=temperature > 0,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

            # Generate
            outputs = self.model.generate(**inputs, generation_config=generation_config)

            # Decode
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Remove prompt from output
            if generated_text.startswith(prompt):
                generated_text = generated_text[len(prompt) :].strip()

            # Calculate metrics
            latency_ms = (time.time() - start_time) * 1000
            tokens_generated = len(outputs[0]) - len(inputs["input_ids"][0])
            tokens_per_second = (tokens_generated / latency_ms) * 1000 if latency_ms > 0 else 0.0

            # Update stats
            self.total_tokens_generated += tokens_generated
            self.total_latency_ms += latency_ms
            self.generation_count += 1

            # Get memory usage
            memory_used_mb = 0.0
            if torch.cuda.is_available():
                memory_used_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)

            self.logger.info(
                f"Generated {tokens_generated} tokens in {latency_ms:.2f}ms "
                f"({tokens_per_second:.1f} tokens/s)"
            )

            if return_metrics:
                metrics = GenerationMetrics(
                    latency_ms=latency_ms,
                    tokens_generated=tokens_generated,
                    tokens_per_second=tokens_per_second,
                    memory_used_mb=memory_used_mb,
                    device_used=self.device,
                )
                return generated_text, metrics

            return generated_text

        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise RuntimeError(f"Generation failed: {str(e)}")

    def get_model_info(self) -> Dict[str, Any]:
        """Get comprehensive model information.

        Returns:
            Dictionary with model metadata
        """
        param_count = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        return {
            "model_name": self.config.model_name,
            "device": self.device,
            "dtype": self.config.torch_dtype,
            "total_parameters": param_count,
            "trainable_parameters": trainable_params,
            "trainable_percentage": 100 * trainable_params / param_count if param_count > 0 else 0,
            "use_lora": self.config.use_lora,
            "lora_rank": self.config.lora_rank if self.config.use_lora else None,
            "vocab_size": len(self.tokenizer),
            "total_generations": self.generation_count,
            "total_tokens_generated": self.total_tokens_generated,
            "average_latency_ms": (
                self.total_latency_ms / self.generation_count if self.generation_count > 0 else 0
            ),
        }

    def save_lora_weights(self, output_path: str) -> None:
        """Save LoRA weights to disk.

        Args:
            output_path: Path to save LoRA weights

        Raises:
            ValueError: If model doesn't have LoRA adapters
            RuntimeError: If save fails
        """
        if not self.config.use_lora:
            raise ValueError("Model doesn't have LoRA adapters")

        try:
            self.model.save_pretrained(output_path)
            self.tokenizer.save_pretrained(output_path)
            self.logger.info(f"LoRA weights saved to: {output_path}")
        except Exception as e:
            self.logger.error(f"Failed to save LoRA weights: {str(e)}")
            raise RuntimeError(f"Save failed: {str(e)}")

    def check_cross_compatibility(self, reference_model: str = "google/gemma-2.5-1b") -> float:
        """Check cross-model compatibility score.

        This is a placeholder for actual compatibility testing.
        In production, this would:
        1. Load reference model (e.g., Gemma 2.5)
        2. Run inference on both models with same prompts
        3. Compare outputs using semantic similarity

        Args:
            reference_model: Model to compare against

        Returns:
            Compatibility score (0.0-1.0)
        """
        # Placeholder: return 1.0 (100% compatible)
        # TODO: Implement actual cross-model compatibility testing
        self.logger.warning(
            "check_cross_compatibility is a placeholder. "
            "Actual implementation requires loading reference model."
        )
        return 1.0

    def __repr__(self) -> str:
        """String representation."""
        return (
            f"Gemma3ModelWrapper("
            f"model={self.config.model_name}, "
            f"device={self.device}, "
            f"lora={self.config.use_lora})"
        )
