
import json
import os

NOTEBOOK_PATH = "examples/notebooks/train_tunix_reasoning.ipynb"

NEW_REWARD_COMPONENTS_CODE = [
    "import re\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "\n",
    "def extract_xml_content(response: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extract content from <reasoning> and <answer> XML tags.\n",
    "\n",
    "    Args:\n",
    "        response: Model-generated response string\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (reasoning_content, answer_content)\n",
    "        Returns (None, None) if tags are malformed or missing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract reasoning\n",
    "        reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', response, re.DOTALL)\n",
    "        reasoning = reasoning_match.group(1).strip() if reasoning_match else None\n",
    "\n",
    "        # Extract answer\n",
    "        answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)\n",
    "        answer = answer_match.group(1).strip() if answer_match else None\n",
    "\n",
    "        return reasoning, answer\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error extracting XML content: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def compute_format_reward(response: str) -> float:\n",
    "    \"\"\"\n",
    "    Reward for valid XML format (10% weight).\n",
    "    \"\"\"\n",
    "    reasoning, answer = extract_xml_content(response)\n",
    "\n",
    "    # Check both tags present and have content\n",
    "    if reasoning is not None and answer is not None:\n",
    "        if len(reasoning.strip()) > 0 and len(answer.strip()) > 0:\n",
    "            return 1.0\n",
    "\n",
    "    return 0.0\n",
    "\n",
    "def compute_legal_accuracy_reward(response: str, query_context: str = \"\") -> float:\n",
    "    \"\"\"\n",
    "    Reward for using proper legal citation format (25% weight).\n",
    "    Checks for presence of standard legal citation patterns.\n",
    "    \"\"\"\n",
    "    reasoning, _ = extract_xml_content(response)\n",
    "    if not reasoning:\n",
    "        return 0.0\n",
    "        \n",
    "    # Basic legal citation patterns\n",
    "    patterns = [\n",
    "        r'\\d+\\s+U\\.S\\.C\\.',       # US Code (e.g., 17 U.S.C.)\n",
    "        r'v\\.',                   # Case names (Plaintiff v. Defendant)\n",
    "        r'¬ß',                     # Section symbol\n",
    "        r'Article\\s+[IVX]+',      # Articles\n",
    "        r'See\\s+also',            # Legal writing style\n",
    "        r'Id\\.',                  # Citation shorthand\n",
    "        r'Cir\\.',                 # Circuit courts\n",
    "        r'Cal\\.',                 # California codes (example)\n",
    "        r'Rev\\.',                 # Review\n",
    "    ]\n",
    "    \n",
    "    matches = 0\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, reasoning, re.IGNORECASE):\n",
    "            matches += 1\n",
    "            \n",
    "    # Cap at 1.0 \n",
    "    return min(1.0, max(0.2, matches * 0.5) if matches > 0 else 0.0)\n",
    "\n",
    "def compute_reasoning_coherence_reward(response: str) -> float:\n",
    "    \"\"\"\n",
    "    Reward for coherence (25% weight).\n",
    "    Penalizes repetition and rewards structure.\n",
    "    \"\"\"\n",
    "    reasoning, _ = extract_xml_content(response)\n",
    "    if not reasoning:\n",
    "        return 0.0\n",
    "        \n",
    "    # 1. Repetition penalty\n",
    "    sentences = [s.strip() for s in reasoning.split('.') if len(s.strip()) > 10]\n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "        \n",
    "    unique_sentences = set(sentences)\n",
    "    repetition_ratio = len(unique_sentences) / len(sentences)\n",
    "    \n",
    "    # 2. Structure heuristic\n",
    "    has_paragraphs = '\\n\\n' in reasoning\n",
    "    transitions = ['Therefore', 'However', 'Furthermore', 'Accordingly', 'Thus']\n",
    "    has_transitions = any(t in reasoning for t in transitions)\n",
    "    \n",
    "    # Combine \n",
    "    score = repetition_ratio * 0.7 + (0.15 if has_paragraphs else 0.0) + (0.15 if has_transitions else 0.0)\n",
    "    return min(1.0, score)\n",
    "\n",
    "def compute_reasoning_length_penalty(response: str, tokenizer, min_tokens: int = 150) -> float:\n",
    "    \"\"\"\n",
    "    Reward for reasoning length (5% weight).\n",
    "    Targeting ~150+ tokens for detailed analysis.\n",
    "    \"\"\"\n",
    "    reasoning, _ = extract_xml_content(response)\n",
    "    if not reasoning:\n",
    "        return 0.0\n",
    "        \n",
    "    # Tokenize reasoning to count tokens\n",
    "    tokens = tokenizer(reasoning, return_tensors=\"np\")[\"input_ids\"]\n",
    "    num_tokens = len(tokens[0])\n",
    "    \n",
    "    # Return 1.0 if meets threshold, otherwise proportional\n",
    "    if num_tokens >= min_tokens:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return num_tokens / min_tokens\n",
    "\n",
    "def compute_answer_correctness_reward(response: str, ground_truth: str, tokenizer) -> float:\n",
    "    \"\"\"\n",
    "    Reward based on answer correctness (35% weight).\n",
    "    \"\"\"\n",
    "    _, answer = extract_xml_content(response)\n",
    "\n",
    "    if answer is None:\n",
    "        return 0.0\n",
    "\n",
    "    # Normalize for comparison\n",
    "    answer_norm = answer.lower().strip()\n",
    "    ground_truth_norm = ground_truth.lower().strip()\n",
    "\n",
    "    # Check exact match\n",
    "    if answer_norm == ground_truth_norm:\n",
    "        return 1.0\n",
    "\n",
    "    # Tokenize both for overlap calculation\n",
    "    answer_tokens = set(tokenizer.tokenize(answer_norm))\n",
    "    truth_tokens = set(tokenizer.tokenize(ground_truth_norm))\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    if len(answer_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = len(answer_tokens & truth_tokens)\n",
    "    union = len(answer_tokens | truth_tokens)\n",
    "\n",
    "    jaccard = intersection / union if union > 0 else 0.0\n",
    "\n",
    "    return jaccard\n",
    "\n",
    "print(\"‚úÖ Reward component functions defined:\")\n",
    "print(\"   - compute_format_reward (10%)\")\n",
    "print(\"   - compute_legal_accuracy_reward (25%)\")\n",
    "print(\"   - compute_reasoning_coherence_reward (25%)\")\n",
    "print(\"   - compute_answer_correctness_reward (35%)\")\n",
    "print(\"   - compute_reasoning_length_penalty (5%)\")"
]

NEW_COMPOSITE_REWARD_CODE = [
    "from typing import List\n",
    "\n",
    "def composite_reward_function(\n",
    "    prompts: List[str],\n",
    "    completions: List[str],\n",
    "    metadata: List[Dict],\n",
    "    tokenizer\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Competition-compliant composite reward function.\n",
    "    \n",
    "    Weights:\n",
    "    - Answer Correctness: 35%\n",
    "    - Reasoning Coherence: 25%\n",
    "    - Legal Accuracy: 25%\n",
    "    - Format Compliance: 10%\n",
    "    - Length Penalty: 5%\n",
    "    \"\"\"\n",
    "    # Competition Weights\n",
    "    W_CORRECTNESS = 0.35\n",
    "    W_COHERENCE = 0.25\n",
    "    W_LEGAL = 0.25\n",
    "    W_FORMAT = 0.10\n",
    "    W_LENGTH = 0.05\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    for i, (prompt, completion, meta) in enumerate(zip(prompts, completions, metadata)):\n",
    "        # Compute each reward component\n",
    "        r_format = compute_format_reward(completion)\n",
    "        r_correctness = compute_answer_correctness_reward(completion, meta.get(\"ground_truth\", \"\"), tokenizer)\n",
    "        r_coherence = compute_reasoning_coherence_reward(completion)\n",
    "        r_legal = compute_legal_accuracy_reward(completion)\n",
    "        r_length = compute_reasoning_length_penalty(completion, tokenizer)\n",
    "        \n",
    "        # Aggregate rewards\n",
    "        total_reward = (\n",
    "            W_CORRECTNESS * r_correctness +\n",
    "            W_COHERENCE * r_coherence +\n",
    "            W_LEGAL * r_legal +\n",
    "            W_FORMAT * r_format +\n",
    "            W_LENGTH * r_length\n",
    "        )\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        \n",
    "        # Log breakdown for first few examples\n",
    "        if i < 3:\n",
    "            print(f\"\\nüìä Example {i} reward breakdown:\")\n",
    "            print(f\"   Correctness ({W_CORRECTNESS}): {r_correctness:.2f}\")\n",
    "            print(f\"   Coherence ({W_COHERENCE}): {r_coherence:.2f}\")\n",
    "            print(f\"   Legal ({W_LEGAL}): {r_legal:.2f}\")\n",
    "            print(f\"   Format ({W_FORMAT}): {r_format:.2f}\")\n",
    "            print(f\"   Length ({W_LENGTH}): {r_length:.2f}\")\n",
    "            print(f\"   TOTAL: {total_reward:.2f}\")\n",
    "            \n",
    "    return rewards\n",
    "\n",
    "\n",
    "def tunix_reward_wrapper(prompts: List[str], outputs: List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Wrapper function matching Tunix RewardFn signature.\n",
    "    \"\"\"\n",
    "    # Build metadata from training dataset\n",
    "    metadata = []\n",
    "    for prompt in prompts:\n",
    "        # Find matching ground truth from training_dataset\n",
    "        found = False\n",
    "        for example in training_dataset:\n",
    "            if example[\"prompt\"] in prompt or prompt in example[\"prompt\"]:\n",
    "                metadata.append({\"ground_truth\": example[\"ground_truth\"]})\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            metadata.append({\"ground_truth\": \"\"})\n",
    "\n",
    "    return composite_reward_function(prompts, outputs, metadata, tokenizer)\n",
    "\n",
    "# Test reward function\n",
    "print(\"üß™ Testing reward function...\")\n",
    "test_prompts = [\"Test question\"]\n",
    "test_completions = [\n",
    "    \"<reasoning>This is a detailed legal analysis with sufficient tokens to explain the reasoning behind the answer. We consider precedent, statutory law (17 U.S.C.), and policy implications. Furthermore, the court in Smith v. Jones held that detailed analysis is required.</reasoning><answer>Yes, it is enforceable.</answer>\"\n",
    "]\n",
    "test_metadata = [{\"ground_truth\": \"Yes, it is enforceable.\"}]\n",
    "\n",
    "test_rewards = composite_reward_function(test_prompts, test_completions, test_metadata, tokenizer)\n",
    "print(f\"\\n‚úÖ Reward function test complete\")\n",
    "print(f\"   Test reward: {test_rewards[0]:.2f}\")"
]

def update_notebook():
    print(f"Reading {NOTEBOOK_PATH}...")
    with open(NOTEBOOK_PATH, 'r') as f:
        nb_data = json.load(f)
        
    cells = nb_data.get('cells', [])
    updated_components = False
    updated_composite = False
    
    for cell in cells:
        if cell.get('cell_type') != 'code':
            continue
            
        source_lines = cell.get('source', [])
        source_text = "".join(source_lines)
        
        # Identification Logic
        if "def compute_format_reward" in source_text and "def compute_reasoning_length_reward" in source_text:
            print("Found Reward Components cell. Updating...")
            cell['source'] = NEW_REWARD_COMPONENTS_CODE
            updated_components = True
            
        elif "def composite_reward_function" in source_text:
            print("Found Composite Reward Function cell. Updating...")
            cell['source'] = NEW_COMPOSITE_REWARD_CODE
            updated_composite = True
            
    if updated_components and updated_composite:
        print(f"Saving updated notebook to {NOTEBOOK_PATH}...")
        with open(NOTEBOOK_PATH, 'w') as f:
            json.dump(nb_data, f, indent=2)
        print("‚úÖ Notebook updated successfully!")
    else:
        print("‚ùå Could not find one or more target cells.")
        print(f"   Components found: {updated_components}")
        print(f"   Composite found: {updated_composite}")

if __name__ == "__main__":
    update_notebook()
