{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Judicaita: GRPO Training with Google Tunix on TPU\n\n## \ud83c\udfaf Hackathon Context\n\nThis notebook demonstrates **GRPO (Group Relative Policy Optimization)** training for the Judicaita legal AI assistant using:\n- **Google Tunix** for RL training infrastructure\n- **Gemma 3-1B-IT** as the base model\n- **TPU v2-8+** for accelerated training\n- **LoRA adapters** for parameter-efficient fine-tuning\n\nThis is developed for the Kaggle hackathon to train models that generate explainable legal reasoning with structured XML-formatted outputs.\n\n## \u26a1 TPU Requirements\n\n**IMPORTANT**: This notebook requires:\n- Google Colab with TPU runtime (TPU v2-8 or higher)\n- Runtime type: TPU (not CPU or GPU)\n- To enable: Runtime \u2192 Change runtime type \u2192 Hardware accelerator: TPU\n\n## \ud83d\udccb What This Notebook Does\n\n1. **Environment Setup**: Install Tunix, JAX, and dependencies for TPU\n2. **Model Loading**: Download and initialize Gemma 3-1B-IT with LoRA\n3. **Dataset Preparation**: Format training data with XML-tagged reasoning\n4. **Reward Function**: Score outputs based on format, reasoning length, and correctness\n5. **GRPO Training**: Train with `GRPOLearner` and `RLCluster` on TPU\n6. **Export**: Package trained LoRA adapters for Kaggle submission\n\n## \ud83d\udd04 Data Flow\n\n```\nDataset \u2192 Prompts \u2192 Model Rollouts \u2192 Reward Scoring \u2192 GRPO Updates\n                                                           \u2193\n                                              LoRA Adapter Checkpoints\n```\n\n## \u26a0\ufe0f Differences from Main Codebase\n\n| Aspect | Main Codebase | This Notebook |\n|--------|---------------|---------------|\n| Format | Step-by-step format | XML `<reasoning>`/`<answer>` |\n| Framework | PyTorch | JAX/Flax |\n| Training | Custom GRPO | Tunix GRPOLearner |\n| Hardware | GPU/CPU | TPU v2-8+ |\n\n## \ud83d\udcda References\n\n- [Google Tunix Documentation](https://tunix.readthedocs.io/)\n- [Tunix GRPO Gemma Example](https://github.com/google/tunix/tree/main/examples/grpo_gemma)\n- [Gemma Model Card](https://ai.google.dev/gemma/docs)\n- [GRPO Paper](https://arxiv.org/abs/2402.03300)\n- [Judicaita Repository](https://github.com/clduab11/judicAIta)\n\n## \u26a0\ufe0f Known Limitations\n\n- **TPU Required**: Cannot run on CPU/GPU without code modifications\n- **Memory**: TPU v2-8 has ~64GB; larger models may need v3 or higher\n- **Dataset**: Assumes generic legal reasoning tasks (not LegalBench-specific)\n- **Checkpoints**: Large checkpoint files may exceed Colab storage limits\n- **API Stability**: Tunix API may change; verify imports match your version\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udce6 Step 1: Install Dependencies\n",
        "\n",
        "Install required packages for TPU training with Tunix and Gemma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install core dependencies\n# IMPORTANT: Google Colab TPU requires specific JAX/Flax versions for compatibility\n# See: https://tunix.readthedocs.io/en/latest/installation.html\n\n# Install Tunix with TPU support\n!pip install -q \"google-tunix[tpu]>=0.5.0\"\n\n# Install JAX with TPU support - use versions compatible with Colab TPU\n!pip install -q jax==0.4.35 jaxlib==0.4.35\n!pip install -q flax==0.10.2\n\n# Install ML dependencies\n!pip install -q transformers>=4.40.0\n!pip install -q huggingface_hub>=0.20.0\n!pip install -q datasets>=2.14.0\n!pip install -q sentencepiece>=0.1.99\n!pip install -q safetensors>=0.4.0\n\n# Verify installation\nprint(\"\\n\ud83d\udce6 Verifying package versions:\")\ntry:\n    import jax\n    print(f\"   JAX: {jax.__version__}\")\nexcept ImportError as e:\n    print(f\"   \u274c JAX import failed: {e}\")\n\ntry:\n    import flax\n    print(f\"   Flax: {flax.__version__}\")\nexcept ImportError as e:\n    print(f\"   \u274c Flax import failed: {e}\")\n\ntry:\n    import tunix\n    print(f\"   Tunix: {tunix.__version__ if hasattr(tunix, '__version__') else 'installed'}\")\nexcept ImportError as e:\n    print(f\"   \u26a0\ufe0f Tunix import will be available after restart: {e}\")\n\nprint(\"\\n\u2705 Dependencies installed successfully!\")\nprint(\"\u26a0\ufe0f  IMPORTANT: Runtime restart required for TPU libraries.\")\nprint(\"   Go to: Runtime \u2192 Restart runtime\")\nprint(\"   Then continue with the next cell.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \u26a0\ufe0f Runtime Restart Required\n",
        "\n",
        "**STOP HERE** and restart the runtime:\n",
        "1. Click `Runtime` \u2192 `Restart runtime` in the menu\n",
        "2. After restart, continue from the next cell\n",
        "\n",
        "This is necessary for TPU libraries to be properly loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 Step 2: Initialize TPU Runtime\n",
        "\n",
        "Set up JAX to use TPU devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.tools import colab_tpu\n",
        "\n",
        "# Initialize TPU\n",
        "print(\"Initializing TPU runtime...\")\n",
        "try:\n",
        "    colab_tpu.setup_tpu()\n",
        "    print(\"\u2705 TPU initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c TPU initialization failed: {e}\")\n",
        "    print(\"\\n\ud83d\udd27 Troubleshooting:\")\n",
        "    print(\"   1. Check runtime type: Runtime \u2192 Change runtime type \u2192 TPU\")\n",
        "    print(\"   2. Ensure TPU v2-8 or higher is available\")\n",
        "    print(\"   3. Try restarting the runtime\")\n",
        "    print(\"   4. Check Google Cloud TPU quota if using custom project\")\n",
        "    raise\n",
        "\n",
        "# Verify TPU devices\n",
        "devices = jax.devices()\n",
        "print(f\"\\n\ud83d\udcca TPU Device Information:\")\n",
        "print(f\"   Number of devices: {len(devices)}\")\n",
        "print(f\"   Device type: {devices[0].platform}\")\n",
        "print(f\"   Devices: {devices}\")\n",
        "\n",
        "if len(devices) == 0:\n",
        "    raise RuntimeError(\"No TPU devices detected! Please check your runtime configuration.\")\n",
        "\n",
        "print(\"\\n\u2705 TPU setup complete and verified!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd10 Step 3: Authenticate with Hugging Face\n",
        "\n",
        "Login to Hugging Face to download the Gemma model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login, snapshot_download\n",
        "import os\n",
        "\n",
        "# Login to Hugging Face\n",
        "# You'll be prompted to enter your HF token\n",
        "# Get your token from: https://huggingface.co/settings/tokens\n",
        "print(\"Please enter your Hugging Face token:\")\n",
        "login()\n",
        "\n",
        "print(\"\\n\u2705 Authenticated with Hugging Face!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udce5 Step 4: Download Gemma 3-1B-IT Model\n\nDownload the model files and initialize the tokenizer.\n\n**Note**: Using `gemma-3-1b-it` as it's the latest available Gemma instruction-tuned model. Update to `gemma-3-1b-it` if/when available."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from transformers import AutoTokenizer\nimport os\n\n# Download model\nMODEL_ID = \"google/gemma-3-1b-it\"  # Using gemma-3-1b-it as gemma-3-1b-it may not be available yet\nCACHE_DIR = \"./gemma_model_cache\"\n\nprint(f\"Downloading {MODEL_ID}...\")\nmodel_path = snapshot_download(\n    repo_id=MODEL_ID,\n    cache_dir=CACHE_DIR,\n    local_dir=f\"{CACHE_DIR}/gemma\",\n    local_dir_use_symlinks=False\n)\nprint(f\"\u2705 Model downloaded to: {model_path}\")\n\n# Initialize tokenizer\nprint(\"\\nInitializing tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nprint(f\"\u2705 Tokenizer initialized\")\nprint(f\"   Vocab size: {len(tokenizer)}\")\nprint(f\"   Special tokens: {tokenizer.special_tokens_map}\")\n\n# Test tokenization\ntest_text = \"What is the legal precedent for breach of contract?\"\ntokens = tokenizer(test_text, return_tensors=\"np\")\nprint(f\"\\n\ud83d\udcdd Test tokenization:\")\nprint(f\"   Input: {test_text}\")\nprint(f\"   Token count: {len(tokens['input_ids'][0])}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Step 5: Create Preprocessing Function\n",
        "\n",
        "Gemma models don't have native system role support. We'll prepend the system prompt to the first user turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_with_system_prompt(messages, system_prompt):\n",
        "    \"\"\"\n",
        "    Prepend system prompt to first user message.\n",
        "    \n",
        "    Gemma doesn't support system role natively, so we merge it with\n",
        "    the first user turn as a workaround.\n",
        "    \n",
        "    Args:\n",
        "        messages: List of message dicts with 'role' and 'content'\n",
        "        system_prompt: System instruction string\n",
        "        \n",
        "    Returns:\n",
        "        Modified messages list with system prompt prepended\n",
        "    \"\"\"\n",
        "    if not messages:\n",
        "        return messages\n",
        "    \n",
        "    processed = messages.copy()\n",
        "    \n",
        "    # Find first user message\n",
        "    for i, msg in enumerate(processed):\n",
        "        if msg.get('role') == 'user':\n",
        "            # Prepend system prompt\n",
        "            original_content = msg['content']\n",
        "            processed[i]['content'] = f\"{system_prompt}\\n\\n{original_content}\"\n",
        "            break\n",
        "    \n",
        "    return processed\n",
        "\n",
        "# Define system prompt for legal reasoning\n",
        "SYSTEM_PROMPT = \"\"\"You are a legal AI assistant. For each question, provide your analysis in this exact format:\n",
        "<reasoning>Your step-by-step legal reasoning here. Include relevant legal principles, precedents, and analysis. Aim for at least 100 tokens of detailed reasoning.</reasoning>\n",
        "<answer>Your final answer or conclusion here.</answer>\n",
        "\n",
        "Always use this XML format and ensure your reasoning is thorough and well-explained.\"\"\"\n",
        "\n",
        "# Test preprocessing\n",
        "test_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Is a non-compete clause enforceable in California?\"}\n",
        "]\n",
        "processed = preprocess_with_system_prompt(test_messages, SYSTEM_PROMPT)\n",
        "print(\"\ud83d\udcdd Test preprocessing:\")\n",
        "print(f\"Original: {test_messages[0]['content'][:50]}...\")\n",
        "print(f\"\\nProcessed length: {len(processed[0]['content'])} chars\")\n",
        "print(f\"System prompt prepended: {'<reasoning>' in processed[0]['content']}\")\n",
        "print(\"\\n\u2705 Preprocessing function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Task 2: Prepare Training Dataset\n",
        "\n",
        "Create a dataset with XML-tagged reasoning format compatible with Tunix GRPO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### JSONL Format Requirements\n",
        "\n",
        "Each training example must be a JSON object with:\n",
        "- `prompt`: The question or task\n",
        "- `ground_truth`: The correct answer for evaluation\n",
        "- `metadata` (optional): Additional info like task_id, difficulty, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def prepare_dataset_for_tunix(examples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Prepare dataset in Tunix-compatible JSONL format.\n",
        "    \n",
        "    Args:\n",
        "        examples: List of dicts with 'question' and 'answer' fields\n",
        "        \n",
        "    Returns:\n",
        "        List of dicts with 'prompt', 'ground_truth', and 'metadata'\n",
        "    \"\"\"\n",
        "    prepared = []\n",
        "    \n",
        "    for idx, ex in enumerate(examples):\n",
        "        prepared.append({\n",
        "            \"prompt\": ex.get(\"question\", ex.get(\"prompt\", \"\")),\n",
        "            \"ground_truth\": ex.get(\"answer\", ex.get(\"ground_truth\", \"\")),\n",
        "            \"metadata\": {\n",
        "                \"example_id\": idx,\n",
        "                \"original_question\": ex.get(\"question\", \"\"),\n",
        "                \"task_type\": ex.get(\"task_type\", \"general_reasoning\")\n",
        "            }\n",
        "        })\n",
        "    \n",
        "    return prepared\n",
        "\n",
        "# Create synthetic sample data (replace with real data)\n",
        "sample_examples = [\n",
        "    {\n",
        "        \"question\": \"Can an employer in California enforce a non-compete clause against a former employee?\",\n",
        "        \"answer\": \"No, non-compete clauses are generally unenforceable in California except in limited circumstances involving sale of business or dissolution of partnership.\",\n",
        "        \"task_type\": \"legal_qa\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the statute of limitations for filing a breach of contract claim?\",\n",
        "        \"answer\": \"The statute of limitations varies by jurisdiction. In many states, it is 4-6 years for written contracts and 2-3 years for oral contracts.\",\n",
        "        \"task_type\": \"legal_qa\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Under what circumstances can a contract be voided for duress?\",\n",
        "        \"answer\": \"A contract can be voided for duress when one party was forced to enter the agreement through threats, violence, or other improper pressure that overcame their free will.\",\n",
        "        \"task_type\": \"legal_qa\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is required to establish an attorney-client privilege?\",\n",
        "        \"answer\": \"Attorney-client privilege requires: (1) an attorney-client relationship, (2) confidential communication, (3) made for the purpose of seeking or providing legal advice.\",\n",
        "        \"task_type\": \"legal_qa\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# Prepare dataset\n",
        "prepared_dataset = prepare_dataset_for_tunix(sample_examples)\n",
        "\n",
        "print(f\"\u2705 Prepared {len(prepared_dataset)} training examples\")\n",
        "print(f\"\\n\ud83d\udcdd Sample example:\")\n",
        "print(json.dumps(prepared_dataset[0], indent=2))\n",
        "\n",
        "# Note: In production, load from file or HuggingFace dataset\n",
        "print(\"\\n\ud83d\udca1 To load from file:\")\n",
        "print(\"   # with open('data.jsonl', 'r') as f:\")\n",
        "print(\"   #     examples = [json.loads(line) for line in f]\")\n",
        "print(\"   #     prepared_dataset = prepare_dataset_for_tunix(examples)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt Template with XML Format\n",
        "\n",
        "Create a template that formats prompts to expect XML-tagged reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prompt_template(question: str, system_prompt: str = SYSTEM_PROMPT) -> str:\n",
        "    \"\"\"\n",
        "    Create a formatted prompt with XML output expectations.\n",
        "    \n",
        "    Args:\n",
        "        question: The legal question to answer\n",
        "        system_prompt: System instructions for format\n",
        "        \n",
        "    Returns:\n",
        "        Formatted prompt string\n",
        "    \"\"\"\n",
        "    template = f\"\"\"{system_prompt}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Response:\"\"\"\n",
        "    return template\n",
        "\n",
        "def validate_xml_format(response: str) -> bool:\n",
        "    \"\"\"\n",
        "    Validate that response contains proper XML tags.\n",
        "    \n",
        "    Args:\n",
        "        response: Model generated response\n",
        "        \n",
        "    Returns:\n",
        "        True if valid XML format, False otherwise\n",
        "    \"\"\"\n",
        "    # Check for both opening and closing tags\n",
        "    has_reasoning = '<reasoning>' in response and '</reasoning>' in response\n",
        "    has_answer = '<answer>' in response and '</answer>' in response\n",
        "    \n",
        "    return has_reasoning and has_answer\n",
        "\n",
        "# Apply template to all examples\n",
        "templated_prompts = []\n",
        "for example in prepared_dataset:\n",
        "    templated = {\n",
        "        \"prompt\": create_prompt_template(example[\"prompt\"]),\n",
        "        \"ground_truth\": example[\"ground_truth\"],\n",
        "        \"metadata\": example[\"metadata\"],\n",
        "        \"original_prompt\": example[\"prompt\"]\n",
        "    }\n",
        "    templated_prompts.append(templated)\n",
        "\n",
        "print(f\"\u2705 Created {len(templated_prompts)} templated prompts\")\n",
        "print(f\"\\n\ud83d\udcdd Sample templated prompt (first 300 chars):\")\n",
        "print(templated_prompts[0][\"prompt\"][:300])\n",
        "print(\"...\")\n",
        "\n",
        "# Test validation\n",
        "test_valid = \"<reasoning>This is reasoning</reasoning><answer>This is answer</answer>\"\n",
        "test_invalid = \"This is just text without tags\"\n",
        "print(f\"\\n\u2705 Validation test:\")\n",
        "print(f\"   Valid format: {validate_xml_format(test_valid)}\")\n",
        "print(f\"   Invalid format: {validate_xml_format(test_invalid)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization and Batching\n",
        "\n",
        "Tokenize prompts and prepare batches for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "# Set maximum prompt length\n",
        "MAX_PROMPT_LENGTH = 512  # Adjust based on your needs (512 or 1024)\n",
        "MAX_RESPONSE_LENGTH = 512\n",
        "\n",
        "def tokenize_prompts(prompts: List[str], tokenizer, max_length: int = MAX_PROMPT_LENGTH):\n",
        "    \"\"\"\n",
        "    Tokenize prompts with padding and truncation.\n",
        "    \n",
        "    Args:\n",
        "        prompts: List of prompt strings\n",
        "        tokenizer: HuggingFace tokenizer\n",
        "        max_length: Maximum token length\n",
        "        \n",
        "    Returns:\n",
        "        Dict with input_ids and attention_mask\n",
        "    \"\"\"\n",
        "    tokenized = tokenizer(\n",
        "        prompts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    return tokenized\n",
        "\n",
        "def create_training_batches(dataset: List[Dict], batch_size: int = 4):\n",
        "    \"\"\"\n",
        "    Create batches from dataset.\n",
        "    \n",
        "    Args:\n",
        "        dataset: List of training examples\n",
        "        batch_size: Number of examples per batch\n",
        "        \n",
        "    Returns:\n",
        "        List of batches, each batch is a list of examples\n",
        "    \"\"\"\n",
        "    batches = []\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        batch = dataset[i:i + batch_size]\n",
        "        batches.append(batch)\n",
        "    return batches\n",
        "\n",
        "# Tokenize all prompts\n",
        "all_prompts = [ex[\"prompt\"] for ex in templated_prompts]\n",
        "tokenized_prompts = tokenize_prompts(all_prompts, tokenizer, MAX_PROMPT_LENGTH)\n",
        "\n",
        "print(f\"\u2705 Tokenized {len(all_prompts)} prompts\")\n",
        "print(f\"   Max length: {MAX_PROMPT_LENGTH} tokens\")\n",
        "print(f\"   Shape: {tokenized_prompts['input_ids'].shape}\")\n",
        "\n",
        "# Create final dataset for training\n",
        "training_dataset = []\n",
        "for i, ex in enumerate(templated_prompts):\n",
        "    training_dataset.append({\n",
        "        \"prompt\": ex[\"prompt\"],\n",
        "        \"prompt_tokens\": tokenized_prompts['input_ids'][i],\n",
        "        \"attention_mask\": tokenized_prompts['attention_mask'][i],\n",
        "        \"ground_truth\": ex[\"ground_truth\"],\n",
        "        \"metadata\": ex[\"metadata\"]\n",
        "    })\n",
        "\n",
        "print(f\"\\n\u2705 Final training dataset: {len(training_dataset)} examples\")\n",
        "print(f\"   Each example has: {list(training_dataset[0].keys())}\")\n",
        "\n",
        "# Validate dataset format\n",
        "required_fields = [\"prompt\", \"ground_truth\", \"metadata\"]\n",
        "all_valid = all(all(field in ex for field in required_fields) for ex in training_dataset)\n",
        "print(f\"\\n\u2705 Dataset validation: {'PASSED' if all_valid else 'FAILED'}\")\n",
        "\n",
        "if not all_valid:\n",
        "    print(\"\u274c Some examples missing required fields!\")\n",
        "else:\n",
        "    print(\"   All examples have required fields: prompt, ground_truth, metadata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Task 3: Implement Custom Reward Function\n",
        "\n",
        "Create a reward function that scores:\n",
        "1. **Format**: Proper XML tags\n",
        "2. **Reasoning Length**: At least 100 tokens\n",
        "3. **Answer Correctness**: Match with ground truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "def extract_xml_content(response: str) -> Tuple[Optional[str], Optional[str]]:\n",
        "    \"\"\"\n",
        "    Extract content from <reasoning> and <answer> XML tags.\n",
        "    \n",
        "    Args:\n",
        "        response: Model-generated response string\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (reasoning_content, answer_content)\n",
        "        Returns (None, None) if tags are malformed or missing\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract reasoning\n",
        "        reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', response, re.DOTALL)\n",
        "        reasoning = reasoning_match.group(1).strip() if reasoning_match else None\n",
        "        \n",
        "        # Extract answer\n",
        "        answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)\n",
        "        answer = answer_match.group(1).strip() if answer_match else None\n",
        "        \n",
        "        return reasoning, answer\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Error extracting XML content: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Test extraction with edge cases\n",
        "test_cases = [\n",
        "    # Valid case\n",
        "    \"<reasoning>Step by step analysis here</reasoning><answer>Final answer</answer>\",\n",
        "    # Missing tags\n",
        "    \"Just plain text without tags\",\n",
        "    # Partial tags\n",
        "    \"<reasoning>Incomplete reasoning\",\n",
        "    # Nested content\n",
        "    \"<reasoning>Analysis with <term>nested</term> content</reasoning><answer>Yes</answer>\",\n",
        "    # Multi-line\n",
        "    \"\"\"<reasoning>\n",
        "Line 1 of reasoning\n",
        "Line 2 of reasoning\n",
        "</reasoning>\n",
        "<answer>Final answer</answer>\"\"\"\n",
        "]\n",
        "\n",
        "print(\"\ud83e\uddea Testing XML extraction:\")\n",
        "for i, test in enumerate(test_cases, 1):\n",
        "    reasoning, answer = extract_xml_content(test)\n",
        "    print(f\"\\nTest {i}:\")\n",
        "    print(f\"  Reasoning found: {reasoning is not None}\")\n",
        "    print(f\"  Answer found: {answer is not None}\")\n",
        "    if reasoning:\n",
        "        print(f\"  Reasoning preview: {reasoning[:50]}...\")\n",
        "    if answer:\n",
        "        print(f\"  Answer: {answer}\")\n",
        "\n",
        "print(\"\\n\u2705 XML extraction function tested with edge cases\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_format_reward(response: str) -> float:\n",
        "    \"\"\"\n",
        "    Reward for valid XML format.\n",
        "    \n",
        "    Returns:\n",
        "        1.0 if both <reasoning> and <answer> tags present and valid\n",
        "        0.0 otherwise\n",
        "    \"\"\"\n",
        "    reasoning, answer = extract_xml_content(response)\n",
        "    \n",
        "    # Check both tags present and have content\n",
        "    if reasoning is not None and answer is not None:\n",
        "        if len(reasoning.strip()) > 0 and len(answer.strip()) > 0:\n",
        "            return 1.0\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def compute_reasoning_length_reward(response: str, tokenizer, min_tokens: int = 100) -> float:\n",
        "    \"\"\"\n",
        "    Reward based on reasoning length.\n",
        "    \n",
        "    Returns:\n",
        "        1.0 if reasoning >= min_tokens\n",
        "        proportional score (tokens/min_tokens) if fewer\n",
        "        0.0 if no reasoning found\n",
        "    \"\"\"\n",
        "    reasoning, _ = extract_xml_content(response)\n",
        "    \n",
        "    if reasoning is None:\n",
        "        return 0.0\n",
        "    \n",
        "    # Tokenize reasoning to count tokens\n",
        "    tokens = tokenizer(reasoning, return_tensors=\"np\")[\"input_ids\"]\n",
        "    num_tokens = len(tokens[0])\n",
        "    \n",
        "    # Return 1.0 if meets threshold, otherwise proportional\n",
        "    if num_tokens >= min_tokens:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return num_tokens / min_tokens\n",
        "\n",
        "def compute_answer_correctness_reward(response: str, ground_truth: str, tokenizer) -> float:\n",
        "    \"\"\"\n",
        "    Reward based on answer correctness.\n",
        "    \n",
        "    Returns:\n",
        "        1.0 for exact match (after normalization)\n",
        "        Partial credit for token overlap (Jaccard similarity)\n",
        "        0.0 if no answer found\n",
        "    \"\"\"\n",
        "    _, answer = extract_xml_content(response)\n",
        "    \n",
        "    if answer is None:\n",
        "        return 0.0\n",
        "    \n",
        "    # Normalize for comparison\n",
        "    answer_norm = answer.lower().strip()\n",
        "    ground_truth_norm = ground_truth.lower().strip()\n",
        "    \n",
        "    # Check exact match\n",
        "    if answer_norm == ground_truth_norm:\n",
        "        return 1.0\n",
        "    \n",
        "    # Tokenize both for overlap calculation\n",
        "    answer_tokens = set(tokenizer.tokenize(answer_norm))\n",
        "    truth_tokens = set(tokenizer.tokenize(ground_truth_norm))\n",
        "    \n",
        "    # Calculate Jaccard similarity (intersection / union)\n",
        "    if len(answer_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    intersection = len(answer_tokens & truth_tokens)\n",
        "    union = len(answer_tokens | truth_tokens)\n",
        "    \n",
        "    jaccard = intersection / union if union > 0 else 0.0\n",
        "    \n",
        "    # Return Jaccard similarity as partial credit\n",
        "    return jaccard\n",
        "\n",
        "print(\"\u2705 Reward component functions defined:\")\n",
        "print(\"   - compute_format_reward()\")\n",
        "print(\"   - compute_reasoning_length_reward()\")\n",
        "print(\"   - compute_answer_correctness_reward()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from typing import List\n\ndef composite_reward_function(\n    prompts: List[str],\n    completions: List[str],\n    metadata: List[Dict],\n    tokenizer  # Required for token counting\n) -> List[float]:\n    \"\"\"\n    Composite reward function compatible with Tunix GRPO.\n    \n    This function scores model outputs based on:\n    - Format reward (30%): Valid XML tags (<reasoning>/<answer>)\n    - Reasoning reward (30%): Sufficient reasoning length (>=100 tokens)\n    - Correctness reward (40%): Answer matches ground truth\n    \n    Args:\n        prompts: List of input prompts\n        completions: List of model completions\n        metadata: List of metadata dicts with ground_truth\n        tokenizer: HuggingFace tokenizer for token counting\n        \n    Returns:\n        List of scalar rewards (one per example)\n    \"\"\"\n    # Reward weights\n    FORMAT_WEIGHT = 0.3\n    REASONING_WEIGHT = 0.3\n    CORRECTNESS_WEIGHT = 0.4\n    \n    rewards = []\n    \n    for i, (prompt, completion, meta) in enumerate(zip(prompts, completions, metadata)):\n        # Compute each reward component\n        format_reward = compute_format_reward(completion)\n        reasoning_reward = compute_reasoning_length_reward(completion, tokenizer, min_tokens=100)\n        \n        # Get ground truth from metadata\n        ground_truth = meta.get(\"ground_truth\", \"\")\n        correctness_reward = compute_answer_correctness_reward(completion, ground_truth, tokenizer)\n        \n        # Aggregate rewards\n        total_reward = (\n            FORMAT_WEIGHT * format_reward +\n            REASONING_WEIGHT * reasoning_reward +\n            CORRECTNESS_WEIGHT * correctness_reward\n        )\n        \n        rewards.append(total_reward)\n        \n        # Log breakdown for first few examples\n        if i < 3:\n            print(f\"\\n\ud83d\udcca Example {i} reward breakdown:\")\n            print(f\"   Format: {format_reward:.2f} (weight: {FORMAT_WEIGHT})\")\n            print(f\"   Reasoning: {reasoning_reward:.2f} (weight: {REASONING_WEIGHT})\")\n            print(f\"   Correctness: {correctness_reward:.2f} (weight: {CORRECTNESS_WEIGHT})\")\n            print(f\"   Total: {total_reward:.2f}\")\n    \n    return rewards\n\n\ndef tunix_reward_wrapper(prompts: List[str], outputs: List[str]) -> List[float]:\n    \"\"\"\n    Wrapper function matching Tunix RewardFn signature: (prompts, outputs) -> rewards.\n    \n    This wrapper adapts our composite_reward_function to Tunix's expected signature.\n    It extracts metadata from the global training_dataset for ground truth comparison.\n    \n    Args:\n        prompts: List of prompt strings\n        outputs: List of generated output strings\n        \n    Returns:\n        List of float reward values\n    \"\"\"\n    # Build metadata from training dataset (prompts contain the original questions)\n    metadata = []\n    for prompt in prompts:\n        # Find matching ground truth from training_dataset\n        found = False\n        for example in training_dataset:\n            if example[\"prompt\"] in prompt or prompt in example[\"prompt\"]:\n                metadata.append({\"ground_truth\": example[\"ground_truth\"]})\n                found = True\n                break\n        if not found:\n            metadata.append({\"ground_truth\": \"\"})\n    \n    return composite_reward_function(prompts, outputs, metadata, tokenizer)\n\n\n# Test reward function\nprint(\"\ud83e\uddea Testing reward function...\")\ntest_prompts = [\"Test question\"]\ntest_completions = [\n    \"<reasoning>This is a detailed legal analysis with sufficient tokens to explain the reasoning behind the answer. We consider precedent, statutory law, and policy implications.</reasoning><answer>Yes, it is enforceable.</answer>\"\n]\ntest_metadata = [{\"ground_truth\": \"Yes, it is enforceable.\"}]\n\ntest_rewards = composite_reward_function(test_prompts, test_completions, test_metadata, tokenizer)\nprint(f\"\\n\u2705 Reward function test complete\")\nprint(f\"   Test reward: {test_rewards[0]:.2f}\")\nprint(\"\\n\u2705 Composite reward function ready for Tunix GRPO!\")\nprint(\"\\n\ud83d\udca1 Use tunix_reward_wrapper() as the reward function for GRPOLearner\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Verify Tunix installation before training setup\nprint(\"\ud83d\udce6 Verifying Tunix installation...\")\n\nimport sys\n\n# Check Tunix availability\ntry:\n    import tunix\n    print(f\"\u2705 Tunix installed: {tunix.__version__ if hasattr(tunix, '__version__') else 'version unknown'}\")\nexcept ImportError as e:\n    print(f\"\u274c Tunix not available: {e}\")\n    print(\"\\n\ud83d\udd27 To install Tunix:\")\n    print(\"   !pip install 'google-tunix[tpu]>=0.5.0'\")\n    print(\"   Then restart runtime and run this cell again.\")\n    raise\n\n# Check required submodules\nmodules_to_check = [\n    (\"tunix.rl.grpo.grpo_learner\", \"GRPOConfig, GRPOLearner\"),\n    (\"tunix.rl.rl_cluster\", \"RLCluster\"),\n    (\"tunix.models.gemma\", \"GemmaForCausalLM\"),\n    (\"tunix.peft.lora\", \"LoRAConfig\"),\n]\n\nprint(\"\\n\ud83d\udccb Checking Tunix submodules:\")\nall_available = True\nfor module_path, expected_exports in modules_to_check:\n    try:\n        module = __import__(module_path, fromlist=[''])\n        print(f\"   \u2705 {module_path}\")\n    except ImportError as e:\n        print(f\"   \u274c {module_path}: {e}\")\n        all_available = False\n\nif all_available:\n    print(\"\\n\u2705 All Tunix modules available!\")\nelse:\n    print(\"\\n\u26a0\ufe0f Some modules not available. Check Tunix version and installation.\")\n    print(\"   The training cells may need adaptation for your Tunix version.\")\n\n# Check JAX backend\nprint(\"\\n\ud83d\udcca JAX Backend Status:\")\nimport jax\nprint(f\"   JAX version: {jax.__version__}\")\nprint(f\"   Backend: {jax.default_backend()}\")\nprint(f\"   Devices: {jax.device_count()} ({jax.devices()[0].platform if jax.devices() else 'none'})\")\n\nprint(\"\\n\u2705 Environment verified - ready for training setup!\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 Task 4: Configure and Execute GRPO Training\n",
        "\n",
        "Set up LoRA adapters and run GRPO training on TPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# LoRA Hyperparameters for parameter-efficient fine-tuning\nLORA_CONFIG = {\n    \"rank\": 16,           # LoRA rank (16 or 32 recommended)\n    \"alpha\": 32,          # LoRA alpha (typically 2x rank)\n    \"dropout\": 0.05,      # LoRA dropout for regularization\n    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n}\n\n# GRPO Configuration matching Tunix GRPOConfig parameters\n# Reference: https://tunix.readthedocs.io/en/latest/api/grpo.html\nGRPO_CONFIG = {\n    # Rollout settings\n    \"num_generations\": 4,           # Number of response samples per prompt for GRPO\n    \"max_tokens_to_generate\": 512,  # Maximum tokens for rollout generation\n    \n    # GRPO algorithm hyperparameters\n    \"beta\": 0.04,                   # KL penalty coefficient (prevents policy divergence)\n    \"epsilon\": 0.2,                 # PPO-style clipping parameter\n    \n    # Training settings\n    \"learning_rate\": 1e-5,          # Learning rate for LoRA parameters\n    \"batch_size\": 4,                # Batch size per TPU core (adjust for memory)\n    \"num_iterations\": 2,            # Number of training epochs/iterations\n    \n    # Evaluation and checkpointing\n    \"eval_every_n_steps\": 50,       # Evaluate model every N steps\n    \"checkpoint_every_n_steps\": 100, # Save checkpoint every N steps\n}\n\n# Training configuration for RLCluster\nTRAINING_CONFIG = {\n    \"warmup_steps\": 10,             # Learning rate warmup steps\n    \"weight_decay\": 0.01,           # Weight decay for regularization\n    \"max_grad_norm\": 1.0,           # Gradient clipping threshold\n    \"log_every_n_steps\": 10,        # Log metrics every N steps\n}\n\nprint(\"\u2705 Configuration defined:\")\nprint(\"\\n\ud83d\udd27 LoRA Configuration:\")\nfor k, v in LORA_CONFIG.items():\n    print(f\"   {k}: {v}\")\nprint(\"\\n\ud83c\udfaf GRPO Configuration:\")\nfor k, v in GRPO_CONFIG.items():\n    print(f\"   {k}: {v}\")\nprint(\"\\n\ud83d\udcca Training Configuration:\")\nfor k, v in TRAINING_CONFIG.items():\n    print(f\"   {k}: {v}\")\n\nprint(\"\\n\ud83d\udca1 Hyperparameter Rationale:\")\nprint(\"   - LoRA rank=16: Balance between capacity and memory efficiency\")\nprint(\"   - num_generations=4: Standard for GRPO variance reduction\")\nprint(\"   - beta=0.04: Conservative KL penalty to prevent policy divergence\")\nprint(\"   - learning_rate=1e-5: Safe starting point for LoRA fine-tuning\")\nprint(\"   - max_tokens_to_generate=512: Sufficient for detailed legal reasoning\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### \ud83d\udd27 Initialize Training Components\n\nThis section sets up the Tunix GRPO training infrastructure:\n\n1. **Import Tunix modules**: GRPOConfig, GRPOLearner, RLCluster\n2. **Load and configure models**: Actor (trainable) and Reference (frozen) policies\n3. **Setup TPU mesh**: Configure sharding for distributed training\n4. **Initialize learner**: Create GRPOLearner with reward function\n\n**Prerequisites**:\n- TPU runtime initialized (verified in Step 2)\n- Model downloaded (completed in Step 4)\n- Reward function defined (completed above)\n- Training dataset prepared (completed above)\n\n**Documentation**:\n- [Tunix GRPO Guide](https://tunix.readthedocs.io/en/latest/tutorials/grpo.html)\n- [Official GRPO Gemma Example](https://github.com/google/tunix/tree/main/examples/grpo_gemma)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Import Tunix GRPO modules\nprint(\"\ud83d\udce6 Importing Tunix modules...\")\n\ntry:\n    from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n    from tunix.rl import rl_cluster as rl_cluster_lib\n    from tunix.rl.rollout import base_rollout\n    from tunix.models import gemma as gemma_lib\n    print(\"\u2705 Tunix modules imported successfully!\")\nexcept ImportError as e:\n    print(f\"\u274c Tunix import failed: {e}\")\n    print(\"\\n\ud83d\udd27 Troubleshooting:\")\n    print(\"   1. Verify Tunix is installed: pip install google-tunix[tpu]\")\n    print(\"   2. Restart runtime after installation\")\n    print(\"   3. Check Tunix version compatibility\")\n    raise\n\nimport jax\nimport jax.numpy as jnp\nfrom jax.sharding import Mesh, NamedSharding, PartitionSpec\nimport os\n\n# Create checkpoint directories\nCHECKPOINT_DIR = \"./checkpoints\"\nFINAL_DIR = \"./final_checkpoint\"\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(FINAL_DIR, exist_ok=True)\n\nprint(f\"\u2705 Checkpoint directories created:\")\nprint(f\"   Intermediate: {CHECKPOINT_DIR}\")\nprint(f\"   Final: {FINAL_DIR}\")\n\n# Setup TPU mesh for distributed training\nprint(\"\\n\ud83d\udd27 Setting up TPU mesh...\")\ndevices = jax.devices()\nnum_devices = len(devices)\n\n# Create 1D mesh for data parallelism across TPU cores\nmesh = Mesh(devices, axis_names=(\"data\",))\nprint(f\"\u2705 TPU mesh created with {num_devices} devices\")\nprint(f\"   Mesh shape: {mesh.shape}\")\nprint(f\"   Axis names: {mesh.axis_names}\")\n\n# Load Gemma model for GRPO training\nprint(\"\\n\ud83d\udce5 Loading Gemma model for GRPO...\")\n\n# Create model configuration\nmodel_config = gemma_lib.GemmaConfig.from_pretrained(model_path)\nprint(f\"   Model config loaded: {type(model_config).__name__}\")\n\n# Initialize actor model (trainable policy with LoRA)\nprint(\"\\n\ud83c\udfad Initializing actor model (trainable)...\")\nactor_model = gemma_lib.GemmaForCausalLM.from_pretrained(\n    model_path,\n    dtype=jnp.bfloat16,  # Use bfloat16 for TPU efficiency\n)\n\n# Apply LoRA configuration to actor model\nfrom tunix.peft import lora as lora_lib\n\nlora_config = lora_lib.LoRAConfig(\n    rank=LORA_CONFIG[\"rank\"],\n    alpha=LORA_CONFIG[\"alpha\"],\n    dropout=LORA_CONFIG[\"dropout\"],\n    target_modules=LORA_CONFIG[\"target_modules\"],\n)\nactor_model = lora_lib.apply_lora(actor_model, lora_config)\nprint(f\"   LoRA applied: rank={LORA_CONFIG['rank']}, alpha={LORA_CONFIG['alpha']}\")\n\n# Initialize reference model (frozen copy for KL penalty)\nprint(\"\\n\ud83d\udccb Initializing reference model (frozen)...\")\nreference_model = gemma_lib.GemmaForCausalLM.from_pretrained(\n    model_path,\n    dtype=jnp.bfloat16,\n)\n# Reference model parameters are frozen (no gradients)\nprint(\"   Reference model loaded (frozen for KL divergence)\")\n\nprint(\"\\n\u2705 Models initialized successfully!\")\nprint(f\"   Actor model: LoRA-adapted, trainable\")\nprint(f\"   Reference model: Frozen for KL penalty calculation\")\n\n# Create RLCluster configuration\nprint(\"\\n\ud83d\udd27 Creating RLCluster...\")\n\n# Define sharding specs for model parallelism\ndata_sharding = NamedSharding(mesh, PartitionSpec(\"data\"))\n\n# Create RLCluster with actor and reference models\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor_model=actor_model,\n    reference_model=reference_model,\n    tokenizer=tokenizer,\n    mesh=mesh,\n    data_sharding=data_sharding,\n)\nprint(\"\u2705 RLCluster created successfully!\")\n\n# Create GRPO configuration\nprint(\"\\n\ud83c\udfaf Creating GRPOConfig...\")\ngrpo_config = GRPOConfig(\n    num_generations=GRPO_CONFIG[\"num_generations\"],\n    max_tokens_to_generate=GRPO_CONFIG[\"max_tokens_to_generate\"],\n    beta=GRPO_CONFIG[\"beta\"],\n    epsilon=GRPO_CONFIG[\"epsilon\"],\n    learning_rate=GRPO_CONFIG[\"learning_rate\"],\n    warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n    weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n    max_grad_norm=TRAINING_CONFIG[\"max_grad_norm\"],\n)\nprint(f\"\u2705 GRPOConfig created:\")\nprint(f\"   num_generations: {grpo_config.num_generations}\")\nprint(f\"   max_tokens_to_generate: {grpo_config.max_tokens_to_generate}\")\nprint(f\"   beta (KL penalty): {grpo_config.beta}\")\nprint(f\"   learning_rate: {grpo_config.learning_rate}\")\n\n# Initialize GRPO Learner\nprint(\"\\n\ud83c\udf93 Initializing GRPOLearner...\")\ngrpo_learner = GRPOLearner(\n    rl_cluster=rl_cluster,\n    algo_config=grpo_config,\n    reward_fns=[tunix_reward_wrapper],  # Use our wrapped reward function\n)\nprint(\"\u2705 GRPOLearner initialized!\")\nprint(\"   Reward function: tunix_reward_wrapper (composite XML/length/correctness)\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\u2705 TRAINING SETUP COMPLETE\")\nprint(\"=\"*60)\nprint(\"\\nReady to execute GRPO training loop in the next cell.\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Execute GRPO Training\nprint(\"\ud83c\udfaf Starting GRPO Training...\")\nprint(\"=\"*60)\n\nimport time\nfrom datetime import datetime\n\n# Prepare training dataset in Tunix format\nprint(\"\\n\ud83d\udcca Preparing training data...\")\ntrain_prompts = [ex[\"prompt\"] for ex in training_dataset]\nprint(f\"   Training examples: {len(train_prompts)}\")\n\n# Training configuration\nnum_iterations = GRPO_CONFIG[\"num_iterations\"]\nbatch_size = GRPO_CONFIG[\"batch_size\"]\neval_every = GRPO_CONFIG[\"eval_every_n_steps\"]\ncheckpoint_every = GRPO_CONFIG[\"checkpoint_every_n_steps\"]\nlog_every = TRAINING_CONFIG[\"log_every_n_steps\"]\n\nprint(f\"\\n\ud83d\udccb Training Configuration:\")\nprint(f\"   Iterations: {num_iterations}\")\nprint(f\"   Batch size: {batch_size}\")\nprint(f\"   Eval every: {eval_every} steps\")\nprint(f\"   Checkpoint every: {checkpoint_every} steps\")\n\n# Training metrics storage\ntraining_metrics = {\n    \"losses\": [],\n    \"rewards\": [],\n    \"kl_divergences\": [],\n    \"steps\": [],\n}\n\n# Execute training\nstart_time = time.time()\nglobal_step = 0\n\ntry:\n    with mesh:\n        for iteration in range(num_iterations):\n            print(f\"\\n{'='*60}\")\n            print(f\"\ud83d\udcc8 Iteration {iteration + 1}/{num_iterations}\")\n            print(f\"{'='*60}\")\n            \n            iteration_start = time.time()\n            \n            # Create batches for this iteration\n            num_batches = (len(train_prompts) + batch_size - 1) // batch_size\n            \n            for batch_idx in range(num_batches):\n                # Get batch prompts\n                start_idx = batch_idx * batch_size\n                end_idx = min(start_idx + batch_size, len(train_prompts))\n                batch_prompts = train_prompts[start_idx:end_idx]\n                \n                # Execute GRPO training step\n                step_metrics = grpo_learner.train_step(\n                    prompts=batch_prompts,\n                )\n                \n                global_step += 1\n                \n                # Store metrics\n                training_metrics[\"losses\"].append(step_metrics.get(\"loss\", 0.0))\n                training_metrics[\"rewards\"].append(step_metrics.get(\"mean_reward\", 0.0))\n                training_metrics[\"kl_divergences\"].append(step_metrics.get(\"kl_divergence\", 0.0))\n                training_metrics[\"steps\"].append(global_step)\n                \n                # Log progress\n                if global_step % log_every == 0:\n                    print(f\"\\n   Step {global_step}:\")\n                    print(f\"      Loss: {step_metrics.get('loss', 0.0):.4f}\")\n                    print(f\"      Mean Reward: {step_metrics.get('mean_reward', 0.0):.4f}\")\n                    print(f\"      KL Divergence: {step_metrics.get('kl_divergence', 0.0):.4f}\")\n                \n                # Evaluation\n                if global_step % eval_every == 0:\n                    print(f\"\\n   \ud83d\udcca Evaluation at step {global_step}:\")\n                    # Generate sample output\n                    sample_prompt = train_prompts[0]\n                    sample_output = grpo_learner.generate(\n                        prompts=[sample_prompt],\n                        max_tokens=GRPO_CONFIG[\"max_tokens_to_generate\"],\n                    )[0]\n                    \n                    # Validate output format\n                    has_format = validate_xml_format(sample_output)\n                    reasoning, answer = extract_xml_content(sample_output)\n                    \n                    print(f\"      Valid XML format: {has_format}\")\n                    if reasoning:\n                        reasoning_tokens = len(tokenizer.encode(reasoning))\n                        print(f\"      Reasoning tokens: {reasoning_tokens}\")\n                    print(f\"      Sample output preview: {sample_output[:200]}...\")\n                \n                # Checkpoint\n                if global_step % checkpoint_every == 0:\n                    checkpoint_path = f\"{CHECKPOINT_DIR}/step_{global_step}\"\n                    grpo_learner.save_checkpoint(checkpoint_path)\n                    print(f\"\\n   \ud83d\udcbe Checkpoint saved: {checkpoint_path}\")\n            \n            iteration_time = time.time() - iteration_start\n            print(f\"\\n   \u23f1\ufe0f Iteration {iteration + 1} completed in {iteration_time:.1f}s\")\n            \n            # Iteration summary\n            recent_losses = training_metrics[\"losses\"][-num_batches:]\n            recent_rewards = training_metrics[\"rewards\"][-num_batches:]\n            print(f\"   \ud83d\udcca Iteration Summary:\")\n            print(f\"      Avg Loss: {sum(recent_losses)/len(recent_losses):.4f}\")\n            print(f\"      Avg Reward: {sum(recent_rewards)/len(recent_rewards):.4f}\")\n\n    # Training complete\n    total_time = time.time() - start_time\n    print(f\"\\n{'='*60}\")\n    print(\"\u2705 TRAINING COMPLETE!\")\n    print(f\"{'='*60}\")\n    print(f\"   Total steps: {global_step}\")\n    print(f\"   Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n    print(f\"   Final avg loss: {sum(training_metrics['losses'][-10:])/10:.4f}\")\n    print(f\"   Final avg reward: {sum(training_metrics['rewards'][-10:])/10:.4f}\")\n\n    # Save final checkpoint\n    print(f\"\\n\ud83d\udcbe Saving final checkpoint to {FINAL_DIR}...\")\n    grpo_learner.save_checkpoint(FINAL_DIR)\n    print(\"\u2705 Final checkpoint saved!\")\n    \nexcept KeyboardInterrupt:\n    print(\"\\n\u26a0\ufe0f Training interrupted by user!\")\n    print(f\"   Completed steps: {global_step}\")\n    # Save emergency checkpoint\n    emergency_path = f\"{CHECKPOINT_DIR}/interrupted_step_{global_step}\"\n    grpo_learner.save_checkpoint(emergency_path)\n    print(f\"   Emergency checkpoint saved: {emergency_path}\")\n    \nexcept Exception as e:\n    print(f\"\\n\u274c Training error: {e}\")\n    print(f\"   Last completed step: {global_step}\")\n    # Try to save checkpoint on error\n    try:\n        error_path = f\"{CHECKPOINT_DIR}/error_step_{global_step}\"\n        grpo_learner.save_checkpoint(error_path)\n        print(f\"   Error checkpoint saved: {error_path}\")\n    except:\n        print(\"   Could not save error checkpoint\")\n    raise\n\n# Display training summary plot\nprint(\"\\n\ud83d\udcca Training Metrics Summary:\")\nprint(f\"   Steps: {len(training_metrics['steps'])}\")\nprint(f\"   Loss range: {min(training_metrics['losses']):.4f} - {max(training_metrics['losses']):.4f}\")\nprint(f\"   Reward range: {min(training_metrics['rewards']):.4f} - {max(training_metrics['rewards']):.4f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udce6 Task 5: Export LoRA Adapters and Create Kaggle Submission\n",
        "\n",
        "Package trained adapters for Kaggle submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create kaggle_upload directory\n",
        "KAGGLE_DIR = \"./kaggle_upload\"\n",
        "os.makedirs(KAGGLE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\u2705 Created Kaggle submission directory: {KAGGLE_DIR}\")\n",
        "print(\"\\n\ud83d\udccb Export checklist:\")\n",
        "print(\"   [ ] adapter_config.json - LoRA configuration\")\n",
        "print(\"   [ ] adapter_model.safetensors - LoRA weights\")\n",
        "print(\"   [ ] tokenizer files (if modified)\")\n",
        "print(\"   [ ] README with inference instructions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Export LoRA Adapters using Tunix API\nprint(\"\ud83d\udce6 Exporting LoRA adapters...\")\n\nimport json\nimport shutil\nfrom safetensors.flax import save_file as save_safetensors\n\n# Export LoRA weights from trained model\nprint(\"\\n\ud83d\udce4 Extracting LoRA weights from actor model...\")\n\ntry:\n    # Method 1: Use Tunix's built-in export (preferred)\n    grpo_learner.export_lora_adapters(\n        output_dir=KAGGLE_DIR,\n        format=\"safetensors\"\n    )\n    print(\"\u2705 LoRA adapters exported using Tunix API\")\n    \nexcept AttributeError:\n    # Method 2: Manual extraction if export method not available\n    print(\"   Using manual extraction method...\")\n    from tunix.peft import lora as lora_lib\n    \n    # Extract LoRA weights\n    lora_weights = lora_lib.extract_lora_weights(actor_model)\n    \n    # Save in safetensors format\n    adapter_path = f\"{KAGGLE_DIR}/adapter_model.safetensors\"\n    save_safetensors(lora_weights, adapter_path)\n    print(f\"\u2705 LoRA weights saved: {adapter_path}\")\n\n# Create adapter_config.json\nadapter_config = {\n    \"peft_type\": \"LORA\",\n    \"task_type\": \"CAUSAL_LM\",\n    \"r\": LORA_CONFIG[\"rank\"],\n    \"lora_alpha\": LORA_CONFIG[\"alpha\"],\n    \"lora_dropout\": LORA_CONFIG[\"dropout\"],\n    \"target_modules\": LORA_CONFIG[\"target_modules\"],\n    \"inference_mode\": True,\n    \"base_model_name_or_path\": MODEL_ID,\n    \"bias\": \"none\",\n    \"fan_in_fan_out\": False,\n}\n\nconfig_path = f\"{KAGGLE_DIR}/adapter_config.json\"\nwith open(config_path, 'w') as f:\n    json.dump(adapter_config, f, indent=2)\nprint(f\"\u2705 Config saved: {config_path}\")\n\n# Copy tokenizer files\nprint(\"\\n\ud83d\udcc1 Copying tokenizer files...\")\ntokenizer_files = [\"tokenizer.json\", \"tokenizer_config.json\", \"special_tokens_map.json\"]\nfor fname in tokenizer_files:\n    src = f\"{model_path}/{fname}\"\n    dst = f\"{KAGGLE_DIR}/{fname}\"\n    if os.path.exists(src):\n        shutil.copy2(src, dst)\n        print(f\"   Copied: {fname}\")\n\n# Create README\nreadme_content = f\"\"\"# Judicaita GRPO-Trained LoRA Adapters\n\n## Model Information\n\n- **Base Model**: {MODEL_ID}\n- **Training Method**: GRPO (Group Relative Policy Optimization)\n- **Framework**: Google Tunix + JAX/Flax\n- **LoRA Rank**: {LORA_CONFIG[\"rank\"]}\n- **LoRA Alpha**: {LORA_CONFIG[\"alpha\"]}\n- **Training Platform**: Google Colab TPU\n\n## Inference Usage\n\n### With Transformers + PEFT\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"{MODEL_ID}\")\ntokenizer = AutoTokenizer.from_pretrained(\"{MODEL_ID}\")\n\n# Load LoRA adapters\nmodel = PeftModel.from_pretrained(\n    base_model,\n    \"./adapter_model\"  # Path to this directory\n)\n\n# Generate\nprompt = \"Your legal question here\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_length=512)\nresponse = tokenizer.decode(outputs[0])\n```\n\n### Expected Output Format\n\nThe model generates responses in XML format:\n```xml\n<reasoning>\nDetailed legal reasoning with analysis...\n</reasoning>\n<answer>\nFinal answer or conclusion\n</answer>\n```\n\n## Training Details\n\n- **Reward Function**: Composite (30% format + 30% length + 40% correctness)\n- **GRPO Beta (KL penalty)**: {GRPO_CONFIG[\"beta\"]}\n- **Num Generations**: {GRPO_CONFIG[\"num_generations\"]}\n- **Learning Rate**: {GRPO_CONFIG[\"learning_rate\"]}\n\n## Validation Criteria\n\n- Reasoning should be >= 100 tokens\n- Both XML tags must be present\n- Answer should be relevant to the question\n\n## License\n\nSame as base model ({MODEL_ID})\n\"\"\"\n\nwith open(f\"{KAGGLE_DIR}/README.md\", 'w') as f:\n    f.write(readme_content)\nprint(\"\u2705 README.md created\")\n\n# List exported files\nprint(\"\\n\ud83d\udccb Exported files:\")\nfor item in os.listdir(KAGGLE_DIR):\n    item_path = os.path.join(KAGGLE_DIR, item)\n    size = os.path.getsize(item_path) if os.path.isfile(item_path) else 0\n    print(f\"   {item}: {size/1024:.1f} KB\" if size > 0 else f\"   {item}/\")\n\nprint(\"\\n\u2705 Export complete!\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate Exported Model\n",
        "\n",
        "Test the exported adapters with inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Validate Exported Model with Inference\nprint(\"\ud83e\uddea Running Inference Validation...\")\nprint(\"=\"*60)\n\n# Test prompts for validation\ntest_prompts = [\n    \"Is a verbal contract enforceable in most jurisdictions?\",\n    \"What are the elements required to prove negligence?\",\n    \"Can a contract be voided if one party was under duress?\",\n]\n\nprint(\"\\n\ud83d\udcdd Test Prompts:\")\nfor i, prompt in enumerate(test_prompts, 1):\n    print(f\"   {i}. {prompt}\")\n\n# Generate responses using trained model\nprint(\"\\n\ud83d\udd04 Generating responses with trained model...\")\n\nvalidation_results = []\n\nfor i, prompt in enumerate(test_prompts):\n    # Create full prompt with system instructions\n    full_prompt = create_prompt_template(prompt)\n    \n    # Generate response\n    try:\n        response = grpo_learner.generate(\n            prompts=[full_prompt],\n            max_tokens=GRPO_CONFIG[\"max_tokens_to_generate\"],\n            temperature=0.7,\n        )[0]\n    except Exception as e:\n        print(f\"\\n\u274c Generation error for prompt {i+1}: {e}\")\n        continue\n    \n    # Validate format\n    has_valid_format = validate_xml_format(response)\n    reasoning, answer = extract_xml_content(response)\n    \n    # Count reasoning tokens\n    reasoning_tokens = 0\n    if reasoning:\n        reasoning_tokens = len(tokenizer.encode(reasoning))\n    \n    # Compute reward\n    reward = composite_reward_function(\n        [full_prompt],\n        [response],\n        [{\"ground_truth\": \"\"}],  # No ground truth for test prompts\n        tokenizer\n    )[0]\n    \n    result = {\n        \"prompt\": prompt,\n        \"response\": response,\n        \"valid_format\": has_valid_format,\n        \"reasoning_tokens\": reasoning_tokens,\n        \"has_reasoning\": reasoning is not None,\n        \"has_answer\": answer is not None,\n        \"reward\": reward,\n    }\n    validation_results.append(result)\n    \n    # Display results\n    print(f\"\\n{'='*60}\")\n    print(f\"\ud83d\udccb Test {i+1}: {prompt[:50]}...\")\n    print(f\"{'='*60}\")\n    print(f\"   \u2713 Valid XML format: {has_valid_format}\")\n    print(f\"   \u2713 Reasoning tokens: {reasoning_tokens}\")\n    print(f\"   \u2713 Has reasoning: {reasoning is not None}\")\n    print(f\"   \u2713 Has answer: {answer is not None}\")\n    print(f\"   \u2713 Reward score: {reward:.3f}\")\n    \n    if reasoning:\n        print(f\"\\n   \ud83d\udcdd Reasoning preview:\")\n        print(f\"      {reasoning[:200]}...\")\n    if answer:\n        print(f\"\\n   \ud83d\udca1 Answer:\")\n        print(f\"      {answer[:200]}\")\n\n# Summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"\ud83d\udcca VALIDATION SUMMARY\")\nprint(\"=\"*60)\n\nvalid_count = sum(1 for r in validation_results if r[\"valid_format\"])\navg_reasoning_tokens = sum(r[\"reasoning_tokens\"] for r in validation_results) / len(validation_results) if validation_results else 0\navg_reward = sum(r[\"reward\"] for r in validation_results) / len(validation_results) if validation_results else 0\n\nprint(f\"   Total test prompts: {len(test_prompts)}\")\nprint(f\"   Valid XML format: {valid_count}/{len(validation_results)} ({100*valid_count/len(validation_results):.0f}%)\" if validation_results else \"   No results\")\nprint(f\"   Avg reasoning tokens: {avg_reasoning_tokens:.0f}\")\nprint(f\"   Avg reward score: {avg_reward:.3f}\")\n\n# Quality assessment\nprint(\"\\n\ud83d\udcc8 Quality Assessment:\")\nif avg_reward >= 0.7:\n    print(\"   \u2705 EXCELLENT: Model produces high-quality legal reasoning\")\nelif avg_reward >= 0.5:\n    print(\"   \u2705 GOOD: Model produces adequate legal reasoning\")\nelif avg_reward >= 0.3:\n    print(\"   \u26a0\ufe0f FAIR: Model needs more training for better quality\")\nelse:\n    print(\"   \u274c POOR: Model requires significant improvement\")\n\nif valid_count == len(validation_results) and validation_results:\n    print(\"   \u2705 All outputs have valid XML format\")\nelif valid_count > 0:\n    print(f\"   \u26a0\ufe0f Some outputs missing proper XML tags ({len(validation_results) - valid_count} invalid)\")\nelse:\n    print(\"   \u274c No outputs have valid XML format - check training\")\n\nprint(\"\\n\u2705 Validation complete!\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Create zip archive\n",
        "def create_submission_zip(source_dir: str, output_file: str):\n",
        "    \"\"\"\n",
        "    Create a zip archive for Kaggle submission.\n",
        "    \n",
        "    Args:\n",
        "        source_dir: Directory containing files to zip\n",
        "        output_file: Output zip file path\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(output_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(source_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, source_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "                print(f\"   Added: {arcname}\")\n",
        "    \n",
        "    # Get zip file size\n",
        "    size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
        "    return size_mb\n",
        "\n",
        "# Create submission\n",
        "submission_zip = \"./judicaita_submission.zip\"\n",
        "print(\"\ud83d\udce6 Creating Kaggle submission package...\")\n",
        "print(f\"   Source: {KAGGLE_DIR}\")\n",
        "print(f\"   Output: {submission_zip}\")\n",
        "print(\"\\n\ud83d\udcc4 Files included:\")\n",
        "\n",
        "try:\n",
        "    size = create_submission_zip(KAGGLE_DIR, submission_zip)\n",
        "    print(f\"\\n\u2705 Submission package created!\")\n",
        "    print(f\"   File: {submission_zip}\")\n",
        "    print(f\"   Size: {size:.2f} MB\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udccb Submission Checklist:\")\n",
        "    print(\"   \u2705 adapter_config.json\")\n",
        "    print(\"   \u2705 README.md with instructions\")\n",
        "    print(\"   \u26a0\ufe0f  adapter_model.safetensors (add after training)\")\n",
        "    print(\"   \u26a0\ufe0f  Validation results (add after testing)\")\n",
        "    \n",
        "    print(\"\\n\ud83c\udfaf Next Steps:\")\n",
        "    print(\"   1. Complete GRPO training\")\n",
        "    print(\"   2. Export adapter weights to kaggle_upload/\")\n",
        "    print(\"   3. Run inference validation\")\n",
        "    print(\"   4. Re-run this cell to create final zip\")\n",
        "    print(\"   5. Upload to Kaggle competition\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error creating zip: {e}\")\n",
        "    print(\"   Make sure kaggle_upload directory has content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### \ud83d\udd27 Troubleshooting Guide\n\n#### Tunix Import Errors\n- **ModuleNotFoundError: No module named 'tunix'**\n  - Ensure you installed with TPU extras: `pip install \"google-tunix[tpu]\"`\n  - Restart runtime after installation\n  - Verify version: `python -c \"import tunix; print(tunix.__version__)\"`\n\n- **ImportError: cannot import name 'GRPOLearner'**\n  - Check Tunix version >= 0.5.0\n  - Verify correct import path: `from tunix.rl.grpo.grpo_learner import GRPOLearner`\n\n#### JAX/TPU Initialization Issues\n- **RuntimeError: TPU not found**\n  - Verify Colab runtime is set to TPU: Runtime \u2192 Change runtime type \u2192 TPU\n  - Try restarting the runtime completely\n  - Check TPU quota in Google Cloud Console if using custom project\n\n- **JAX version mismatch errors**\n  - Install pinned versions: `pip install jax==0.4.35 jaxlib==0.4.35`\n  - Restart runtime after JAX installation\n  - Verify: `python -c \"import jax; print(jax.__version__, jax.devices())\"`\n\n#### RLCluster Configuration Errors\n- **ValueError: Mesh shape mismatch**\n  - Ensure mesh is created with correct number of devices\n  - Check `len(jax.devices())` matches expected TPU cores\n  - For TPU v2-8, expect 8 devices\n\n- **Sharding errors during training**\n  - Verify data_sharding is compatible with batch size\n  - Reduce batch_size to 1 or 2 for debugging\n  - Check model dtype is bfloat16 for TPU\n\n#### Memory Errors (OOM)\n- **Out of Memory during rollout generation**\n  - Reduce `num_generations` from 4 to 2\n  - Reduce `max_tokens_to_generate` from 512 to 256\n  - Reduce `batch_size` from 4 to 2 or 1\n\n- **Out of Memory during backward pass**\n  - Use smaller LoRA rank: try rank=8 instead of 16\n  - Enable gradient checkpointing if available\n  - Reduce sequence length\n\n#### Reward Function Issues\n- **Reward function signature mismatch**\n  - Tunix expects `reward_fn(prompts: List[str], outputs: List[str]) -> List[float]`\n  - Use `tunix_reward_wrapper` instead of `composite_reward_function` directly\n  - Ensure function returns Python list of floats, not numpy/jax arrays\n\n- **All rewards are 0.0**\n  - Check if model is generating XML tags properly\n  - Verify `extract_xml_content()` is working correctly\n  - Test reward function manually with sample outputs\n\n#### Checkpoint Issues\n- **Checkpoint save fails**\n  - Ensure checkpoint directory exists and is writable\n  - Check disk space (Colab has ~100GB limit)\n  - For large models, consider saving to Google Drive\n\n- **Checkpoint load fails**\n  - Verify checkpoint path is correct\n  - Check if checkpoint was saved completely (no interruption)\n  - Try loading with `strict=False` to ignore missing keys\n\n#### Training Not Converging\n- **Loss not decreasing**\n  - Try lower learning rate: 5e-6 or 1e-6\n  - Increase warmup steps\n  - Check if rewards are providing meaningful signal\n\n- **KL divergence too high**\n  - Increase beta (KL penalty coefficient)\n  - Reduce learning rate\n  - Ensure reference model is properly frozen\n\n- **Rewards not improving**\n  - Verify ground truth data quality\n  - Check reward function components individually\n  - Increase training iterations\n\n#### Export Issues\n- **safetensors export fails**\n  - Install safetensors: `pip install safetensors>=0.4.0`\n  - Verify weights are on CPU before saving\n  - Check file path permissions\n\n- **Exported adapters don't load in PyTorch**\n  - Ensure adapter_config.json has correct format\n  - Verify target_modules match PyTorch model layer names\n  - Check if conversion from Flax to PyTorch is needed\n\n#### Colab-Specific Issues\n- **Runtime disconnection during training**\n  - Save checkpoints frequently (every 50-100 steps)\n  - Keep browser tab active\n  - Consider using Colab Pro for longer runtime\n\n- **Storage limit reached**\n  - Clear old checkpoints: keep only latest + final\n  - Export to Google Drive\n  - Use smaller checkpoint format\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udf89 Conclusion\n\nThis notebook demonstrates end-to-end GRPO training for legal reasoning using Google Tunix on TPU:\n\n### What We Built\n\n1. \u2705 **TPU Setup**: Initialized JAX with TPU v2-8 using `colab_tpu.setup_tpu()`\n2. \u2705 **Model Loading**: Downloaded Gemma 3-1B-IT and initialized with LoRA adapters\n3. \u2705 **Dataset Preparation**: Created XML-formatted prompts for legal reasoning\n4. \u2705 **Reward Function**: Implemented composite scoring (format + length + correctness)\n5. \u2705 **GRPO Training**: Executed training with `GRPOLearner` and `RLCluster`\n6. \u2705 **Export**: Packaged LoRA adapters in safetensors format for submission\n\n### Training Results\n\nAfter training, the model should:\n- Generate responses in valid XML format (`<reasoning>...</reasoning><answer>...</answer>`)\n- Produce detailed legal reasoning (100+ tokens)\n- Provide accurate answers based on legal principles\n\n### Files Produced\n\n| File | Description |\n|------|-------------|\n| `adapter_config.json` | LoRA configuration for PEFT |\n| `adapter_model.safetensors` | Trained LoRA weights |\n| `README.md` | Inference instructions |\n| `judicaita_submission.zip` | Kaggle submission package |\n\n### Next Steps\n\n1. **Upload to Kaggle**: Submit `judicaita_submission.zip` to the competition\n2. **Fine-tune Further**: Increase training iterations for better results\n3. **Add More Data**: Include additional legal reasoning examples\n4. **Evaluate on LegalBench**: Test on official benchmark tasks\n\n### Resources\n\n- [Tunix Documentation](https://tunix.readthedocs.io/)\n- [Tunix GRPO Gemma Example](https://github.com/google/tunix/tree/main/examples/grpo_gemma)\n- [Judicaita Repository](https://github.com/clduab11/judicAIta)\n- [Gemma Model Cards](https://ai.google.dev/gemma)\n- [JAX TPU Guide](https://jax.readthedocs.io/en/latest/notebooks/TPU_Colab.html)\n\n### Troubleshooting & Support\n\nIf you encounter issues:\n1. Check the Troubleshooting Guide section above\n2. Open an issue: https://github.com/clduab11/judicAIta/issues\n3. Review Tunix documentation for API changes\n\n### Contributing\n\nImprovements welcome! Submit a PR with:\n- Additional reward function components\n- Better data preprocessing\n- Performance optimizations\n- Documentation improvements\n\n---\n\n**Made with \u2764\ufe0f for the Kaggle hackathon and legal tech community**\n\n*Powered by Google Tunix, JAX, and Gemma*\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}