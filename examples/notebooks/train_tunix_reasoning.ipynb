{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Judicaita: GRPO Training with Google Tunix on TPU\n\n## \ud83c\udfaf Hackathon Context\n\nThis notebook demonstrates **GRPO (Group Relative Policy Optimization)** training for the Judicaita legal AI assistant using:\n- **Google Tunix** for RL training infrastructure\n- **Gemma 3-1B-IT** as the base model\n- **TPU v2-8+** for accelerated training\n- **LoRA adapters** for parameter-efficient fine-tuning\n\nThis is developed for the Kaggle hackathon to train models that generate explainable legal reasoning with structured XML-formatted outputs.\n\n## \u26a1 TPU Requirements\n\n**IMPORTANT**: This notebook requires:\n- Google Colab with TPU runtime (TPU v2-8 or higher)\n- Runtime type: TPU (not CPU or GPU)\n- To enable: Runtime \u2192 Change runtime type \u2192 Hardware accelerator: TPU\n\n## \ud83d\udccb What This Notebook Does\n\n1. **Environment Setup**: Install Tunix, JAX, and dependencies for TPU\n2. **Model Loading**: Download and initialize Gemma 3-1B-IT\n3. **Dataset Preparation**: Format training data with XML-tagged reasoning\n4. **Reward Function**: Score outputs based on format, reasoning length, and correctness\n5. **GRPO Training**: Train with LoRA adapters on TPU\n6. **Export**: Package trained adapters for Kaggle submission\n\n## \ud83d\udd04 Data Flow\n\n```\nDataset \u2192 Prompts \u2192 Model Rollouts \u2192 Reward Scoring \u2192 GRPO Updates\n                                                           \u2193\n                                              LoRA Adapter Checkpoints\n```\n\n## \u26a0\ufe0f Differences from Main Codebase\n\n| Aspect | Main Codebase | This Notebook |\n|--------|---------------|---------------|\n| Format | Step-by-step format | XML `<reasoning>`/`<answer>` |\n| Framework | PyTorch | JAX/Flax |\n| Training | Custom GRPO | Tunix GRPOLearner |\n| Hardware | GPU/CPU | TPU v2-8+ |\n\n## \ud83d\udcda References\n\n- [Google Tunix Documentation](https://github.com/google/tunix)\n- [Gemma Model Card](https://ai.google.dev/gemma/docs)\n- [GRPO Paper](https://arxiv.org/abs/2402.03300)\n- [Judicaita Repository](https://github.com/clduab11/judicAIta)\n\n## \u26a0\ufe0f Known Limitations\n\n- **TPU Required**: Cannot run on CPU/GPU without code modifications\n- **Memory**: TPU v2-8 has ~64GB; larger models may need v3 or higher\n- **Dataset**: Assumes generic legal reasoning tasks (not LegalBench-specific)\n- **Checkpoints**: Large checkpoint files may exceed Colab storage limits"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udce6 Step 1: Install Dependencies\n",
        "\n",
        "Install required packages for TPU training with Tunix and Gemma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core dependencies\n",
        "!pip install -q google-tunix\n",
        "!pip install -q 'jax[tpu]>=0.4.20' -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -q flax>=0.8.0\n",
        "!pip install -q transformers>=4.40.0\n",
        "!pip install -q huggingface_hub>=0.20.0\n",
        "!pip install -q datasets>=2.14.0\n",
        "!pip install -q sentencepiece>=0.1.99\n",
        "\n",
        "print(\"\u2705 Dependencies installed successfully!\")\n",
        "print(\"\u26a0\ufe0f  IMPORTANT: Runtime restart required for TPU libraries.\")\n",
        "print(\"   Go to: Runtime \u2192 Restart runtime\")\n",
        "print(\"   Then continue with the next cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \u26a0\ufe0f Runtime Restart Required\n",
        "\n",
        "**STOP HERE** and restart the runtime:\n",
        "1. Click `Runtime` \u2192 `Restart runtime` in the menu\n",
        "2. After restart, continue from the next cell\n",
        "\n",
        "This is necessary for TPU libraries to be properly loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 Step 2: Initialize TPU Runtime\n",
        "\n",
        "Set up JAX to use TPU devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.tools import colab_tpu\n",
        "\n",
        "# Initialize TPU\n",
        "print(\"Initializing TPU runtime...\")\n",
        "try:\n",
        "    colab_tpu.setup_tpu()\n",
        "    print(\"\u2705 TPU initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c TPU initialization failed: {e}\")\n",
        "    print(\"\\n\ud83d\udd27 Troubleshooting:\")\n",
        "    print(\"   1. Check runtime type: Runtime \u2192 Change runtime type \u2192 TPU\")\n",
        "    print(\"   2. Ensure TPU v2-8 or higher is available\")\n",
        "    print(\"   3. Try restarting the runtime\")\n",
        "    print(\"   4. Check Google Cloud TPU quota if using custom project\")\n",
        "    raise\n",
        "\n",
        "# Verify TPU devices\n",
        "devices = jax.devices()\n",
        "print(f\"\\n\ud83d\udcca TPU Device Information:\")\n",
        "print(f\"   Number of devices: {len(devices)}\")\n",
        "print(f\"   Device type: {devices[0].platform}\")\n",
        "print(f\"   Devices: {devices}\")\n",
        "\n",
        "if len(devices) == 0:\n",
        "    raise RuntimeError(\"No TPU devices detected! Please check your runtime configuration.\")\n",
        "\n",
        "print(\"\\n\u2705 TPU setup complete and verified!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd10 Step 3: Authenticate with Hugging Face\n",
        "\n",
        "Login to Hugging Face to download the Gemma model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login, snapshot_download\n",
        "import os\n",
        "\n",
        "# Login to Hugging Face\n",
        "# You'll be prompted to enter your HF token\n",
        "# Get your token from: https://huggingface.co/settings/tokens\n",
        "print(\"Please enter your Hugging Face token:\")\n",
        "login()\n",
        "\n",
        "print(\"\\n\u2705 Authenticated with Hugging Face!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udce5 Step 4: Download Gemma 3-1B-IT Model\n\nDownload the model files and initialize the tokenizer.\n\n**Note**: Using `gemma-3-1b-it` as it's the latest available Gemma instruction-tuned model. Update to `gemma-3-1b-it` if/when available."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from transformers import AutoTokenizer\nimport os\n\n# Download model\nMODEL_ID = \"google/gemma-3-1b-it\"  # Using gemma-3-1b-it as gemma-3-1b-it may not be available yet\nCACHE_DIR = \"./gemma_model_cache\"\n\nprint(f\"Downloading {MODEL_ID}...\")\nmodel_path = snapshot_download(\n    repo_id=MODEL_ID,\n    cache_dir=CACHE_DIR,\n    local_dir=f\"{CACHE_DIR}/gemma\",\n    local_dir_use_symlinks=False\n)\nprint(f\"\u2705 Model downloaded to: {model_path}\")\n\n# Initialize tokenizer\nprint(\"\\nInitializing tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nprint(f\"\u2705 Tokenizer initialized\")\nprint(f\"   Vocab size: {len(tokenizer)}\")\nprint(f\"   Special tokens: {tokenizer.special_tokens_map}\")\n\n# Test tokenization\ntest_text = \"What is the legal precedent for breach of contract?\"\ntokens = tokenizer(test_text, return_tensors=\"np\")\nprint(f\"\\n\ud83d\udcdd Test tokenization:\")\nprint(f\"   Input: {test_text}\")\nprint(f\"   Token count: {len(tokens['input_ids'][0])}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Step 5: Create Preprocessing Function\n",
        "\n",
        "Gemma models don't have native system role support. We'll prepend the system prompt to the first user turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_with_system_prompt(messages, system_prompt):\n",
        "    \"\"\"\n",
        "    Prepend system prompt to first user message.\n",
        "    \n",
        "    Gemma doesn't support system role natively, so we merge it with\n",
        "    the first user turn as a workaround.\n",
        "    \n",
        "    Args:\n",
        "        messages: List of message dicts with 'role' and 'content'\n",
        "        system_prompt: System instruction string\n",
        "        \n",
        "    Returns:\n",
        "        Modified messages list with system prompt prepended\n",
        "    \"\"\"\n",
        "    if not messages:\n",
        "        return messages\n",
        "    \n",
        "    processed = messages.copy()\n",
        "    \n",
        "    # Find first user message\n",
        "    for i, msg in enumerate(processed):\n",
        "        if msg.get('role') == 'user':\n",
        "            # Prepend system prompt\n",
        "            original_content = msg['content']\n",
        "            processed[i]['content'] = f\"{system_prompt}\\n\\n{original_content}\"\n",
        "            break\n",
        "    \n",
        "    return processed\n",
        "\n",
        "# Define system prompt for legal reasoning\n",
        "SYSTEM_PROMPT = \"\"\"You are a legal AI assistant. For each question, provide your analysis in this exact format:\n",
        "<reasoning>Your step-by-step legal reasoning here. Include relevant legal principles, precedents, and analysis. Aim for at least 100 tokens of detailed reasoning.</reasoning>\n",
        "<answer>Your final answer or conclusion here.</answer>\n",
        "\n",
        "Always use this XML format and ensure your reasoning is thorough and well-explained.\"\"\"\n",
        "\n",
        "# Test preprocessing\n",
        "test_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Is a non-compete clause enforceable in California?\"}\n",
        "]\n",
        "processed = preprocess_with_system_prompt(test_messages, SYSTEM_PROMPT)\n",
        "print(\"\ud83d\udcdd Test preprocessing:\")\n",
        "print(f\"Original: {test_messages[0]['content'][:50]}...\")\n",
        "print(f\"\\nProcessed length: {len(processed[0]['content'])} chars\")\n",
        "print(f\"System prompt prepended: {'<reasoning>' in processed[0]['content']}\")\n",
        "print(\"\\n\u2705 Preprocessing function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca Task 2: Prepare Training Dataset\n",
        "\n",
        "Create a dataset with XML-tagged reasoning format compatible with Tunix GRPO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### JSONL Format Requirements\n",
        "\n",
        "Each training example must be a JSON object with:\n",
        "- `prompt`: The question or task\n",
        "- `ground_truth`: The correct answer for evaluation\n",
        "- `metadata` (optional): Additional info like task_id, difficulty, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def prepare_dataset_for_tunix(examples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Prepare dataset in Tunix-compatible JSONL format.\n",
        "    \n",
        "    Args:\n",
        "        examples: List of dicts with 'question' and 'answer' fields\n",
        "        \n",
        "    Returns:\n",
        "        List of dicts with 'prompt', 'ground_truth', and 'metadata'\n",
        "    \"\"\"\n",
        "    prepared = []\n",
        "    \n",
        "    for idx, ex in enumerate(examples):\n",
        "        prepared.append({\n",
        "            \"prompt\": ex.get(\"question\", ex.get(\"prompt\", \"\")),\n",
        "            \"ground_truth\": ex.get(\"answer\", ex.get(\"ground_truth\", \"\")),\n",
        "            \"metadata\": {\n",
        "                \"example_id\": idx,\n",
        "                \"original_question\": ex.get(\"question\", \"\"),\n",
        "                \"task_type\": ex.get(\"task_type\", \"general_reasoning\")\n",
        "            }\n",
        "        })\n",
        "    \n",
        "    return prepared\n",
        "\n",
        "# Create synthetic sample data (replace with real data)\n",
        "sample_examples = [\n",
        "    {\n",
        "        \"question\": \"Can an employer in California enforce a non-compete clause against a former employee?\",\n",
        "        \"answer\": \"No, non-compete clauses are generally unenforceable in California except in limited circumstances involving sale of business or dissolution of partnership.\",\n",
        "        \"task_type\": \"legal_qa\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the statute of limitations for filing a breach of contract claim?\",\n",
        "        \"answer\": \"The statute of limitations varies by jurisdiction. In many states, it is 4-6 years for written contracts and 2-3 years for oral contracts.\",\n",
        "        \"task_type\": \"legal_qa\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Under what circumstances can a contract be voided for duress?\",\n",
        "        \"answer\": \"A contract can be voided for duress when one party was forced to enter the agreement through threats, violence, or other improper pressure that overcame their free will.\",\n",
        "        \"task_type\": \"legal_qa\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is required to establish an attorney-client privilege?\",\n",
        "        \"answer\": \"Attorney-client privilege requires: (1) an attorney-client relationship, (2) confidential communication, (3) made for the purpose of seeking or providing legal advice.\",\n",
        "        \"task_type\": \"legal_qa\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# Prepare dataset\n",
        "prepared_dataset = prepare_dataset_for_tunix(sample_examples)\n",
        "\n",
        "print(f\"\u2705 Prepared {len(prepared_dataset)} training examples\")\n",
        "print(f\"\\n\ud83d\udcdd Sample example:\")\n",
        "print(json.dumps(prepared_dataset[0], indent=2))\n",
        "\n",
        "# Note: In production, load from file or HuggingFace dataset\n",
        "print(\"\\n\ud83d\udca1 To load from file:\")\n",
        "print(\"   # with open('data.jsonl', 'r') as f:\")\n",
        "print(\"   #     examples = [json.loads(line) for line in f]\")\n",
        "print(\"   #     prepared_dataset = prepare_dataset_for_tunix(examples)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt Template with XML Format\n",
        "\n",
        "Create a template that formats prompts to expect XML-tagged reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prompt_template(question: str, system_prompt: str = SYSTEM_PROMPT) -> str:\n",
        "    \"\"\"\n",
        "    Create a formatted prompt with XML output expectations.\n",
        "    \n",
        "    Args:\n",
        "        question: The legal question to answer\n",
        "        system_prompt: System instructions for format\n",
        "        \n",
        "    Returns:\n",
        "        Formatted prompt string\n",
        "    \"\"\"\n",
        "    template = f\"\"\"{system_prompt}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Response:\"\"\"\n",
        "    return template\n",
        "\n",
        "def validate_xml_format(response: str) -> bool:\n",
        "    \"\"\"\n",
        "    Validate that response contains proper XML tags.\n",
        "    \n",
        "    Args:\n",
        "        response: Model generated response\n",
        "        \n",
        "    Returns:\n",
        "        True if valid XML format, False otherwise\n",
        "    \"\"\"\n",
        "    # Check for both opening and closing tags\n",
        "    has_reasoning = '<reasoning>' in response and '</reasoning>' in response\n",
        "    has_answer = '<answer>' in response and '</answer>' in response\n",
        "    \n",
        "    return has_reasoning and has_answer\n",
        "\n",
        "# Apply template to all examples\n",
        "templated_prompts = []\n",
        "for example in prepared_dataset:\n",
        "    templated = {\n",
        "        \"prompt\": create_prompt_template(example[\"prompt\"]),\n",
        "        \"ground_truth\": example[\"ground_truth\"],\n",
        "        \"metadata\": example[\"metadata\"],\n",
        "        \"original_prompt\": example[\"prompt\"]\n",
        "    }\n",
        "    templated_prompts.append(templated)\n",
        "\n",
        "print(f\"\u2705 Created {len(templated_prompts)} templated prompts\")\n",
        "print(f\"\\n\ud83d\udcdd Sample templated prompt (first 300 chars):\")\n",
        "print(templated_prompts[0][\"prompt\"][:300])\n",
        "print(\"...\")\n",
        "\n",
        "# Test validation\n",
        "test_valid = \"<reasoning>This is reasoning</reasoning><answer>This is answer</answer>\"\n",
        "test_invalid = \"This is just text without tags\"\n",
        "print(f\"\\n\u2705 Validation test:\")\n",
        "print(f\"   Valid format: {validate_xml_format(test_valid)}\")\n",
        "print(f\"   Invalid format: {validate_xml_format(test_invalid)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization and Batching\n",
        "\n",
        "Tokenize prompts and prepare batches for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "# Set maximum prompt length\n",
        "MAX_PROMPT_LENGTH = 512  # Adjust based on your needs (512 or 1024)\n",
        "MAX_RESPONSE_LENGTH = 512\n",
        "\n",
        "def tokenize_prompts(prompts: List[str], tokenizer, max_length: int = MAX_PROMPT_LENGTH):\n",
        "    \"\"\"\n",
        "    Tokenize prompts with padding and truncation.\n",
        "    \n",
        "    Args:\n",
        "        prompts: List of prompt strings\n",
        "        tokenizer: HuggingFace tokenizer\n",
        "        max_length: Maximum token length\n",
        "        \n",
        "    Returns:\n",
        "        Dict with input_ids and attention_mask\n",
        "    \"\"\"\n",
        "    tokenized = tokenizer(\n",
        "        prompts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    return tokenized\n",
        "\n",
        "def create_training_batches(dataset: List[Dict], batch_size: int = 4):\n",
        "    \"\"\"\n",
        "    Create batches from dataset.\n",
        "    \n",
        "    Args:\n",
        "        dataset: List of training examples\n",
        "        batch_size: Number of examples per batch\n",
        "        \n",
        "    Returns:\n",
        "        List of batches, each batch is a list of examples\n",
        "    \"\"\"\n",
        "    batches = []\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        batch = dataset[i:i + batch_size]\n",
        "        batches.append(batch)\n",
        "    return batches\n",
        "\n",
        "# Tokenize all prompts\n",
        "all_prompts = [ex[\"prompt\"] for ex in templated_prompts]\n",
        "tokenized_prompts = tokenize_prompts(all_prompts, tokenizer, MAX_PROMPT_LENGTH)\n",
        "\n",
        "print(f\"\u2705 Tokenized {len(all_prompts)} prompts\")\n",
        "print(f\"   Max length: {MAX_PROMPT_LENGTH} tokens\")\n",
        "print(f\"   Shape: {tokenized_prompts['input_ids'].shape}\")\n",
        "\n",
        "# Create final dataset for training\n",
        "training_dataset = []\n",
        "for i, ex in enumerate(templated_prompts):\n",
        "    training_dataset.append({\n",
        "        \"prompt\": ex[\"prompt\"],\n",
        "        \"prompt_tokens\": tokenized_prompts['input_ids'][i],\n",
        "        \"attention_mask\": tokenized_prompts['attention_mask'][i],\n",
        "        \"ground_truth\": ex[\"ground_truth\"],\n",
        "        \"metadata\": ex[\"metadata\"]\n",
        "    })\n",
        "\n",
        "print(f\"\\n\u2705 Final training dataset: {len(training_dataset)} examples\")\n",
        "print(f\"   Each example has: {list(training_dataset[0].keys())}\")\n",
        "\n",
        "# Validate dataset format\n",
        "required_fields = [\"prompt\", \"ground_truth\", \"metadata\"]\n",
        "all_valid = all(all(field in ex for field in required_fields) for ex in training_dataset)\n",
        "print(f\"\\n\u2705 Dataset validation: {'PASSED' if all_valid else 'FAILED'}\")\n",
        "\n",
        "if not all_valid:\n",
        "    print(\"\u274c Some examples missing required fields!\")\n",
        "else:\n",
        "    print(\"   All examples have required fields: prompt, ground_truth, metadata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Task 3: Implement Custom Reward Function\n",
        "\n",
        "Create a reward function that scores:\n",
        "1. **Format**: Proper XML tags\n",
        "2. **Reasoning Length**: At least 100 tokens\n",
        "3. **Answer Correctness**: Match with ground truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "def extract_xml_content(response: str) -> Tuple[Optional[str], Optional[str]]:\n",
        "    \"\"\"\n",
        "    Extract content from <reasoning> and <answer> XML tags.\n",
        "    \n",
        "    Args:\n",
        "        response: Model-generated response string\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (reasoning_content, answer_content)\n",
        "        Returns (None, None) if tags are malformed or missing\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract reasoning\n",
        "        reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', response, re.DOTALL)\n",
        "        reasoning = reasoning_match.group(1).strip() if reasoning_match else None\n",
        "        \n",
        "        # Extract answer\n",
        "        answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)\n",
        "        answer = answer_match.group(1).strip() if answer_match else None\n",
        "        \n",
        "        return reasoning, answer\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Error extracting XML content: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Test extraction with edge cases\n",
        "test_cases = [\n",
        "    # Valid case\n",
        "    \"<reasoning>Step by step analysis here</reasoning><answer>Final answer</answer>\",\n",
        "    # Missing tags\n",
        "    \"Just plain text without tags\",\n",
        "    # Partial tags\n",
        "    \"<reasoning>Incomplete reasoning\",\n",
        "    # Nested content\n",
        "    \"<reasoning>Analysis with <term>nested</term> content</reasoning><answer>Yes</answer>\",\n",
        "    # Multi-line\n",
        "    \"\"\"<reasoning>\n",
        "Line 1 of reasoning\n",
        "Line 2 of reasoning\n",
        "</reasoning>\n",
        "<answer>Final answer</answer>\"\"\"\n",
        "]\n",
        "\n",
        "print(\"\ud83e\uddea Testing XML extraction:\")\n",
        "for i, test in enumerate(test_cases, 1):\n",
        "    reasoning, answer = extract_xml_content(test)\n",
        "    print(f\"\\nTest {i}:\")\n",
        "    print(f\"  Reasoning found: {reasoning is not None}\")\n",
        "    print(f\"  Answer found: {answer is not None}\")\n",
        "    if reasoning:\n",
        "        print(f\"  Reasoning preview: {reasoning[:50]}...\")\n",
        "    if answer:\n",
        "        print(f\"  Answer: {answer}\")\n",
        "\n",
        "print(\"\\n\u2705 XML extraction function tested with edge cases\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_format_reward(response: str) -> float:\n",
        "    \"\"\"\n",
        "    Reward for valid XML format.\n",
        "    \n",
        "    Returns:\n",
        "        1.0 if both <reasoning> and <answer> tags present and valid\n",
        "        0.0 otherwise\n",
        "    \"\"\"\n",
        "    reasoning, answer = extract_xml_content(response)\n",
        "    \n",
        "    # Check both tags present and have content\n",
        "    if reasoning is not None and answer is not None:\n",
        "        if len(reasoning.strip()) > 0 and len(answer.strip()) > 0:\n",
        "            return 1.0\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def compute_reasoning_length_reward(response: str, tokenizer, min_tokens: int = 100) -> float:\n",
        "    \"\"\"\n",
        "    Reward based on reasoning length.\n",
        "    \n",
        "    Returns:\n",
        "        1.0 if reasoning >= min_tokens\n",
        "        proportional score (tokens/min_tokens) if fewer\n",
        "        0.0 if no reasoning found\n",
        "    \"\"\"\n",
        "    reasoning, _ = extract_xml_content(response)\n",
        "    \n",
        "    if reasoning is None:\n",
        "        return 0.0\n",
        "    \n",
        "    # Tokenize reasoning to count tokens\n",
        "    tokens = tokenizer(reasoning, return_tensors=\"np\")[\"input_ids\"]\n",
        "    num_tokens = len(tokens[0])\n",
        "    \n",
        "    # Return 1.0 if meets threshold, otherwise proportional\n",
        "    if num_tokens >= min_tokens:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return num_tokens / min_tokens\n",
        "\n",
        "def compute_answer_correctness_reward(response: str, ground_truth: str, tokenizer) -> float:\n",
        "    \"\"\"\n",
        "    Reward based on answer correctness.\n",
        "    \n",
        "    Returns:\n",
        "        1.0 for exact match (after normalization)\n",
        "        Partial credit for token overlap (Jaccard similarity)\n",
        "        0.0 if no answer found\n",
        "    \"\"\"\n",
        "    _, answer = extract_xml_content(response)\n",
        "    \n",
        "    if answer is None:\n",
        "        return 0.0\n",
        "    \n",
        "    # Normalize for comparison\n",
        "    answer_norm = answer.lower().strip()\n",
        "    ground_truth_norm = ground_truth.lower().strip()\n",
        "    \n",
        "    # Check exact match\n",
        "    if answer_norm == ground_truth_norm:\n",
        "        return 1.0\n",
        "    \n",
        "    # Tokenize both for overlap calculation\n",
        "    answer_tokens = set(tokenizer.tokenize(answer_norm))\n",
        "    truth_tokens = set(tokenizer.tokenize(ground_truth_norm))\n",
        "    \n",
        "    # Calculate Jaccard similarity (intersection / union)\n",
        "    if len(answer_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    intersection = len(answer_tokens & truth_tokens)\n",
        "    union = len(answer_tokens | truth_tokens)\n",
        "    \n",
        "    jaccard = intersection / union if union > 0 else 0.0\n",
        "    \n",
        "    # Return Jaccard similarity as partial credit\n",
        "    return jaccard\n",
        "\n",
        "print(\"\u2705 Reward component functions defined:\")\n",
        "print(\"   - compute_format_reward()\")\n",
        "print(\"   - compute_reasoning_length_reward()\")\n",
        "print(\"   - compute_answer_correctness_reward()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def composite_reward_function(\n",
        "    prompts: List[str],\n",
        "    completions: List[str],\n",
        "    metadata: List[Dict],\n",
        "    tokenizer  # ADD: tokenizer parameter\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Main reward function with Tunix RewardFn signature.\n",
        "    \n",
        "    Combines three reward components:\n",
        "    - Format reward (30%): Valid XML tags\n",
        "    - Reasoning reward (30%): Sufficient reasoning length\n",
        "    - Correctness reward (40%): Answer accuracy\n",
        "    \n",
        "    Args:\n",
        "        prompts: List of input prompts\n",
        "        completions: List of model completions\n",
        "        metadata: List of metadata dicts with ground_truth\n",
        "        tokenizer: HuggingFace tokenizer for token counting\n",
        "        \n",
        "    Returns:\n",
        "        List of scalar rewards (one per example)\n",
        "    \"\"\"\n",
        "    # Reward weights\n",
        "    FORMAT_WEIGHT = 0.3\n",
        "    REASONING_WEIGHT = 0.3\n",
        "    CORRECTNESS_WEIGHT = 0.4\n",
        "    \n",
        "    rewards = []\n",
        "    \n",
        "    for i, (prompt, completion, meta) in enumerate(zip(prompts, completions, metadata)):\n",
        "        # Compute each reward component\n",
        "        format_reward = compute_format_reward(completion)\n",
        "        reasoning_reward = compute_reasoning_length_reward(completion, tokenizer, min_tokens=100)\n",
        "        \n",
        "        # Get ground truth from metadata\n",
        "        ground_truth = meta.get(\"ground_truth\", \"\")\n",
        "        correctness_reward = compute_answer_correctness_reward(completion, ground_truth, tokenizer)\n",
        "        \n",
        "        # Aggregate rewards\n",
        "        total_reward = (\n",
        "            FORMAT_WEIGHT * format_reward +\n",
        "            REASONING_WEIGHT * reasoning_reward +\n",
        "            CORRECTNESS_WEIGHT * correctness_reward\n",
        "        )\n",
        "        \n",
        "        rewards.append(total_reward)\n",
        "        \n",
        "        # Log breakdown for first few examples\n",
        "        if i < 3:\n",
        "            print(f\"\\n\ud83d\udcca Example {i} reward breakdown:\")\n",
        "            print(f\"   Format: {format_reward:.2f} (weight: {FORMAT_WEIGHT})\")\n",
        "            print(f\"   Reasoning: {reasoning_reward:.2f} (weight: {REASONING_WEIGHT})\")\n",
        "            print(f\"   Correctness: {correctness_reward:.2f} (weight: {CORRECTNESS_WEIGHT})\")\n",
        "            print(f\"   Total: {total_reward:.2f}\")\n",
        "    \n",
        "    return rewards\n",
        "\n",
        "# Test reward function with tokenizer parameter\n",
        "test_prompts = [\"Test question\"]\n",
        "test_completions = [\n",
        "    \"<reasoning>This is a detailed legal analysis with sufficient tokens to explain the reasoning behind the answer. We consider precedent, statutory law, and policy implications.</reasoning><answer>Yes, it is enforceable.</answer>\"\n",
        "]\n",
        "test_metadata = [{\"ground_truth\": \"Yes, it is enforceable.\"}]\n",
        "\n",
        "test_rewards = composite_reward_function(test_prompts, test_completions, test_metadata, tokenizer)\n",
        "print(f\"\\n\u2705 Reward function test complete\")\n",
        "print(f\"   Test reward: {test_rewards[0]:.2f}\")\n",
        "print(\"\\n\u2705 Composite reward function ready for Tunix GRPO!\")\n",
        "print(\"\\n\ud83d\udca1 Note: When using with Tunix, wrap this function to match their exact signature.\")\n",
        "print(\"   Example: lambda p, c, m: composite_reward_function(p, c, m, tokenizer)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 Task 4: Configure and Execute GRPO Training\n",
        "\n",
        "Set up LoRA adapters and run GRPO training on TPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA Hyperparameters\n",
        "LORA_CONFIG = {\n",
        "    \"rank\": 16,  # LoRA rank (16 or 32)\n",
        "    \"alpha\": 32,  # LoRA alpha (32 or 64)\n",
        "    \"dropout\": 0.05,  # LoRA dropout\n",
        "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
        "}\n",
        "\n",
        "# GRPO Configuration\n",
        "GRPO_CONFIG = {\n",
        "    \"num_generations\": 4,  # Number of rollouts per prompt\n",
        "    \"num_iterations\": 2,  # Training iterations/epochs\n",
        "    \"beta\": 0.04,  # KL penalty coefficient\n",
        "    \"epsilon\": 0.2,  # Clipping parameter\n",
        "    \"learning_rate\": 1e-5,  # Learning rate\n",
        "    \"batch_size\": 4,  # Batch size (adjust for TPU memory)\n",
        "    \"gradient_accumulation_steps\": 2,  # Gradient accumulation\n",
        "    \"eval_frequency\": 50,  # Evaluate every N steps\n",
        "    \"checkpoint_interval\": 100,  # Save checkpoint every N steps\n",
        "}\n",
        "\n",
        "print(\"\u2705 Configuration defined:\")\n",
        "print(\"\\n\ud83d\udd27 LoRA Configuration:\")\n",
        "for k, v in LORA_CONFIG.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "print(\"\\n\ud83c\udfaf GRPO Configuration:\")\n",
        "for k, v in GRPO_CONFIG.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Hyperparameter Rationale:\")\n",
        "print(\"   - LoRA rank=16: Balance between capacity and memory\")\n",
        "print(\"   - num_generations=4: Good variance reduction per Tunix defaults\")\n",
        "print(\"   - beta=0.04: Standard KL penalty to prevent policy divergence\")\n",
        "print(\"   - learning_rate=1e-5: Conservative for fine-tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \u26a0\ufe0f  Initialize Training Components\n",
        "\n",
        "**IMPORTANT: The cells below contain TEMPLATE CODE only.**\n",
        "\n",
        "The actual `google-tunix` library API may differ from this template. This notebook provides:\n",
        "- Conceptual structure for GRPO training\n",
        "- Typical patterns used in RL training\n",
        "- Placeholder code to be adapted\n",
        "\n",
        "**Before running training:**\n",
        "1. Verify `google-tunix` is publicly available\n",
        "2. Review official Tunix documentation and examples\n",
        "3. Update import statements and API calls to match actual library\n",
        "4. Test with small dataset first\n",
        "\n",
        "**Alternatives if Tunix is unavailable:**\n",
        "- Use [TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl) library\n",
        "- Adapt GRPO implementation from main Judicaita codebase (PyTorch)\n",
        "- Use other RL frameworks (RLlib, Stable-Baselines3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u26a0\ufe0f  IMPORTANT: TEMPLATE CODE - ADAPT TO ACTUAL TUNIX API\n",
        "# The code below is a template based on typical RL training patterns.\n",
        "# The actual google-tunix library API may differ significantly.\n",
        "# Refer to official Tunix documentation and examples to adapt this code.\n",
        "# \n",
        "# If google-tunix is not publicly available, consider using:\n",
        "# - TRL (Transformer Reinforcement Learning) library\n",
        "# - Custom GRPO implementation (see main codebase)\n",
        "# - Other RL frameworks like RLlib or SB3\n",
        "\n",
        "print(\"\ud83d\udce6 Setting up training components...\")\n",
        "\n",
        "# Create checkpoint directories\n",
        "import os\n",
        "CHECKPOINT_DIR = \"./checkpoints\"\n",
        "FINAL_DIR = \"./final_checkpoint\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\u2705 Checkpoint directories created:\")\n",
        "print(f\"   Intermediate: {CHECKPOINT_DIR}\")\n",
        "print(f\"   Final: {FINAL_DIR}\")\n",
        "\n",
        "# TEMPLATE: Typical Tunix-style setup (adapt to actual API)\n",
        "print(\"\\n\ud83d\udccb Training setup template (MUST BE ADAPTED):\")\n",
        "print(\"\"\"\n",
        "# Example imports (adapt based on actual library):\n",
        "# from tunix.rl import RLCluster, GRPOLearner, GRPOConfig\n",
        "# from tunix.models import load_model_with_lora\n",
        "# import jax\n",
        "\n",
        "# 1. Initialize model mesh for TPU sharding\n",
        "# mesh = jax.sharding.Mesh(...)\n",
        "\n",
        "# 2. Load base model and create LoRA config\n",
        "# base_model = load_model_with_lora(\n",
        "#     model_path=model_path,\n",
        "#     lora_config=LORA_CONFIG\n",
        "# )\n",
        "\n",
        "# 3. Create RLCluster with actor and reference policies\n",
        "# rl_cluster = RLCluster(\n",
        "#     actor_policy=base_model,  # Model being trained\n",
        "#     reference_policy=base_model,  # Frozen reference for KL penalty\n",
        "#     tokenizer=tokenizer\n",
        "# )\n",
        "\n",
        "# 4. Create GRPO learner\n",
        "# learner = GRPOLearner(\n",
        "#     rl_cluster=rl_cluster,\n",
        "#     reward_fn=composite_reward_function,\n",
        "#     config=GRPOConfig(**GRPO_CONFIG)\n",
        "# )\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\u26a0\ufe0f  ACTION REQUIRED:\")\n",
        "print(\"   1. Check if google-tunix is publicly available\")\n",
        "print(\"   2. Review actual Tunix API documentation\")\n",
        "print(\"   3. Replace template code with correct imports and calls\")\n",
        "print(\"   4. Test with a small dataset first\")\n",
        "print(\"\\n\ud83d\udca1 Alternative: Use TRL library or main codebase GRPO implementation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop template\n",
        "print(\"\ud83c\udfaf Training Loop Template:\")\n",
        "print(\"\"\"\n",
        "# Execute training within JAX mesh context\n",
        "# with mesh:\n",
        "#     for iteration in range(GRPO_CONFIG['num_iterations']):\n",
        "#         print(f\"\\n{'='*60}\")\n",
        "#         print(f\"Iteration {iteration + 1}/{GRPO_CONFIG['num_iterations']}\")\n",
        "#         print(f\"{'='*60}\")\n",
        "#         \n",
        "#         # Training step\n",
        "#         metrics = learner.train_step(\n",
        "#             dataset=training_dataset,\n",
        "#             batch_size=GRPO_CONFIG['batch_size']\n",
        "#         )\n",
        "#         \n",
        "#         # Log metrics\n",
        "#         print(f\"  Loss: {metrics.get('loss', 0):.4f}\")\n",
        "#         print(f\"  Avg Reward: {metrics.get('avg_reward', 0):.4f}\")\n",
        "#         print(f\"  Policy KL: {metrics.get('policy_kl', 0):.4f}\")\n",
        "#         \n",
        "#         # Sample outputs\n",
        "#         if iteration % GRPO_CONFIG['eval_frequency'] == 0:\n",
        "#             sample_output = learner.generate(\n",
        "#                 prompts=[training_dataset[0]['prompt']],\n",
        "#                 max_length=MAX_RESPONSE_LENGTH\n",
        "#             )\n",
        "#             print(f\"\\n  Sample output: {sample_output[0][:200]}...\")\n",
        "#         \n",
        "#         # Save checkpoint\n",
        "#         if iteration % GRPO_CONFIG['checkpoint_interval'] == 0:\n",
        "#             checkpoint_path = f\"{CHECKPOINT_DIR}/iter_{iteration}\"\n",
        "#             learner.save_checkpoint(checkpoint_path)\n",
        "#             print(f\"  \u2705 Checkpoint saved: {checkpoint_path}\")\n",
        "#     \n",
        "#     # Save final checkpoint\n",
        "#     learner.save_checkpoint(FINAL_DIR)\n",
        "#     print(f\"\\n\u2705 Training complete! Final checkpoint: {FINAL_DIR}\")\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Training Tips:\")\n",
        "print(\"   - Monitor loss and reward trends\")\n",
        "print(\"   - Check sample outputs for quality\")\n",
        "print(\"   - Save checkpoints frequently for recovery\")\n",
        "print(\"   - Adjust batch_size if OOM errors occur\")\n",
        "\n",
        "print(\"\\n\u26a0\ufe0f  Since actual Tunix API may differ, refer to:\")\n",
        "print(\"   - https://github.com/google/tunix (if available)\")\n",
        "print(\"   - Tunix documentation and examples\")\n",
        "print(\"   - Adapt the template above to match the actual API\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udce6 Task 5: Export LoRA Adapters and Create Kaggle Submission\n",
        "\n",
        "Package trained adapters for Kaggle submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create kaggle_upload directory\n",
        "KAGGLE_DIR = \"./kaggle_upload\"\n",
        "os.makedirs(KAGGLE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\u2705 Created Kaggle submission directory: {KAGGLE_DIR}\")\n",
        "print(\"\\n\ud83d\udccb Export checklist:\")\n",
        "print(\"   [ ] adapter_config.json - LoRA configuration\")\n",
        "print(\"   [ ] adapter_model.safetensors - LoRA weights\")\n",
        "print(\"   [ ] tokenizer files (if modified)\")\n",
        "print(\"   [ ] README with inference instructions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Template for exporting adapters\n",
        "print(\"\ud83d\udce6 Export Template (adapt to actual Tunix API):\")\n",
        "print(\"\"\"\n",
        "# Option 1: Export LoRA adapters separately\n",
        "# learner.export_lora_adapters(\n",
        "#     output_dir=KAGGLE_DIR,\n",
        "#     format='safetensors'\n",
        "# )\n",
        "\n",
        "# Option 2: Export merged model (if needed)\n",
        "# learner.export_merged_model(\n",
        "#     output_dir=KAGGLE_DIR,\n",
        "#     format='safetensors'\n",
        "# )\n",
        "\n",
        "# Copy tokenizer files\n",
        "# shutil.copytree(\n",
        "#     model_path,\n",
        "#     f\"{KAGGLE_DIR}/tokenizer\",\n",
        "#     dirs_exist_ok=True\n",
        "# )\n",
        "\"\"\")\n",
        "\n",
        "# Create adapter_config.json manually as example\n",
        "import json\n",
        "\n",
        "adapter_config = {\n",
        "    \"peft_type\": \"LORA\",\n",
        "    \"task_type\": \"CAUSAL_LM\",\n",
        "    \"r\": LORA_CONFIG[\"rank\"],\n",
        "    \"lora_alpha\": LORA_CONFIG[\"alpha\"],\n",
        "    \"lora_dropout\": LORA_CONFIG[\"dropout\"],\n",
        "    \"target_modules\": LORA_CONFIG[\"target_modules\"],\n",
        "    \"inference_mode\": False,\n",
        "    \"base_model_name_or_path\": MODEL_ID,\n",
        "}\n",
        "\n",
        "config_path = f\"{KAGGLE_DIR}/adapter_config.json\"\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(adapter_config, f, indent=2)\n",
        "\n",
        "print(f\"\\n\u2705 Created {config_path}\")\n",
        "print(\"\\n\ud83d\udcc4 adapter_config.json contents:\")\n",
        "print(json.dumps(adapter_config, indent=2))\n",
        "\n",
        "# Create README\n",
        "readme_content = \"\"\"# Judicaita GRPO-Trained LoRA Adapters\n",
        "\n",
        "## Model Information\n",
        "\n",
        "- Base Model: {model_id}\n",
        "- Training Method: GRPO (Group Relative Policy Optimization)\n",
        "- Framework: Google Tunix + JAX/Flax\n",
        "- LoRA Rank: {rank}\n",
        "- LoRA Alpha: {alpha}\n",
        "\n",
        "## Inference Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"{model_id}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{model_id}\")\n",
        "\n",
        "# Load LoRA adapters\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"./adapter_model\"  # Path to this directory\n",
        ")\n",
        "\n",
        "# Generate\n",
        "prompt = \"Your legal question here\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=512)\n",
        "response = tokenizer.decode(outputs[0])\n",
        "```\n",
        "\n",
        "## Expected Output Format\n",
        "\n",
        "The model generates responses in XML format:\n",
        "```xml\n",
        "<reasoning>\n",
        "Detailed legal reasoning with analysis...\n",
        "</reasoning>\n",
        "<answer>\n",
        "Final answer or conclusion\n",
        "</answer>\n",
        "```\n",
        "\n",
        "## Validation\n",
        "\n",
        "- Reasoning should be >= 100 tokens\n",
        "- Both XML tags must be present\n",
        "- Answer should be relevant to the question\n",
        "\"\"\".format(\n",
        "    model_id=MODEL_ID,\n",
        "    rank=LORA_CONFIG[\"rank\"],\n",
        "    alpha=LORA_CONFIG[\"alpha\"]\n",
        ")\n",
        "\n",
        "with open(f\"{KAGGLE_DIR}/README.md\", 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "print(f\"\\n\u2705 Created README.md with inference instructions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate Exported Model\n",
        "\n",
        "Test the exported adapters with inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference validation template\n",
        "print(\"\ud83e\uddea Inference Validation Template:\")\n",
        "print(\"\"\"\n",
        "# Load exported model\n",
        "# from transformers import AutoModelForCausalLM\n",
        "# from peft import PeftModel\n",
        "# \n",
        "# base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
        "# model = PeftModel.from_pretrained(base_model, KAGGLE_DIR)\n",
        "# \n",
        "# # Test prompts\n",
        "# test_prompts = [\n",
        "#     \"Is a verbal contract enforceable?\",\n",
        "#     \"What is required to prove negligence?\",\n",
        "# ]\n",
        "# \n",
        "# print(\"\\n\ud83d\udd0d Testing fine-tuned model...\")\n",
        "# for prompt in test_prompts:\n",
        "#     full_prompt = create_prompt_template(prompt)\n",
        "#     inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
        "#     \n",
        "#     outputs = model.generate(\n",
        "#         **inputs,\n",
        "#         max_length=MAX_RESPONSE_LENGTH,\n",
        "#         temperature=0.7,\n",
        "#         do_sample=True\n",
        "#     )\n",
        "#     \n",
        "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "#     \n",
        "#     # Validate format\n",
        "#     has_valid_format = validate_xml_format(response)\n",
        "#     reasoning, answer = extract_xml_content(response)\n",
        "#     \n",
        "#     if reasoning:\n",
        "#         reasoning_tokens = len(tokenizer(reasoning)[\"input_ids\"])\n",
        "#     else:\n",
        "#         reasoning_tokens = 0\n",
        "#     \n",
        "#     print(f\"\\n{'='*60}\")\n",
        "#     print(f\"Prompt: {prompt}\")\n",
        "#     print(f\"Valid format: {has_valid_format}\")\n",
        "#     print(f\"Reasoning tokens: {reasoning_tokens}\")\n",
        "#     print(f\"Response preview: {response[:200]}...\")\n",
        "#     \n",
        "#     # Compute reward\n",
        "#     reward = composite_reward_function(\n",
        "#         [full_prompt],\n",
        "#         [response],\n",
        "#         [{\"ground_truth\": \"test\"}]\n",
        "#     )[0]\n",
        "#     print(f\"Reward score: {reward:.2f}\")\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\u2705 Validation template ready\")\n",
        "print(\"   Run this to verify model quality before submission\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Create zip archive\n",
        "def create_submission_zip(source_dir: str, output_file: str):\n",
        "    \"\"\"\n",
        "    Create a zip archive for Kaggle submission.\n",
        "    \n",
        "    Args:\n",
        "        source_dir: Directory containing files to zip\n",
        "        output_file: Output zip file path\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(output_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(source_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, source_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "                print(f\"   Added: {arcname}\")\n",
        "    \n",
        "    # Get zip file size\n",
        "    size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
        "    return size_mb\n",
        "\n",
        "# Create submission\n",
        "submission_zip = \"./judicaita_submission.zip\"\n",
        "print(\"\ud83d\udce6 Creating Kaggle submission package...\")\n",
        "print(f\"   Source: {KAGGLE_DIR}\")\n",
        "print(f\"   Output: {submission_zip}\")\n",
        "print(\"\\n\ud83d\udcc4 Files included:\")\n",
        "\n",
        "try:\n",
        "    size = create_submission_zip(KAGGLE_DIR, submission_zip)\n",
        "    print(f\"\\n\u2705 Submission package created!\")\n",
        "    print(f\"   File: {submission_zip}\")\n",
        "    print(f\"   Size: {size:.2f} MB\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udccb Submission Checklist:\")\n",
        "    print(\"   \u2705 adapter_config.json\")\n",
        "    print(\"   \u2705 README.md with instructions\")\n",
        "    print(\"   \u26a0\ufe0f  adapter_model.safetensors (add after training)\")\n",
        "    print(\"   \u26a0\ufe0f  Validation results (add after testing)\")\n",
        "    \n",
        "    print(\"\\n\ud83c\udfaf Next Steps:\")\n",
        "    print(\"   1. Complete GRPO training\")\n",
        "    print(\"   2. Export adapter weights to kaggle_upload/\")\n",
        "    print(\"   3. Run inference validation\")\n",
        "    print(\"   4. Re-run this cell to create final zip\")\n",
        "    print(\"   5. Upload to Kaggle competition\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error creating zip: {e}\")\n",
        "    print(\"   Make sure kaggle_upload directory has content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udd27 Troubleshooting Guide\n",
        "\n",
        "#### Memory Errors\n",
        "- **Reduce batch_size**: Try 2 or 1\n",
        "- **Reduce num_generations**: Try 2 instead of 4\n",
        "- **Reduce max_length**: Lower MAX_PROMPT_LENGTH and MAX_RESPONSE_LENGTH\n",
        "- **Use smaller LoRA rank**: Try rank=8 instead of 16\n",
        "\n",
        "#### TPU Timeouts\n",
        "- **Reduce num_iterations**: Start with 1 iteration for testing\n",
        "- **Save checkpoints frequently**: Checkpoint every 50 steps\n",
        "- **Use gradient accumulation**: Increase gradient_accumulation_steps\n",
        "\n",
        "#### Checkpoint Corruption\n",
        "- **Keep multiple checkpoints**: Don't overwrite old checkpoints\n",
        "- **Verify after saving**: Load checkpoint immediately after saving\n",
        "- **Export early and often**: Export at multiple stages\n",
        "\n",
        "#### Low Rewards\n",
        "- **Check data quality**: Verify ground_truth accuracy\n",
        "- **Adjust reward weights**: Increase correctness_weight\n",
        "- **Increase training iterations**: More epochs may help\n",
        "- **Verify XML format**: Check model generates proper tags\n",
        "\n",
        "#### Export Issues\n",
        "- **Check file permissions**: Ensure write access to kaggle_upload/\n",
        "- **Verify adapter format**: Ensure safetensors format is used\n",
        "- **Test loading**: Try loading exported adapters before zipping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf89 Conclusion\n",
        "\n",
        "This notebook demonstrated:\n",
        "1. \u2705 TPU setup and Gemma 3 model loading\n",
        "2. \u2705 XML-formatted dataset preparation for legal reasoning\n",
        "3. \u2705 Custom reward function with format, length, and correctness components\n",
        "4. \u2705 GRPO training configuration with LoRA adapters\n",
        "5. \u2705 Export and Kaggle submission preparation\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Adapt Tunix API**: Update training code with actual google-tunix API\n",
        "2. **Load Real Data**: Replace synthetic examples with actual legal datasets\n",
        "3. **Run Training**: Execute GRPO training on TPU\n",
        "4. **Validate Outputs**: Test model quality and XML format compliance\n",
        "5. **Submit to Kaggle**: Upload trained adapters\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Judicaita Repository](https://github.com/clduab11/judicAIta)\n",
        "- [Google Tunix Documentation](https://github.com/google/tunix)\n",
        "- [Gemma Model Cards](https://ai.google.dev/gemma/docs)\n",
        "- [TPU Best Practices](https://cloud.google.com/tpu/docs/best-practices)\n",
        "\n",
        "### Feedback\n",
        "\n",
        "If you encounter issues or have improvements:\n",
        "- Open an issue: https://github.com/clduab11/judicAIta/issues\n",
        "- Submit a PR with fixes or enhancements\n",
        "\n",
        "---\n",
        "\n",
        "**Made with \u2764\ufe0f for the Kaggle hackathon and legal tech community**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}