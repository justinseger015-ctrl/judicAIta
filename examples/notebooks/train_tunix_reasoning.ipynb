{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clduab11/judicAIta/blob/main/examples/notebooks/train_tunix_reasoning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n32mE_hKL1eB"
      },
      "source": [
        "# Judicaita: GRPO Training with Google Tunix on TPU\n",
        "\n",
        "## üéØ Hackathon Context\n",
        "\n",
        "This notebook demonstrates **GRPO (Group Relative Policy Optimization)** training for the Judicaita legal AI assistant using:\n",
        "- **Google Tunix** for RL training infrastructure\n",
        "- **Gemma 3-1B-IT** as the base model\n",
        "- **TPU v2-8+** for accelerated training\n",
        "- **LoRA adapters** for parameter-efficient fine-tuning\n",
        "\n",
        "This is developed for the Kaggle hackathon to train models that generate explainable legal reasoning with structured XML-formatted outputs.\n",
        "\n",
        "## ‚ö° TPU Requirements\n",
        "\n",
        "**IMPORTANT**: This notebook requires:\n",
        "- Google Colab with TPU runtime (TPU v2-8 or higher)\n",
        "- Runtime type: TPU (not CPU or GPU)\n",
        "- To enable: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: TPU\n",
        "\n",
        "## üìã What This Notebook Does\n",
        "\n",
        "1. **Environment Setup**: Install Tunix, JAX, and dependencies for TPU\n",
        "2. **Model Loading**: Download and initialize Gemma 3-1B-IT with LoRA\n",
        "3. **Dataset Preparation**: Format training data with XML-tagged reasoning\n",
        "4. **Reward Function**: Score outputs based on format, reasoning length, and correctness\n",
        "5. **GRPO Training**: Train with `GRPOLearner` and `RLCluster` on TPU\n",
        "6. **Export**: Package trained LoRA adapters for Kaggle submission\n",
        "\n",
        "## üîÑ Data Flow\n",
        "\n",
        "```\n",
        "Dataset ‚Üí Prompts ‚Üí Model Rollouts ‚Üí Reward Scoring ‚Üí GRPO Updates\n",
        "                                                           ‚Üì\n",
        "                                              LoRA Adapter Checkpoints\n",
        "```\n",
        "\n",
        "## ‚ö†Ô∏è Differences from Main Codebase\n",
        "\n",
        "| Aspect | Main Codebase | This Notebook |\n",
        "|--------|---------------|---------------|\n",
        "| Format | Step-by-step format | XML `<reasoning>`/`<answer>` |\n",
        "| Framework | PyTorch | JAX/Flax |\n",
        "| Training | Custom GRPO | Tunix GRPOLearner |\n",
        "| Hardware | GPU/CPU | TPU v2-8+ |\n",
        "\n",
        "## üìö References\n",
        "\n",
        "- [Google Tunix Documentation](https://tunix.readthedocs.io/)\n",
        "- [Tunix GRPO Gemma Example](https://github.com/google/tunix/tree/main/examples/grpo_gemma)\n",
        "- [Gemma Model Card](https://ai.google.dev/gemma/docs)\n",
        "- [GRPO Paper](https://arxiv.org/abs/2402.03300)\n",
        "- [Judicaita Repository](https://github.com/clduab11/judicAIta)\n",
        "\n",
        "## ‚ö†Ô∏è Known Limitations\n",
        "\n",
        "- **TPU Required**: Cannot run on CPU/GPU without code modifications\n",
        "- **Memory**: TPU v2-8 has ~64GB; larger models may need v3 or higher\n",
        "- **Dataset**: Assumes generic legal reasoning tasks (not LegalBench-specific)\n",
        "- **Checkpoints**: Large checkpoint files may exceed Colab storage limits\n",
        "- **API Stability**: Tunix API may change; verify imports match your version\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwgwQnSXL1eD"
      },
      "source": [
        "## üì¶ Step 1: Install Dependencies\n",
        "\n",
        "Install required packages for TPU training with Tunix and Gemma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFytxSM9L1eD",
        "outputId": "42d44bf8-fa64-4965-e355-ca884e7a12c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: google-tunix 0.1.5 does not provide the extra 'tpu'\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "üì¶ Verifying package versions:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.8.2, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   JAX: 0.8.2\n",
            "   Flax: 0.12.2\n",
            "   Tunix: 0.1.5\n",
            "\n",
            "‚úÖ Dependencies installed successfully!\n",
            "‚ö†Ô∏è  IMPORTANT: Runtime restart required for TPU libraries.\n",
            "   Go to: Runtime ‚Üí Restart runtime\n",
            "   Then continue with the next cell.\n"
          ]
        }
      ],
      "source": [
        "# Install core dependencies\n",
        "# IMPORTANT: Google Colab TPU requires specific JAX/Flax versions for compatibility\n",
        "# See: https://tunix.readthedocs.io/en/latest/installation.html\n",
        "#\n",
        "# VERSION NOTES (as of December 2025):\n",
        "# - google-tunix max available version is 0.1.5 (not 0.5.0)\n",
        "# - JAX 0.4+ requires TPU VMs, which Colab does not provide\n",
        "# - Using jax[tpu] with official libtpu release feed ensures TPU compatibility\n",
        "# - jax_cuda12_plugin warnings are expected and harmless for TPU training\n",
        "\n",
        "# Install JAX with TPU support using official libtpu releases for Colab TPU compatibility\n",
        "!pip install -q \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "\n",
        "\n",
        "# IMPORTANT: Upgrade Flax to fix 'ModelAndOptimizer' compatibility issue\n",
        "# Colab's pre-installed Flax 0.10.2 is too old; tunix requires flax>=0.12.0\n",
        "!pip install -q \"flax>=0.12.0\"\n",
        "# Install Tunix with TPU support (max available version is 0.1.5)\n",
        "# This implicitly handles flax dependency, which requires flax>=0.12.0 via qwix\n",
        "# Note: If this still causes issues, there might be an unresolvable dependency conflict\n",
        "# between tunix's direct and indirect (via qwix) flax requirements for this tunix version.\n",
        "!pip install -q \"google-tunix[tpu]>=0.1.0,<=0.1.5\"\n",
        "\n",
        "# Install ML dependencies\n",
        "!pip install -q transformers>=4.40.0\n",
        "!pip install -q huggingface_hub>=0.20.0\n",
        "!pip install -q datasets>=2.14.0\n",
        "!pip install -q sentencepiece>=0.1.99\n",
        "!pip install -q safetensors>=0.4.0\n",
        "\n",
        "# Verify installation\n",
        "print(\"\\nüì¶ Verifying package versions:\")\n",
        "try:\n",
        "    import jax\n",
        "    print(f\"   JAX: {jax.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"   ‚ùå JAX import failed: {e}\")\n",
        "\n",
        "try:\n",
        "    import flax\n",
        "    print(f\"   Flax: {flax.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"   ‚ùå Flax import failed: {e}\")\n",
        "\n",
        "try:\n",
        "    import tunix\n",
        "    print(f\"   Tunix: {tunix.__version__ if hasattr(tunix, '__version__') else 'installed'}\")\n",
        "except ImportError as e:\n",
        "    print(f\"   ‚ö†Ô∏è Tunix import will be available after restart: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Dependencies installed successfully!\")\n",
        "print(\"‚ö†Ô∏è  IMPORTANT: Runtime restart required for TPU libraries.\")\n",
        "print(\"   Go to: Runtime ‚Üí Restart runtime\")\n",
        "print(\"   Then continue with the next cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5LS68YDL1eE"
      },
      "source": [
        "### üìù Version Compatibility Summary\n",
        "\n",
        "| Package | Version Range | Notes |\n",
        "|---------|---------------|-------|\n",
        "| google-tunix | 0.1.0 - 0.1.5 | Max available version is 0.1.5 (as of Dec 2025) |\n",
        "| JAX | TPU-compatible | Use `jax[tpu]` with libtpu releases for Colab TPU |\n",
        "| Flax | 0.10.2 | Compatible with JAX TPU builds |\n",
        "\n",
        "**Expected Warnings:**\n",
        "- `jax_cuda12_plugin` warnings are harmless for TPU training\n",
        "- These warnings appear because Colab environments may have GPU packages pre-installed\n",
        "\n",
        "**Important:** After running the install cell above, you must restart the runtime for TPU libraries to be properly loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vExNxM2sL1eE"
      },
      "source": [
        "### ‚ö†Ô∏è Runtime Restart Required\n",
        "\n",
        "**STOP HERE** and restart the runtime:\n",
        "1. Click `Runtime` ‚Üí `Restart runtime` in the menu\n",
        "2. After restart, continue from the next cell\n",
        "\n",
        "This is necessary for TPU libraries to be properly loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WvlgQSnL1eE"
      },
      "source": [
        "## üöÄ Step 2: Initialize TPU Runtime\n",
        "\n",
        "Set up JAX to use TPU devices and import core Tunix modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIUtOIfwL1eE"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.tools import colab_tpu\n",
        "from tunix.peft import lora as lora_lib\n",
        "\n",
        "# Initialize TPU\n",
        "print(\"Initializing TPU runtime...\")\n",
        "try:\n",
        "    colab_tpu.setup_tpu()\n",
        "    print(\"‚úÖ TPU initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå TPU initialization failed: {e}\")\n",
        "    print(\"\\nüîß Troubleshooting:\")\n",
        "    print(\"   1. Check runtime type: Runtime ‚Üí Change runtime type ‚Üí TPU\")\n",
        "    print(\"   2. Ensure TPU v2-8 or higher is available\")\n",
        "    print(\"   3. Try restarting the runtime\")\n",
        "    print(\"   4. Check Google Cloud TPU quota if using custom project\")\n",
        "    raise\n",
        "\n",
        "# Verify TPU devices\n",
        "devices = jax.devices()\n",
        "print(f\"\\nüìä TPU Device Information:\")\n",
        "print(f\"   Number of devices: {len(devices)}\")\n",
        "print(f\"   Device type: {devices[0].platform}\")\n",
        "print(f\"   Devices: {devices}\")\n",
        "\n",
        "if len(devices) == 0:\n",
        "    raise RuntimeError(\"No TPU devices detected! Please check your runtime configuration.\")\n",
        "\n",
        "print(\"\\n‚úÖ TPU setup complete and verified!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DsZY3GlL1eF"
      },
      "source": [
        "## üîê Step 3: Authenticate with Hugging Face\n",
        "\n",
        "Login to Hugging Face to download the Gemma model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6lw-SfPL1eF"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login, snapshot_download\n",
        "import os\n",
        "\n",
        "# Login to Hugging Face\n",
        "# You'll be prompted to enter your HF token\n",
        "# Get your token from: https://huggingface.co/settings/tokens\n",
        "print(\"Please enter your Hugging Face token:\")\n",
        "login()\n",
        "\n",
        "print(\"\\n‚úÖ Authenticated with Hugging Face!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZb1Gs9nL1eF"
      },
      "source": [
        "## üì• Step 4: Download Gemma 3-1B-IT Model\n",
        "\n",
        "Download the model files and initialize the tokenizer.\n",
        "\n",
        "**Note**: Using `gemma-3-1b-it` as it's the latest available Gemma instruction-tuned model. Update to `gemma-3-1b-it` if/when available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXYTtu15L1eF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "\n",
        "# Download model\n",
        "MODEL_ID = \"google/gemma-3-1b-it\"  # Using gemma-3-1b-it as gemma-3-1b-it may not be available yet\n",
        "CACHE_DIR = \"./gemma_model_cache\"\n",
        "\n",
        "print(f\"Downloading {MODEL_ID}...\")\n",
        "model_path = snapshot_download(\n",
        "    repo_id=MODEL_ID,\n",
        "    cache_dir=CACHE_DIR,\n",
        "    local_dir=f\"{CACHE_DIR}/gemma\",\n",
        "    local_dir_use_symlinks=False\n",
        ")\n",
        "print(f\"‚úÖ Model downloaded to: {model_path}\")\n",
        "\n",
        "# Initialize tokenizer\n",
        "print(\"\\nInitializing tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "print(f\"‚úÖ Tokenizer initialized\")\n",
        "print(f\"   Vocab size: {len(tokenizer)}\")\n",
        "print(f\"   Special tokens: {tokenizer.special_tokens_map}\")\n",
        "\n",
        "# Test tokenization\n",
        "test_text = \"What is the legal precedent for breach of contract?\"\n",
        "tokens = tokenizer(test_text, return_tensors=\"np\")\n",
        "print(f\"\\nüìù Test tokenization:\")\n",
        "print(f\"   Input: {test_text}\")\n",
        "print(f\"   Token count: {len(tokens['input_ids'][0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usIGOshsL1eF"
      },
      "source": [
        "## üîß Step 5: Create Preprocessing Function\n",
        "\n",
        "Gemma models don't have native system role support. We'll prepend the system prompt to the first user turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sNu7HH0L1eF"
      },
      "outputs": [],
      "source": [
        "def preprocess_with_system_prompt(messages, system_prompt):\n",
        "    \"\"\"\n",
        "    Prepend system prompt to first user message.\n",
        "\n",
        "    Gemma doesn't support system role natively, so we merge it with\n",
        "    the first user turn as a workaround.\n",
        "\n",
        "    Args:\n",
        "        messages: List of message dicts with 'role' and 'content'\n",
        "        system_prompt: System instruction string\n",
        "\n",
        "    Returns:\n",
        "        Modified messages list with system prompt prepended\n",
        "    \"\"\"\n",
        "    if not messages:\n",
        "        return messages\n",
        "\n",
        "    processed = messages.copy()\n",
        "\n",
        "    # Find first user message\n",
        "    for i, msg in enumerate(processed):\n",
        "        if msg.get('role') == 'user':\n",
        "            # Prepend system prompt\n",
        "            original_content = msg['content']\n",
        "            processed[i]['content'] = f\"{system_prompt}\\n\\n{original_content}\"\n",
        "            break\n",
        "\n",
        "    return processed\n",
        "\n",
        "# Define system prompt for legal reasoning\n",
        "SYSTEM_PROMPT = \"\"\"You are a legal AI assistant. For each question, provide your analysis in this exact format:\n",
        "<reasoning>Your step-by-step legal reasoning here. Include relevant legal principles, precedents, and analysis. Aim for at least 100 tokens of detailed reasoning.</reasoning>\n",
        "<answer>Your final answer or conclusion here.</answer>\n",
        "\n",
        "Always use this XML format and ensure your reasoning is thorough and well-explained.\"\"\"\n",
        "\n",
        "# Test preprocessing\n",
        "test_messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Is a non-compete clause enforceable in California?\"}\n",
        "]\n",
        "processed = preprocess_with_system_prompt(test_messages, SYSTEM_PROMPT)\n",
        "print(\"üìù Test preprocessing:\")\n",
        "print(f\"Original: {test_messages[0]['content'][:50]}...\")\n",
        "print(f\"\\nProcessed length: {len(processed[0]['content'])} chars\")\n",
        "print(f\"System prompt prepended: {'<reasoning>' in processed[0]['content']}\")\n",
        "print(\"\\n‚úÖ Preprocessing function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Y7D8ZDL1eF"
      },
      "source": [
        "## üìä Task 2: Prepare Training Dataset\n",
        "\n",
        "Create a dataset with XML-tagged reasoning format compatible with Tunix GRPO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQC6uBQRL1eF"
      },
      "source": [
        "### JSONL Format Requirements\n",
        "\n",
        "Each training example must be a JSON object with:\n",
        "- `prompt`: The question or task\n",
        "- `ground_truth`: The correct answer for evaluation\n",
        "- `metadata` (optional): Additional info like task_id, difficulty, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-jmuHzQL1eG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def prepare_dataset_for_tunix(examples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Prepare dataset in Tunix-compatible JSONL format.\n",
        "\n",
        "    Args:\n",
        "        examples: List of dicts with 'question' and 'answer' fields\n",
        "\n",
        "    Returns:\n",
        "        List of dicts with 'prompt', 'ground_truth', and 'metadata'\n",
        "    \"\"\"\n",
        "    prepared = []\n",
        "\n",
        "    for idx, ex in enumerate(examples):\n",
        "        prepared.append({\n",
        "            \"prompt\": ex.get(\"question\", ex.get(\"prompt\", \"\")),\n",
        "            \"ground_truth\": ex.get(\"answer\", ex.get(\"ground_truth\", \"\")),\n",
        "            \"metadata\": {\n",
        "                \"example_id\": idx,\n",
        "                \"original_question\": ex.get(\"question\", \"\"),\n",
        "                \"task_type\": ex.get(\"task_type\", \"general_reasoning\")\n",
        "            }\n",
        "        })\n",
        "\n",
        "    return prepared\n",
        "\n",
        "# Create synthetic sample data (replace with real data)\n",
        "sample_examples = [\n",
        "    {\n",
        "        \"question\": \"Can an employer in California enforce a non-compete clause against a former employee?\",\n",
        "        \"answer\": \"No, non-compete clauses are generally unenforceable in California except in limited circumstances involving sale of business or dissolution of partnership.\",\n",
        "        \"task_type\": \"legal_qa\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the statute of limitations for filing a breach of contract claim?\",\n",
        "        \"answer\": \"The statute of limitations varies by jurisdiction. In many states, it is 4-6 years for written contracts and 2-3 years for oral contracts.\",\n",
        "        \"task_type\": \"legal_qa\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Under what circumstances can a contract be voided for duress?\",\n",
        "        \"answer\": \"A contract can be voided for duress when one party was forced to enter the agreement through threats, violence, or other improper pressure that overcame their free will.\",\n",
        "        \"task_type\": \"legal_qa\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is required to establish an attorney-client privilege?\",\n",
        "        \"answer\": \"Attorney-client privilege requires: (1) an attorney-client relationship, (2) confidential communication, (3) made for the purpose of seeking or providing legal advice.\",\n",
        "        \"task_type\": \"legal_qa\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# Prepare dataset\n",
        "prepared_dataset = prepare_dataset_for_tunix(sample_examples)\n",
        "\n",
        "print(f\"‚úÖ Prepared {len(prepared_dataset)} training examples\")\n",
        "print(f\"\\nüìù Sample example:\")\n",
        "print(json.dumps(prepared_dataset[0], indent=2))\n",
        "\n",
        "# Note: In production, load from file or HuggingFace dataset\n",
        "print(\"\\nüí° To load from file:\")\n",
        "print(\"   # with open('data.jsonl', 'r') as f:\")\n",
        "print(\"   #     examples = [json.loads(line) for line in f]\")\n",
        "print(\"   #     prepared_dataset = prepare_dataset_for_tunix(examples)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGaUPGu5L1eG"
      },
      "source": [
        "### Prompt Template with XML Format\n",
        "\n",
        "Create a template that formats prompts to expect XML-tagged reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efXvIT8TL1eG"
      },
      "outputs": [],
      "source": [
        "def create_prompt_template(question: str, system_prompt: str = SYSTEM_PROMPT) -> str:\n",
        "    \"\"\"\n",
        "    Create a formatted prompt with XML output expectations.\n",
        "\n",
        "    Args:\n",
        "        question: The legal question to answer\n",
        "        system_prompt: System instructions for format\n",
        "\n",
        "    Returns:\n",
        "        Formatted prompt string\n",
        "    \"\"\"\n",
        "    template = f\"\"\"{system_prompt}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Response:\"\"\"\n",
        "    return template\n",
        "\n",
        "def validate_xml_format(response: str) -> bool:\n",
        "    \"\"\"\n",
        "    Validate that response contains proper XML tags.\n",
        "\n",
        "    Args:\n",
        "        response: Model generated response\n",
        "\n",
        "    Returns:\n",
        "        True if valid XML format, False otherwise\n",
        "    \"\"\"\n",
        "    # Check for both opening and closing tags\n",
        "    has_reasoning = '<reasoning>' in response and '</reasoning>' in response\n",
        "    has_answer = '<answer>' in response and '</answer>' in response\n",
        "\n",
        "    return has_reasoning and has_answer\n",
        "\n",
        "# Apply template to all examples\n",
        "templated_prompts = []\n",
        "for example in prepared_dataset:\n",
        "    templated = {\n",
        "        \"prompt\": create_prompt_template(example[\"prompt\"]),\n",
        "        \"ground_truth\": example[\"ground_truth\"],\n",
        "        \"metadata\": example[\"metadata\"],\n",
        "        \"original_prompt\": example[\"prompt\"]\n",
        "    }\n",
        "    templated_prompts.append(templated)\n",
        "\n",
        "print(f\"‚úÖ Created {len(templated_prompts)} templated prompts\")\n",
        "print(f\"\\nüìù Sample templated prompt (first 300 chars):\")\n",
        "print(templated_prompts[0][\"prompt\"][:300])\n",
        "print(\"...\")\n",
        "\n",
        "# Test validation\n",
        "test_valid = \"<reasoning>This is reasoning</reasoning><answer>This is answer</answer>\"\n",
        "test_invalid = \"This is just text without tags\"\n",
        "print(f\"\\n‚úÖ Validation test:\")\n",
        "print(f\"   Valid format: {validate_xml_format(test_valid)}\")\n",
        "print(f\"   Invalid format: {validate_xml_format(test_invalid)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgkWNF_EL1eG"
      },
      "source": [
        "### Tokenization and Batching\n",
        "\n",
        "Tokenize prompts and prepare batches for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AInVAlZZL1eG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "# Set maximum prompt length\n",
        "MAX_PROMPT_LENGTH = 512  # Adjust based on your needs (512 or 1024)\n",
        "MAX_RESPONSE_LENGTH = 512\n",
        "\n",
        "def tokenize_prompts(prompts: List[str], tokenizer, max_length: int = MAX_PROMPT_LENGTH):\n",
        "    \"\"\"\n",
        "    Tokenize prompts with padding and truncation.\n",
        "\n",
        "    Args:\n",
        "        prompts: List of prompt strings\n",
        "        tokenizer: HuggingFace tokenizer\n",
        "        max_length: Maximum token length\n",
        "\n",
        "    Returns:\n",
        "        Dict with input_ids and attention_mask\n",
        "    \"\"\"\n",
        "    tokenized = tokenizer(\n",
        "        prompts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"np\"\n",
        "    )\n",
        "    return tokenized\n",
        "\n",
        "def create_training_batches(dataset: List[Dict], batch_size: int = 4):\n",
        "    \"\"\"\n",
        "    Create batches from dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: List of training examples\n",
        "        batch_size: Number of examples per batch\n",
        "\n",
        "    Returns:\n",
        "        List of batches, each batch is a list of examples\n",
        "    \"\"\"\n",
        "    batches = []\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        batch = dataset[i:i + batch_size]\n",
        "        batches.append(batch)\n",
        "    return batches\n",
        "\n",
        "# Tokenize all prompts\n",
        "all_prompts = [ex[\"prompt\"] for ex in templated_prompts]\n",
        "tokenized_prompts = tokenize_prompts(all_prompts, tokenizer, MAX_PROMPT_LENGTH)\n",
        "\n",
        "print(f\"‚úÖ Tokenized {len(all_prompts)} prompts\")\n",
        "print(f\"   Max length: {MAX_PROMPT_LENGTH} tokens\")\n",
        "print(f\"   Shape: {tokenized_prompts['input_ids'].shape}\")\n",
        "\n",
        "# Create final dataset for training\n",
        "training_dataset = []\n",
        "for i, ex in enumerate(templated_prompts):\n",
        "    training_dataset.append({\n",
        "        \"prompt\": ex[\"prompt\"],\n",
        "        \"prompt_tokens\": tokenized_prompts['input_ids'][i],\n",
        "        \"attention_mask\": tokenized_prompts['attention_mask'][i],\n",
        "        \"ground_truth\": ex[\"ground_truth\"],\n",
        "        \"metadata\": ex[\"metadata\"]\n",
        "    })\n",
        "\n",
        "print(f\"\\n‚úÖ Final training dataset: {len(training_dataset)} examples\")\n",
        "print(f\"   Each example has: {list(training_dataset[0].keys())}\")\n",
        "\n",
        "# Validate dataset format\n",
        "required_fields = [\"prompt\", \"ground_truth\", \"metadata\"]\n",
        "all_valid = all(all(field in ex for field in required_fields) for ex in training_dataset)\n",
        "print(f\"\\n‚úÖ Dataset validation: {'PASSED' if all_valid else 'FAILED'}\")\n",
        "\n",
        "if not all_valid:\n",
        "    print(\"‚ùå Some examples missing required fields!\")\n",
        "else:\n",
        "    print(\"   All examples have required fields: prompt, ground_truth, metadata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMG3okLhL1eG"
      },
      "source": [
        "## üéØ Task 3: Implement Custom Reward Function\n",
        "\n",
        "Create a reward function that scores:\n",
        "1. **Format**: Proper XML tags\n",
        "2. **Reasoning Length**: At least 100 tokens\n",
        "3. **Answer Correctness**: Match with ground truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1NTvRVUL1eG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "def extract_xml_content(response: str) -> Tuple[Optional[str], Optional[str]]:\n",
        "    \"\"\"\n",
        "    Extract content from <reasoning> and <answer> XML tags.\n",
        "\n",
        "    Args:\n",
        "        response: Model-generated response string\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (reasoning_content, answer_content)\n",
        "        Returns (None, None) if tags are malformed or missing\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract reasoning\n",
        "        reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', response, re.DOTALL)\n",
        "        reasoning = reasoning_match.group(1).strip() if reasoning_match else None\n",
        "\n",
        "        # Extract answer\n",
        "        answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)\n",
        "        answer = answer_match.group(1).strip() if answer_match else None\n",
        "\n",
        "        return reasoning, answer\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Error extracting XML content: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Test extraction with edge cases\n",
        "test_cases = [\n",
        "    # Valid case\n",
        "    \"<reasoning>Step by step analysis here</reasoning><answer>Final answer</answer>\",\n",
        "    # Missing tags\n",
        "    \"Just plain text without tags\",\n",
        "    # Partial tags\n",
        "    \"<reasoning>Incomplete reasoning\",\n",
        "    # Nested content\n",
        "    \"<reasoning>Analysis with <term>nested</term> content</reasoning><answer>Yes</answer>\",\n",
        "    # Multi-line\n",
        "    \"\"\"<reasoning>\n",
        "Line 1 of reasoning\n",
        "Line 2 of reasoning\n",
        "</reasoning>\n",
        "<answer>Final answer</answer>\"\"\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing XML extraction:\")\n",
        "for i, test in enumerate(test_cases, 1):\n",
        "    reasoning, answer = extract_xml_content(test)\n",
        "    print(f\"\\nTest {i}:\")\n",
        "    print(f\"  Reasoning found: {reasoning is not None}\")\n",
        "    print(f\"  Answer found: {answer is not None}\")\n",
        "    if reasoning:\n",
        "        print(f\"  Reasoning preview: {reasoning[:50]}...\")\n",
        "    if answer:\n",
        "        print(f\"  Answer: {answer}\")\n",
        "\n",
        "print(\"\\n‚úÖ XML extraction function tested with edge cases\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzinMYgVL1eH"
      },
      "outputs": [],
      "source": [
        "def compute_format_reward(response: str) -> float:\n",
        "    \"\"\"\n",
        "    Reward for valid XML format.\n",
        "\n",
        "    Returns:\n",
        "        1.0 if both <reasoning> and <answer> tags present and valid\n",
        "        0.0 otherwise\n",
        "    \"\"\"\n",
        "    reasoning, answer = extract_xml_content(response)\n",
        "\n",
        "    # Check both tags present and have content\n",
        "    if reasoning is not None and answer is not None:\n",
        "        if len(reasoning.strip()) > 0 and len(answer.strip()) > 0:\n",
        "            return 1.0\n",
        "\n",
        "    return 0.0\n",
        "\n",
        "def compute_reasoning_length_reward(response: str, tokenizer, min_tokens: int = 100) -> float:\n",
        "    \"\"\"\n",
        "    Reward based on reasoning length.\n",
        "\n",
        "    Returns:\n",
        "        1.0 if reasoning >= min_tokens\n",
        "        proportional score (tokens/min_tokens) if fewer\n",
        "        0.0 if no reasoning found\n",
        "    \"\"\"\n",
        "    reasoning, _ = extract_xml_content(response)\n",
        "\n",
        "    if reasoning is None:\n",
        "        return 0.0\n",
        "\n",
        "    # Tokenize reasoning to count tokens\n",
        "    tokens = tokenizer(reasoning, return_tensors=\"np\")[\"input_ids\"]\n",
        "    num_tokens = len(tokens[0])\n",
        "\n",
        "    # Return 1.0 if meets threshold, otherwise proportional\n",
        "    if num_tokens >= min_tokens:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return num_tokens / min_tokens\n",
        "\n",
        "def compute_answer_correctness_reward(response: str, ground_truth: str, tokenizer) -> float:\n",
        "    \"\"\"\n",
        "    Reward based on answer correctness.\n",
        "\n",
        "    Returns:\n",
        "        1.0 for exact match (after normalization)\n",
        "        Partial credit for token overlap (Jaccard similarity)\n",
        "        0.0 if no answer found\n",
        "    \"\"\"\n",
        "    _, answer = extract_xml_content(response)\n",
        "\n",
        "    if answer is None:\n",
        "        return 0.0\n",
        "\n",
        "    # Normalize for comparison\n",
        "    answer_norm = answer.lower().strip()\n",
        "    ground_truth_norm = ground_truth.lower().strip()\n",
        "\n",
        "    # Check exact match\n",
        "    if answer_norm == ground_truth_norm:\n",
        "        return 1.0\n",
        "\n",
        "    # Tokenize both for overlap calculation\n",
        "    answer_tokens = set(tokenizer.tokenize(answer_norm))\n",
        "    truth_tokens = set(tokenizer.tokenize(ground_truth_norm))\n",
        "\n",
        "    # Calculate Jaccard similarity (intersection / union)\n",
        "    if len(answer_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    intersection = len(answer_tokens & truth_tokens)\n",
        "    union = len(answer_tokens | truth_tokens)\n",
        "\n",
        "    jaccard = intersection / union if union > 0 else 0.0\n",
        "\n",
        "    # Return Jaccard similarity as partial credit\n",
        "    return jaccard\n",
        "\n",
        "print(\"‚úÖ Reward component functions defined:\")\n",
        "print(\"   - compute_format_reward()\")\n",
        "print(\"   - compute_reasoning_length_reward()\")\n",
        "print(\"   - compute_answer_correctness_reward()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTDgkt1pL1eH"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def composite_reward_function(\n",
        "    prompts: List[str],\n",
        "    completions: List[str],\n",
        "    metadata: List[Dict],\n",
        "    tokenizer  # Required for token counting\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Composite reward function compatible with Tunix GRPO.\n",
        "\n",
        "    This function scores model outputs based on:\n",
        "    - Format reward (30%): Valid XML tags (<reasoning>/<answer>)\n",
        "    - Reasoning reward (30%): Sufficient reasoning length (>=100 tokens)\n",
        "    - Correctness reward (40%): Answer matches ground truth\n",
        "\n",
        "    Args:\n",
        "        prompts: List of input prompts\n",
        "        completions: List of model completions\n",
        "        metadata: List of metadata dicts with ground_truth\n",
        "        tokenizer: HuggingFace tokenizer for token counting\n",
        "\n",
        "    Returns:\n",
        "        List of scalar rewards (one per example)\n",
        "    \"\"\"\n",
        "    # Reward weights\n",
        "    FORMAT_WEIGHT = 0.3\n",
        "    REASONING_WEIGHT = 0.3\n",
        "    CORRECTNESS_WEIGHT = 0.4\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for i, (prompt, completion, meta) in enumerate(zip(prompts, completions, metadata)):\n",
        "        # Compute each reward component\n",
        "        format_reward = compute_format_reward(completion)\n",
        "        reasoning_reward = compute_reasoning_length_reward(completion, tokenizer, min_tokens=100)\n",
        "\n",
        "        # Get ground truth from metadata\n",
        "        ground_truth = meta.get(\"ground_truth\", \"\")\n",
        "        correctness_reward = compute_answer_correctness_reward(completion, ground_truth, tokenizer)\n",
        "\n",
        "        # Aggregate rewards\n",
        "        total_reward = (\n",
        "            FORMAT_WEIGHT * format_reward +\n",
        "            REASONING_WEIGHT * reasoning_reward +\n",
        "            CORRECTNESS_WEIGHT * correctness_reward\n",
        "        )\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        # Log breakdown for first few examples\n",
        "        if i < 3:\n",
        "            print(f\"\\nüìä Example {i} reward breakdown:\")\n",
        "            print(f\"   Format: {format_reward:.2f} (weight: {FORMAT_WEIGHT})\")\n",
        "            print(f\"   Reasoning: {reasoning_reward:.2f} (weight: {REASONING_WEIGHT})\")\n",
        "            print(f\"   Correctness: {correctness_reward:.2f} (weight: {CORRECTNESS_WEIGHT})\")\n",
        "            print(f\"   Total: {total_reward:.2f}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def tunix_reward_wrapper(prompts: List[str], outputs: List[str]) -> List[float]:\n",
        "    \"\"\"\n",
        "    Wrapper function matching Tunix RewardFn signature: (prompts, outputs) -> rewards.\n",
        "\n",
        "    This wrapper adapts our composite_reward_function to Tunix's expected signature.\n",
        "    It extracts metadata from the global training_dataset for ground truth comparison.\n",
        "\n",
        "    Args:\n",
        "        prompts: List of prompt strings\n",
        "        outputs: List of generated output strings\n",
        "\n",
        "    Returns:\n",
        "        List of float reward values\n",
        "    \"\"\"\n",
        "    # Build metadata from training dataset (prompts contain the original questions)\n",
        "    metadata = []\n",
        "    for prompt in prompts:\n",
        "        # Find matching ground truth from training_dataset\n",
        "        found = False\n",
        "        for example in training_dataset:\n",
        "            if example[\"prompt\"] in prompt or prompt in example[\"prompt\"]:\n",
        "                metadata.append({\"ground_truth\": example[\"ground_truth\"]})\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            metadata.append({\"ground_truth\": \"\"})\n",
        "\n",
        "    return composite_reward_function(prompts, outputs, metadata, tokenizer)\n",
        "\n",
        "\n",
        "# Test reward function\n",
        "print(\"üß™ Testing reward function...\")\n",
        "test_prompts = [\"Test question\"]\n",
        "test_completions = [\n",
        "    \"<reasoning>This is a detailed legal analysis with sufficient tokens to explain the reasoning behind the answer. We consider precedent, statutory law, and policy implications.</reasoning><answer>Yes, it is enforceable.</answer>\"\n",
        "]\n",
        "test_metadata = [{\"ground_truth\": \"Yes, it is enforceable.\"}]\n",
        "\n",
        "test_rewards = composite_reward_function(test_prompts, test_completions, test_metadata, tokenizer)\n",
        "print(f\"\\n‚úÖ Reward function test complete\")\n",
        "print(f\"   Test reward: {test_rewards[0]:.2f}\")\n",
        "print(\"\\n‚úÖ Composite reward function ready for Tunix GRPO!\")\n",
        "print(\"\\nüí° Use tunix_reward_wrapper() as the reward function for GRPOLearner\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLq5dwQuL1eH"
      },
      "outputs": [],
      "source": [
        "# Verify Tunix installation before training setup\n",
        "print(\"üì¶ Verifying Tunix installation...\")\n",
        "\n",
        "import sys\n",
        "\n",
        "# Check Tunix availability\n",
        "try:\n",
        "    import tunix\n",
        "    print(f\"‚úÖ Tunix installed: {tunix.__version__ if hasattr(tunix, '__version__') else 'version unknown'}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Tunix not available: {e}\")\n",
        "    print(\"\\nüîß To install Tunix:\")\n",
        "    print(\"   !pip install 'google-tunix[tpu]>=0.1.0'\")\n",
        "    print(\"   Then restart runtime and run this cell again.\")\n",
        "    raise\n",
        "\n",
        "# Check required submodules\n",
        "modules_to_check = [\n",
        "    (\"tunix.rl.grpo.grpo_learner\", \"GRPOConfig, GRPOLearner\"),\n",
        "    (\"tunix.rl.rl_cluster\", \"RLCluster\"),\n",
        "    (\"tunix.models.gemma\", \"GemmaForCausalLM\"),\n",
        "    (\"tunix.peft.lora\", \"LoRAConfig\"),\n",
        "]\n",
        "\n",
        "print(\"\\nüìã Checking Tunix submodules:\")\n",
        "all_available = True\n",
        "for module_path, expected_exports in modules_to_check:\n",
        "    try:\n",
        "        module = __import__(module_path, fromlist=[''])\n",
        "        print(f\"   ‚úÖ {module_path}\")\n",
        "    except ImportError as e:\n",
        "        print(f\"   ‚ùå {module_path}: {e}\")\n",
        "        all_available = False\n",
        "\n",
        "if all_available:\n",
        "    print(\"\\n‚úÖ All Tunix modules available!\")\n",
        "    print(\"\\nüí° Note: tunix.peft.lora is imported as lora_lib in Step 2 after TPU initialization.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Some modules not available. Check Tunix version and installation.\")\n",
        "    print(\"   The training cells may need adaptation for your Tunix version.\")\n",
        "\n",
        "# Check JAX backend\n",
        "print(\"\\nüìä JAX Backend Status:\")\n",
        "import jax\n",
        "print(f\"   JAX version: {jax.__version__}\")\n",
        "print(f\"   Backend: {jax.default_backend()}\")\n",
        "print(f\"   Devices: {jax.device_count()} ({jax.devices()[0].platform if jax.devices() else 'none'})\")\n",
        "\n",
        "print(\"\\n‚úÖ Environment verified - ready for training setup!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_MODf-BL1eH"
      },
      "source": [
        "## üöÄ Task 4: Configure and Execute GRPO Training\n",
        "\n",
        "Set up LoRA adapters and run GRPO training on TPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RxEzBSLL1eH"
      },
      "outputs": [],
      "source": [
        "# LoRA Hyperparameters for parameter-efficient fine-tuning\n",
        "LORA_CONFIG = {\n",
        "    \"rank\": 16,           # LoRA rank (16 or 32 recommended)\n",
        "    \"alpha\": 32,          # LoRA alpha (typically 2x rank)\n",
        "    \"dropout\": 0.05,      # LoRA dropout for regularization\n",
        "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
        "}\n",
        "\n",
        "# GRPO Configuration matching Tunix GRPOConfig parameters\n",
        "# Reference: https://tunix.readthedocs.io/en/latest/api/grpo.html\n",
        "GRPO_CONFIG = {\n",
        "    # Rollout settings\n",
        "    \"num_generations\": 4,           # Number of response samples per prompt for GRPO\n",
        "    \"max_tokens_to_generate\": 512,  # Maximum tokens for rollout generation\n",
        "\n",
        "    # GRPO algorithm hyperparameters\n",
        "    \"beta\": 0.04,                   # KL penalty coefficient (prevents policy divergence)\n",
        "    \"epsilon\": 0.2,                 # PPO-style clipping parameter\n",
        "\n",
        "    # Training settings\n",
        "    \"learning_rate\": 1e-5,          # Learning rate for LoRA parameters\n",
        "    \"batch_size\": 4,                # Batch size per TPU core (adjust for memory)\n",
        "    \"num_iterations\": 2,            # Number of training epochs/iterations\n",
        "\n",
        "    # Evaluation and checkpointing\n",
        "    \"eval_every_n_steps\": 50,       # Evaluate model every N steps\n",
        "    \"checkpoint_every_n_steps\": 100, # Save checkpoint every N steps\n",
        "}\n",
        "\n",
        "# Training configuration for RLCluster\n",
        "TRAINING_CONFIG = {\n",
        "    \"warmup_steps\": 10,             # Learning rate warmup steps\n",
        "    \"weight_decay\": 0.01,           # Weight decay for regularization\n",
        "    \"max_grad_norm\": 1.0,           # Gradient clipping threshold\n",
        "    \"log_every_n_steps\": 10,        # Log metrics every N steps\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Configuration defined:\")\n",
        "print(\"\\nüîß LoRA Configuration:\")\n",
        "for k, v in LORA_CONFIG.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "print(\"\\nüéØ GRPO Configuration:\")\n",
        "for k, v in GRPO_CONFIG.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "print(\"\\nüìä Training Configuration:\")\n",
        "for k, v in TRAINING_CONFIG.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "\n",
        "print(\"\\nüí° Hyperparameter Rationale:\")\n",
        "print(\"   - LoRA rank=16: Balance between capacity and memory efficiency\")\n",
        "print(\"   - num_generations=4: Standard for GRPO variance reduction\")\n",
        "print(\"   - beta=0.04: Conservative KL penalty to prevent policy divergence\")\n",
        "print(\"   - learning_rate=1e-5: Safe starting point for LoRA fine-tuning\")\n",
        "print(\"   - max_tokens_to_generate=512: Sufficient for detailed legal reasoning\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziCWkm-PL1eH"
      },
      "source": [
        "### üîß Initialize Training Components\n",
        "\n",
        "This section sets up the Tunix GRPO training infrastructure:\n",
        "\n",
        "1. **Import Tunix modules**: GRPOConfig, GRPOLearner, RLCluster\n",
        "2. **Load and configure models**: Actor (trainable) and Reference (frozen) policies\n",
        "3. **Setup TPU mesh**: Configure sharding for distributed training\n",
        "4. **Initialize learner**: Create GRPOLearner with reward function\n",
        "\n",
        "**Prerequisites**:\n",
        "- TPU runtime initialized (verified in Step 2)\n",
        "- Model downloaded (completed in Step 4)\n",
        "- Reward function defined (completed above)\n",
        "- Training dataset prepared (completed above)\n",
        "\n",
        "**Documentation**:\n",
        "- [Tunix GRPO Guide](https://tunix.readthedocs.io/en/latest/tutorials/grpo.html)\n",
        "- [Official GRPO Gemma Example](https://github.com/google/tunix/tree/main/examples/grpo_gemma)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9bHhh9kL1eH"
      },
      "outputs": [],
      "source": [
        "# Import Tunix GRPO modules\n",
        "print(\"üì¶ Importing Tunix modules...\")\n",
        "\n",
        "try:\n",
        "    from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
        "    from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "    from tunix.rl.rollout import base_rollout\n",
        "    from tunix.models import gemma as gemma_lib\n",
        "    print(\"‚úÖ Tunix modules imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Tunix import failed: {e}\")\n",
        "    print(\"\\nüîß Troubleshooting:\")\n",
        "    print(\"   1. Verify Tunix is installed: pip install google-tunix[tpu]\")\n",
        "    print(\"   2. Restart runtime after installation\")\n",
        "    print(\"   3. Check Tunix version compatibility\")\n",
        "    raise\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
        "import os\n",
        "\n",
        "# Create checkpoint directories\n",
        "CHECKPOINT_DIR = \"./checkpoints\"\n",
        "FINAL_DIR = \"./final_checkpoint\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Checkpoint directories created:\")\n",
        "print(f\"   Intermediate: {CHECKPOINT_DIR}\")\n",
        "print(f\"   Final: {FINAL_DIR}\")\n",
        "\n",
        "# Setup TPU mesh for distributed training\n",
        "print(\"\\nüîß Setting up TPU mesh...\")\n",
        "devices = jax.devices()\n",
        "num_devices = len(devices)\n",
        "\n",
        "# Create 1D mesh for data parallelism across TPU cores\n",
        "mesh = Mesh(devices, axis_names=(\"data\",))\n",
        "print(f\"‚úÖ TPU mesh created with {num_devices} devices\")\n",
        "print(f\"   Mesh shape: {mesh.shape}\")\n",
        "print(f\"   Axis names: {mesh.axis_names}\")\n",
        "\n",
        "# Load Gemma model for GRPO training\n",
        "print(\"\\nüì• Loading Gemma model for GRPO...\")\n",
        "\n",
        "# Create model configuration\n",
        "model_config = gemma_lib.GemmaConfig.from_pretrained(model_path)\n",
        "print(f\"   Model config loaded: {type(model_config).__name__}\")\n",
        "\n",
        "# Initialize actor model (trainable policy with LoRA)\n",
        "print(\"\\nüé≠ Initializing actor model (trainable)...\")\n",
        "actor_model = gemma_lib.GemmaForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    dtype=jnp.bfloat16,  # Use bfloat16 for TPU efficiency\n",
        ")\n",
        "\n",
        "# Apply LoRA configuration to actor model (lora_lib imported in Step 2)\n",
        "lora_config = lora_lib.LoRAConfig(\n",
        "    rank=LORA_CONFIG[\"rank\"],\n",
        "    alpha=LORA_CONFIG[\"alpha\"],\n",
        "    dropout=LORA_CONFIG[\"dropout\"],\n",
        "    target_modules=LORA_CONFIG[\"target_modules\"],\n",
        ")\n",
        "actor_model = lora_lib.apply_lora(actor_model, lora_config)\n",
        "print(f\"   LoRA applied: rank={LORA_CONFIG['rank']}, alpha={LORA_CONFIG['alpha']}\")\n",
        "\n",
        "# Initialize reference model (frozen copy for KL penalty)\n",
        "print(\"\\nüìã Initializing reference model (frozen)...\")\n",
        "reference_model = gemma_lib.GemmaForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    dtype=jnp.bfloat16,\n",
        ")\n",
        "# Reference model parameters are frozen (no gradients)\n",
        "print(\"   Reference model loaded (frozen for KL divergence)\")\n",
        "\n",
        "print(\"\\n‚úÖ Models initialized successfully!\")\n",
        "print(f\"   Actor model: LoRA-adapted, trainable\")\n",
        "print(f\"   Reference model: Frozen for KL penalty calculation\")\n",
        "\n",
        "# Create RLCluster configuration\n",
        "print(\"\\nüîß Creating RLCluster...\")\n",
        "\n",
        "# Define sharding specs for model parallelism\n",
        "data_sharding = NamedSharding(mesh, PartitionSpec(\"data\"))\n",
        "\n",
        "# Create RLCluster with actor and reference models\n",
        "rl_cluster = rl_cluster_lib.RLCluster(\n",
        "    actor_model=actor_model,\n",
        "    reference_model=reference_model,\n",
        "    tokenizer=tokenizer,\n",
        "    mesh=mesh,\n",
        "    data_sharding=data_sharding,\n",
        ")\n",
        "print(\"‚úÖ RLCluster created successfully!\")\n",
        "\n",
        "# Create GRPO configuration\n",
        "print(\"\\nüéØ Creating GRPOConfig...\")\n",
        "grpo_config = GRPOConfig(\n",
        "    num_generations=GRPO_CONFIG[\"num_generations\"],\n",
        "    max_tokens_to_generate=GRPO_CONFIG[\"max_tokens_to_generate\"],\n",
        "    beta=GRPO_CONFIG[\"beta\"],\n",
        "    epsilon=GRPO_CONFIG[\"epsilon\"],\n",
        "    learning_rate=GRPO_CONFIG[\"learning_rate\"],\n",
        "    warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
        "    weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n",
        "    max_grad_norm=TRAINING_CONFIG[\"max_grad_norm\"],\n",
        ")\n",
        "print(f\"‚úÖ GRPOConfig created:\")\n",
        "print(f\"   num_generations: {grpo_config.num_generations}\")\n",
        "print(f\"   max_tokens_to_generate: {grpo_config.max_tokens_to_generate}\")\n",
        "print(f\"   beta (KL penalty): {grpo_config.beta}\")\n",
        "print(f\"   learning_rate: {grpo_config.learning_rate}\")\n",
        "\n",
        "# Initialize GRPO Learner\n",
        "print(\"\\nüéì Initializing GRPOLearner...\")\n",
        "grpo_learner = GRPOLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    algo_config=grpo_config,\n",
        "    reward_fns=[tunix_reward_wrapper],  # Use our wrapped reward function\n",
        ")\n",
        "print(\"‚úÖ GRPOLearner initialized!\")\n",
        "print(\"   Reward function: tunix_reward_wrapper (composite XML/length/correctness)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ TRAINING SETUP COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nReady to execute GRPO training loop in the next cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqVM2k-LL1eH"
      },
      "outputs": [],
      "source": [
        "# Execute GRPO Training\n",
        "print(\"üéØ Starting GRPO Training...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Prepare training dataset in Tunix format\n",
        "print(\"\\nüìä Preparing training data...\")\n",
        "train_prompts = [ex[\"prompt\"] for ex in training_dataset]\n",
        "print(f\"   Training examples: {len(train_prompts)}\")\n",
        "\n",
        "# Training configuration\n",
        "num_iterations = GRPO_CONFIG[\"num_iterations\"]\n",
        "batch_size = GRPO_CONFIG[\"batch_size\"]\n",
        "eval_every = GRPO_CONFIG[\"eval_every_n_steps\"]\n",
        "checkpoint_every = GRPO_CONFIG[\"checkpoint_every_n_steps\"]\n",
        "log_every = TRAINING_CONFIG[\"log_every_n_steps\"]\n",
        "\n",
        "print(f\"\\nüìã Training Configuration:\")\n",
        "print(f\"   Iterations: {num_iterations}\")\n",
        "print(f\"   Batch size: {batch_size}\")\n",
        "print(f\"   Eval every: {eval_every} steps\")\n",
        "print(f\"   Checkpoint every: {checkpoint_every} steps\")\n",
        "\n",
        "# Training metrics storage\n",
        "training_metrics = {\n",
        "    \"losses\": [],\n",
        "    \"rewards\": [],\n",
        "    \"kl_divergences\": [],\n",
        "    \"steps\": [],\n",
        "}\n",
        "\n",
        "# Execute training\n",
        "start_time = time.time()\n",
        "global_step = 0\n",
        "\n",
        "try:\n",
        "    with mesh:\n",
        "        for iteration in range(num_iterations):\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"üìà Iteration {iteration + 1}/{num_iterations}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            iteration_start = time.time()\n",
        "\n",
        "            # Create batches for this iteration\n",
        "            num_batches = (len(train_prompts) + batch_size - 1) // batch_size\n",
        "\n",
        "            for batch_idx in range(num_batches):\n",
        "                # Get batch prompts\n",
        "                start_idx = batch_idx * batch_size\n",
        "                end_idx = min(start_idx + batch_size, len(train_prompts))\n",
        "                batch_prompts = train_prompts[start_idx:end_idx]\n",
        "\n",
        "                # Execute GRPO training step\n",
        "                step_metrics = grpo_learner.train_step(\n",
        "                    prompts=batch_prompts,\n",
        "                )\n",
        "\n",
        "                global_step += 1\n",
        "\n",
        "                # Store metrics\n",
        "                training_metrics[\"losses\"].append(step_metrics.get(\"loss\", 0.0))\n",
        "                training_metrics[\"rewards\"].append(step_metrics.get(\"mean_reward\", 0.0))\n",
        "                training_metrics[\"kl_divergences\"].append(step_metrics.get(\"kl_divergence\", 0.0))\n",
        "                training_metrics[\"steps\"].append(global_step)\n",
        "\n",
        "                # Log progress\n",
        "                if global_step % log_every == 0:\n",
        "                    print(f\"\\n   Step {global_step}:\")\n",
        "                    print(f\"      Loss: {step_metrics.get('loss', 0.0):.4f}\")\n",
        "                    print(f\"      Mean Reward: {step_metrics.get('mean_reward', 0.0):.4f}\")\n",
        "                    print(f\"      KL Divergence: {step_metrics.get('kl_divergence', 0.0):.4f}\")\n",
        "\n",
        "                # Evaluation\n",
        "                if global_step % eval_every == 0:\n",
        "                    print(f\"\\n   üìä Evaluation at step {global_step}:\")\n",
        "                    # Generate sample output\n",
        "                    sample_prompt = train_prompts[0]\n",
        "                    sample_output = grpo_learner.generate(\n",
        "                        prompts=[sample_prompt],\n",
        "                        max_tokens=GRPO_CONFIG[\"max_tokens_to_generate\"],\n",
        "                    )[0]\n",
        "\n",
        "                    # Validate output format\n",
        "                    has_format = validate_xml_format(sample_output)\n",
        "                    reasoning, answer = extract_xml_content(sample_output)\n",
        "\n",
        "                    print(f\"      Valid XML format: {has_format}\")\n",
        "                    if reasoning:\n",
        "                        reasoning_tokens = len(tokenizer.encode(reasoning))\n",
        "                        print(f\"      Reasoning tokens: {reasoning_tokens}\")\n",
        "                    print(f\"      Sample output preview: {sample_output[:200]}...\")\n",
        "\n",
        "                # Checkpoint\n",
        "                if global_step % checkpoint_every == 0:\n",
        "                    checkpoint_path = f\"{CHECKPOINT_DIR}/step_{global_step}\"\n",
        "                    grpo_learner.save_checkpoint(checkpoint_path)\n",
        "                    print(f\"\\n   üíæ Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "            iteration_time = time.time() - iteration_start\n",
        "            print(f\"\\n   ‚è±Ô∏è Iteration {iteration + 1} completed in {iteration_time:.1f}s\")\n",
        "\n",
        "            # Iteration summary\n",
        "            recent_losses = training_metrics[\"losses\"][-num_batches:]\n",
        "            recent_rewards = training_metrics[\"rewards\"][-num_batches:]\n",
        "            print(f\"   üìä Iteration Summary:\")\n",
        "            print(f\"      Avg Loss: {sum(recent_losses)/len(recent_losses):.4f}\")\n",
        "            print(f\"      Avg Reward: {sum(recent_rewards)/len(recent_rewards):.4f}\")\n",
        "\n",
        "    # Training complete\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"   Total steps: {global_step}\")\n",
        "    print(f\"   Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
        "    print(f\"   Final avg loss: {sum(training_metrics['losses'][-10:])/10:.4f}\")\n",
        "    print(f\"   Final avg reward: {sum(training_metrics['rewards'][-10:])/10:.4f}\")\n",
        "\n",
        "    # Save final checkpoint\n",
        "    print(f\"\\nüíæ Saving final checkpoint to {FINAL_DIR}...\")\n",
        "    grpo_learner.save_checkpoint(FINAL_DIR)\n",
        "    print(\"‚úÖ Final checkpoint saved!\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è Training interrupted by user!\")\n",
        "    print(f\"   Completed steps: {global_step}\")\n",
        "    # Save emergency checkpoint\n",
        "    emergency_path = f\"{CHECKPOINT_DIR}/interrupted_step_{global_step}\"\n",
        "    grpo_learner.save_checkpoint(emergency_path)\n",
        "    print(f\"   Emergency checkpoint saved: {emergency_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training error: {e}\")\n",
        "    print(f\"   Last completed step: {global_step}\")\n",
        "    # Try to save checkpoint on error\n",
        "    try:\n",
        "        error_path = f\"{CHECKPOINT_DIR}/error_step_{global_step}\"\n",
        "        grpo_learner.save_checkpoint(error_path)\n",
        "        print(f\"   Error checkpoint saved: {error_path}\")\n",
        "    except:\n",
        "        print(\"   Could not save error checkpoint\")\n",
        "    raise\n",
        "\n",
        "# Display training summary plot\n",
        "print(\"\\nüìä Training Metrics Summary:\")\n",
        "print(f\"   Steps: {len(training_metrics['steps'])}\")\n",
        "print(f\"   Loss range: {min(training_metrics['losses']):.4f} - {max(training_metrics['losses']):.4f}\")\n",
        "print(f\"   Reward range: {min(training_metrics['rewards']):.4f} - {max(training_metrics['rewards']):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tJc7--0L1eH"
      },
      "source": [
        "## üì¶ Task 5: Export LoRA Adapters and Create Kaggle Submission\n",
        "\n",
        "Package trained adapters for Kaggle submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yopqXC2zL1eI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create kaggle_upload directory\n",
        "KAGGLE_DIR = \"./kaggle_upload\"\n",
        "os.makedirs(KAGGLE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Created Kaggle submission directory: {KAGGLE_DIR}\")\n",
        "print(\"\\nüìã Export checklist:\")\n",
        "print(\"   [ ] adapter_config.json - LoRA configuration\")\n",
        "print(\"   [ ] adapter_model.safetensors - LoRA weights\")\n",
        "print(\"   [ ] tokenizer files (if modified)\")\n",
        "print(\"   [ ] README with inference instructions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5rH7ZDzL1eI"
      },
      "outputs": [],
      "source": [
        "# Export LoRA Adapters using Tunix API\n",
        "print(\"üì¶ Exporting LoRA adapters...\")\n",
        "\n",
        "import json\n",
        "import shutil\n",
        "from safetensors.flax import save_file as save_safetensors\n",
        "\n",
        "# Export LoRA weights from trained model\n",
        "print(\"\\nüì§ Extracting LoRA weights from actor model...\")\n",
        "\n",
        "try:\n",
        "    # Method 1: Use Tunix's built-in export (preferred)\n",
        "    grpo_learner.export_lora_adapters(\n",
        "        output_dir=KAGGLE_DIR,\n",
        "        format=\"safetensors\"\n",
        "    )\n",
        "    print(\"‚úÖ LoRA adapters exported using Tunix API\")\n",
        "\n",
        "except AttributeError:\n",
        "    # Method 2: Manual extraction if export method not available\n",
        "    # lora_lib was imported in Step 2 after TPU initialization\n",
        "    print(\"   Using manual extraction method...\")\n",
        "\n",
        "    # Extract LoRA weights\n",
        "    lora_weights = lora_lib.extract_lora_weights(actor_model)\n",
        "\n",
        "    # Save in safetensors format\n",
        "    adapter_path = f\"{KAGGLE_DIR}/adapter_model.safetensors\"\n",
        "    save_safetensors(lora_weights, adapter_path)\n",
        "    print(f\"‚úÖ LoRA weights saved: {adapter_path}\")\n",
        "\n",
        "# Create adapter_config.json\n",
        "adapter_config = {\n",
        "    \"peft_type\": \"LORA\",\n",
        "    \"task_type\": \"CAUSAL_LM\",\n",
        "    \"r\": LORA_CONFIG[\"rank\"],\n",
        "    \"lora_alpha\": LORA_CONFIG[\"alpha\"],\n",
        "    \"lora_dropout\": LORA_CONFIG[\"dropout\"],\n",
        "    \"target_modules\": LORA_CONFIG[\"target_modules\"],\n",
        "    \"inference_mode\": True,\n",
        "    \"base_model_name_or_path\": MODEL_ID,\n",
        "    \"bias\": \"none\",\n",
        "    \"fan_in_fan_out\": False,\n",
        "}\n",
        "\n",
        "config_path = f\"{KAGGLE_DIR}/adapter_config.json\"\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(adapter_config, f, indent=2)\n",
        "print(f\"‚úÖ Config saved: {config_path}\")\n",
        "\n",
        "# Copy tokenizer files\n",
        "print(\"\\nüìÅ Copying tokenizer files...\")\n",
        "tokenizer_files = [\"tokenizer.json\", \"tokenizer_config.json\", \"special_tokens_map.json\"]\n",
        "for fname in tokenizer_files:\n",
        "    src = f\"{model_path}/{fname}\"\n",
        "    dst = f\"{KAGGLE_DIR}/{fname}\"\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\"   Copied: {fname}\")\n",
        "\n",
        "# Create README\n",
        "readme_content = f\"\"\"# Judicaita GRPO-Trained LoRA Adapters\n",
        "\n",
        "## Model Information\n",
        "\n",
        "- **Base Model**: {MODEL_ID}\n",
        "- **Training Method**: GRPO (Group Relative Policy Optimization)\n",
        "- **Framework**: Google Tunix + JAX/Flax\n",
        "- **LoRA Rank**: {LORA_CONFIG[\"rank\"]}\n",
        "- **LoRA Alpha**: {LORA_CONFIG[\"alpha\"]}\n",
        "- **Training Platform**: Google Colab TPU\n",
        "\n",
        "## Inference Usage\n",
        "\n",
        "### With Transformers + PEFT\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"{MODEL_ID}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{MODEL_ID}\")\n",
        "\n",
        "# Load LoRA adapters\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"./adapter_model\"  # Path to this directory\n",
        ")\n",
        "\n",
        "# Generate\n",
        "prompt = \"Your legal question here\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=512)\n",
        "response = tokenizer.decode(outputs[0])\n",
        "```\n",
        "\n",
        "### Expected Output Format\n",
        "\n",
        "The model generates responses in XML format:\n",
        "```xml\n",
        "<reasoning>\n",
        "Detailed legal reasoning with analysis...\n",
        "</reasoning>\n",
        "<answer>\n",
        "Final answer or conclusion\n",
        "</answer>\n",
        "```\n",
        "\n",
        "## Training Details\n",
        "\n",
        "- **Reward Function**: Composite (30% format + 30% length + 40% correctness)\n",
        "- **GRPO Beta (KL penalty)**: {GRPO_CONFIG[\"beta\"]}\n",
        "- **Num Generations**: {GRPO_CONFIG[\"num_generations\"]}\n",
        "- **Learning Rate**: {GRPO_CONFIG[\"learning_rate\"]}\n",
        "\n",
        "## Validation Criteria\n",
        "\n",
        "- Reasoning should be >= 100 tokens\n",
        "- Both XML tags must be present\n",
        "- Answer should be relevant to the question\n",
        "\n",
        "## License\n",
        "\n",
        "Same as base model ({MODEL_ID})\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{KAGGLE_DIR}/README.md\", 'w') as f:\n",
        "    f.write(readme_content)\n",
        "print(\"‚úÖ README.md created\")\n",
        "\n",
        "# List exported files\n",
        "print(\"\\nüìã Exported files:\")\n",
        "for item in os.listdir(KAGGLE_DIR):\n",
        "    item_path = os.path.join(KAGGLE_DIR, item)\n",
        "    size = os.path.getsize(item_path) if os.path.isfile(item_path) else 0\n",
        "    print(f\"   {item}: {size/1024:.1f} KB\" if size > 0 else f\"   {item}/\")\n",
        "\n",
        "print(\"\\n‚úÖ Export complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2eOl8I3L1eI"
      },
      "source": [
        "### Validate Exported Model\n",
        "\n",
        "Test the exported adapters with inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1MX-caUL1eI"
      },
      "outputs": [],
      "source": [
        "# Validate Exported Model with Inference\n",
        "print(\"üß™ Running Inference Validation...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test prompts for validation\n",
        "test_prompts = [\n",
        "    \"Is a verbal contract enforceable in most jurisdictions?\",\n",
        "    \"What are the elements required to prove negligence?\",\n",
        "    \"Can a contract be voided if one party was under duress?\",\n",
        "]\n",
        "\n",
        "print(\"\\nüìù Test Prompts:\")\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"   {i}. {prompt}\")\n",
        "\n",
        "# Generate responses using trained model\n",
        "print(\"\\nüîÑ Generating responses with trained model...\")\n",
        "\n",
        "validation_results = []\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    # Create full prompt with system instructions\n",
        "    full_prompt = create_prompt_template(prompt)\n",
        "\n",
        "    # Generate response\n",
        "    try:\n",
        "        response = grpo_learner.generate(\n",
        "            prompts=[full_prompt],\n",
        "            max_tokens=GRPO_CONFIG[\"max_tokens_to_generate\"],\n",
        "            temperature=0.7,\n",
        "        )[0]\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Generation error for prompt {i+1}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Validate format\n",
        "    has_valid_format = validate_xml_format(response)\n",
        "    reasoning, answer = extract_xml_content(response)\n",
        "\n",
        "    # Count reasoning tokens\n",
        "    reasoning_tokens = 0\n",
        "    if reasoning:\n",
        "        reasoning_tokens = len(tokenizer.encode(reasoning))\n",
        "\n",
        "    # Compute reward\n",
        "    reward = composite_reward_function(\n",
        "        [full_prompt],\n",
        "        [response],\n",
        "        [{\"ground_truth\": \"\"}],  # No ground truth for test prompts\n",
        "        tokenizer\n",
        "    )[0]\n",
        "\n",
        "    result = {\n",
        "        \"prompt\": prompt,\n",
        "        \"response\": response,\n",
        "        \"valid_format\": has_valid_format,\n",
        "        \"reasoning_tokens\": reasoning_tokens,\n",
        "        \"has_reasoning\": reasoning is not None,\n",
        "        \"has_answer\": answer is not None,\n",
        "        \"reward\": reward,\n",
        "    }\n",
        "    validation_results.append(result)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìã Test {i+1}: {prompt[:50]}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"   ‚úì Valid XML format: {has_valid_format}\")\n",
        "    print(f\"   ‚úì Reasoning tokens: {reasoning_tokens}\")\n",
        "    print(f\"   ‚úì Has reasoning: {reasoning is not None}\")\n",
        "    print(f\"   ‚úì Has answer: {answer is not None}\")\n",
        "    print(f\"   ‚úì Reward score: {reward:.3f}\")\n",
        "\n",
        "    if reasoning:\n",
        "        print(f\"\\n   üìù Reasoning preview:\")\n",
        "        print(f\"      {reasoning[:200]}...\")\n",
        "    if answer:\n",
        "        print(f\"\\n   üí° Answer:\")\n",
        "        print(f\"      {answer[:200]}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä VALIDATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "valid_count = sum(1 for r in validation_results if r[\"valid_format\"])\n",
        "avg_reasoning_tokens = sum(r[\"reasoning_tokens\"] for r in validation_results) / len(validation_results) if validation_results else 0\n",
        "avg_reward = sum(r[\"reward\"] for r in validation_results) / len(validation_results) if validation_results else 0\n",
        "\n",
        "print(f\"   Total test prompts: {len(test_prompts)}\")\n",
        "print(f\"   Valid XML format: {valid_count}/{len(validation_results)} ({100*valid_count/len(validation_results):.0f}%)\" if validation_results else \"   No results\")\n",
        "print(f\"   Avg reasoning tokens: {avg_reasoning_tokens:.0f}\")\n",
        "print(f\"   Avg reward score: {avg_reward:.3f}\")\n",
        "\n",
        "# Quality assessment\n",
        "print(\"\\nüìà Quality Assessment:\")\n",
        "if avg_reward >= 0.7:\n",
        "    print(\"   ‚úÖ EXCELLENT: Model produces high-quality legal reasoning\")\n",
        "elif avg_reward >= 0.5:\n",
        "    print(\"   ‚úÖ GOOD: Model produces adequate legal reasoning\")\n",
        "elif avg_reward >= 0.3:\n",
        "    print(\"   ‚ö†Ô∏è FAIR: Model needs more training for better quality\")\n",
        "else:\n",
        "    print(\"   ‚ùå POOR: Model requires significant improvement\")\n",
        "\n",
        "if valid_count == len(validation_results) and validation_results:\n",
        "    print(\"   ‚úÖ All outputs have valid XML format\")\n",
        "elif valid_count > 0:\n",
        "    print(f\"   ‚ö†Ô∏è Some outputs missing proper XML tags ({len(validation_results) - valid_count} invalid)\")\n",
        "else:\n",
        "    print(\"   ‚ùå No outputs have valid XML format - check training\")\n",
        "\n",
        "print(\"\\n‚úÖ Validation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX3n-IzVL1eI"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Create zip archive\n",
        "def create_submission_zip(source_dir: str, output_file: str):\n",
        "    \"\"\"\n",
        "    Create a zip archive for Kaggle submission.\n",
        "\n",
        "    Args:\n",
        "        source_dir: Directory containing files to zip\n",
        "        output_file: Output zip file path\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(output_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(source_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, source_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "                print(f\"   Added: {arcname}\")\n",
        "\n",
        "    # Get zip file size\n",
        "    size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
        "    return size_mb\n",
        "\n",
        "# Create submission\n",
        "submission_zip = \"./judicaita_submission.zip\"\n",
        "print(\"üì¶ Creating Kaggle submission package...\")\n",
        "print(f\"   Source: {KAGGLE_DIR}\")\n",
        "print(f\"   Output: {submission_zip}\")\n",
        "print(\"\\nüìÑ Files included:\")\n",
        "\n",
        "try:\n",
        "    size = create_submission_zip(KAGGLE_DIR, submission_zip)\n",
        "    print(f\"\\n‚úÖ Submission package created!\")\n",
        "    print(f\"   File: {submission_zip}\")\n",
        "    print(f\"   Size: {size:.2f} MB\")\n",
        "\n",
        "    print(\"\\nüìã Submission Checklist:\")\n",
        "    print(\"   ‚úÖ adapter_config.json\")\n",
        "    print(\"   ‚úÖ README.md with instructions\")\n",
        "    print(\"   ‚ö†Ô∏è  adapter_model.safetensors (add after training)\")\n",
        "    print(\"   ‚ö†Ô∏è  Validation results (add after testing)\")\n",
        "\n",
        "    print(\"\\nüéØ Next Steps:\")\n",
        "    print(\"   1. Complete GRPO training\")\n",
        "    print(\"   2. Export adapter weights to kaggle_upload/\")\n",
        "    print(\"   3. Run inference validation\")\n",
        "    print(\"   4. Re-run this cell to create final zip\")\n",
        "    print(\"   5. Upload to Kaggle competition\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating zip: {e}\")\n",
        "    print(\"   Make sure kaggle_upload directory has content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00EHM5U8L1eI"
      },
      "source": [
        "### üîß Troubleshooting Guide\n",
        "\n",
        "#### Tunix Import Errors\n",
        "- **ModuleNotFoundError: No module named 'tunix'**\n",
        "  - Ensure you installed with TPU extras: `pip install \"google-tunix[tpu]>=0.1.0,<=0.1.5\"`\n",
        "  - Restart runtime after installation\n",
        "  - Verify version: `python -c \"import tunix; print(tunix.__version__)\"`\n",
        "\n",
        "- **ImportError: cannot import name 'GRPOLearner'**\n",
        "  - Check Tunix version >= 0.1.0 (max available: 0.1.5)\n",
        "  - Verify correct import path: `from tunix.rl.grpo.grpo_learner import GRPOLearner`\n",
        "  - Note: API may vary between versions; check Tunix documentation for your version\n",
        "\n",
        "#### JAX/TPU Initialization Issues\n",
        "- **RuntimeError: TPU not found**\n",
        "  - Verify Colab runtime is set to TPU: Runtime ‚Üí Change runtime type ‚Üí TPU\n",
        "  - Try restarting the runtime completely\n",
        "  - Check TPU quota in Google Cloud Console if using custom project\n",
        "\n",
        "- **JAX version mismatch errors**\n",
        "  - Install JAX with TPU support: `pip install \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html`\n",
        "  - JAX 0.4+ requires TPU VMs and is NOT supported on Colab TPU\n",
        "  - Restart runtime after JAX installation\n",
        "  - Verify: `python -c \"import jax; print(jax.__version__, jax.devices())\"`\n",
        "\n",
        "- **jax_cuda12_plugin warnings**\n",
        "  - These warnings are expected and harmless for TPU training\n",
        "  - They appear because Colab environments may have GPU packages pre-installed\n",
        "  - You can safely ignore them when using TPU runtime\n",
        "\n",
        "#### RLCluster Configuration Errors\n",
        "- **ValueError: Mesh shape mismatch**\n",
        "  - Ensure mesh is created with correct number of devices\n",
        "  - Check `len(jax.devices())` matches expected TPU cores\n",
        "  - For TPU v2-8, expect 8 devices\n",
        "\n",
        "- **Sharding errors during training**\n",
        "  - Verify data_sharding is compatible with batch size\n",
        "  - Reduce batch_size to 1 or 2 for debugging\n",
        "  - Check model dtype is bfloat16 for TPU\n",
        "\n",
        "#### Memory Errors (OOM)\n",
        "- **Out of Memory during rollout generation**\n",
        "  - Reduce `num_generations` from 4 to 2\n",
        "  - Reduce `max_tokens_to_generate` from 512 to 256\n",
        "  - Reduce `batch_size` from 4 to 2 or 1\n",
        "\n",
        "- **Out of Memory during backward pass**\n",
        "  - Use smaller LoRA rank: try rank=8 instead of 16\n",
        "  - Enable gradient checkpointing if available\n",
        "  - Reduce sequence length\n",
        "\n",
        "#### Reward Function Issues\n",
        "- **Reward function signature mismatch**\n",
        "  - Tunix expects `reward_fn(prompts: List[str], outputs: List[str]) -> List[float]`\n",
        "  - Use `tunix_reward_wrapper` instead of `composite_reward_function` directly\n",
        "  - Ensure function returns Python list of floats, not numpy/jax arrays\n",
        "\n",
        "- **All rewards are 0.0**\n",
        "  - Check if model is generating XML tags properly\n",
        "  - Verify `extract_xml_content()` is working correctly\n",
        "  - Test reward function manually with sample outputs\n",
        "\n",
        "#### Checkpoint Issues\n",
        "- **Checkpoint save fails**\n",
        "  - Ensure checkpoint directory exists and is writable\n",
        "  - Check disk space (Colab has ~100GB limit)\n",
        "  - For large models, consider saving to Google Drive\n",
        "\n",
        "- **Checkpoint load fails**\n",
        "  - Verify checkpoint path is correct\n",
        "  - Check if checkpoint was saved completely (no interruption)\n",
        "  - Try loading with `strict=False` to ignore missing keys\n",
        "\n",
        "#### Training Not Converging\n",
        "- **Loss not decreasing**\n",
        "  - Try lower learning rate: 5e-6 or 1e-6\n",
        "  - Increase warmup steps\n",
        "  - Check if rewards are providing meaningful signal\n",
        "\n",
        "- **KL divergence too high**\n",
        "  - Increase beta (KL penalty coefficient)\n",
        "  - Reduce learning rate\n",
        "  - Ensure reference model is properly frozen\n",
        "\n",
        "- **Rewards not improving**\n",
        "  - Verify ground truth data quality\n",
        "  - Check reward function components individually\n",
        "  - Increase training iterations\n",
        "\n",
        "#### Export Issues\n",
        "- **safetensors export fails**\n",
        "  - Install safetensors: `pip install safetensors>=0.4.0`\n",
        "  - Verify weights are on CPU before saving\n",
        "  - Check file path permissions\n",
        "\n",
        "- **Exported adapters don't load in PyTorch**\n",
        "  - Ensure adapter_config.json has correct format\n",
        "  - Verify target_modules match PyTorch model layer names\n",
        "  - Check if conversion from Flax to PyTorch is needed\n",
        "\n",
        "#### Colab-Specific Issues\n",
        "- **Runtime disconnection during training**\n",
        "  - Save checkpoints frequently (every 50-100 steps)\n",
        "  - Keep browser tab active\n",
        "  - Consider using Colab Pro for longer runtime\n",
        "\n",
        "- **Storage limit reached**\n",
        "  - Clear old checkpoints: keep only latest + final\n",
        "  - Export to Google Drive\n",
        "  - Use smaller checkpoint format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f1AnqMBL1eI"
      },
      "source": [
        "## üéâ Conclusion\n",
        "\n",
        "This notebook demonstrates end-to-end GRPO training for legal reasoning using Google Tunix on TPU:\n",
        "\n",
        "### What We Built\n",
        "\n",
        "1. ‚úÖ **TPU Setup**: Initialized JAX with TPU v2-8 using `colab_tpu.setup_tpu()`\n",
        "2. ‚úÖ **Model Loading**: Downloaded Gemma 3-1B-IT and initialized with LoRA adapters\n",
        "3. ‚úÖ **Dataset Preparation**: Created XML-formatted prompts for legal reasoning\n",
        "4. ‚úÖ **Reward Function**: Implemented composite scoring (format + length + correctness)\n",
        "5. ‚úÖ **GRPO Training**: Executed training with `GRPOLearner` and `RLCluster`\n",
        "6. ‚úÖ **Export**: Packaged LoRA adapters in safetensors format for submission\n",
        "\n",
        "### Training Results\n",
        "\n",
        "After training, the model should:\n",
        "- Generate responses in valid XML format (`<reasoning>...</reasoning><answer>...</answer>`)\n",
        "- Produce detailed legal reasoning (100+ tokens)\n",
        "- Provide accurate answers based on legal principles\n",
        "\n",
        "### Files Produced\n",
        "\n",
        "| File | Description |\n",
        "|------|-------------|\n",
        "| `adapter_config.json` | LoRA configuration for PEFT |\n",
        "| `adapter_model.safetensors` | Trained LoRA weights |\n",
        "| `README.md` | Inference instructions |\n",
        "| `judicaita_submission.zip` | Kaggle submission package |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Upload to Kaggle**: Submit `judicaita_submission.zip` to the competition\n",
        "2. **Fine-tune Further**: Increase training iterations for better results\n",
        "3. **Add More Data**: Include additional legal reasoning examples\n",
        "4. **Evaluate on LegalBench**: Test on official benchmark tasks\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Tunix Documentation](https://tunix.readthedocs.io/)\n",
        "- [Tunix GRPO Gemma Example](https://github.com/google/tunix/tree/main/examples/grpo_gemma)\n",
        "- [Judicaita Repository](https://github.com/clduab11/judicAIta)\n",
        "- [Gemma Model Cards](https://ai.google.dev/gemma)\n",
        "- [JAX TPU Guide](https://jax.readthedocs.io/en/latest/notebooks/TPU_Colab.html)\n",
        "\n",
        "### Troubleshooting & Support\n",
        "\n",
        "If you encounter issues:\n",
        "1. Check the Troubleshooting Guide section above\n",
        "2. Open an issue: https://github.com/clduab11/judicAIta/issues\n",
        "3. Review Tunix documentation for API changes\n",
        "\n",
        "### Contributing\n",
        "\n",
        "Improvements welcome! Submit a PR with:\n",
        "- Additional reward function components\n",
        "- Better data preprocessing\n",
        "- Performance optimizations\n",
        "- Documentation improvements\n",
        "\n",
        "---\n",
        "\n",
        "**Made with ‚ù§Ô∏è for the Kaggle hackathon and legal tech community**\n",
        "\n",
        "*Powered by Google Tunix, JAX, and Gemma*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}