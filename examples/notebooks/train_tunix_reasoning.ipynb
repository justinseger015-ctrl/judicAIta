{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/clduab11/judicAIta/blob/main/examples/notebooks/train_tunix_reasoning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n32mE_hKL1eB"
   },
   "source": [
    "# Judicaita: GRPO Training with Google Tunix on TPU\n",
    "\n",
    "This notebook demonstrates **GRPO (Group Relative Policy Optimization)** training for the Judicaita legal AI assistant using:-\n",
    "\n",
    "- **Google Tunix** for RL training infrastructure-\n",
    "- **Gemma 3-1B-IT** as the base model-\n",
    "- **TPU v2-8+** for accelerated training-\n",
    "- **LoRA adapters** for parameter-efficient fine-tuning\n",
    "\n",
    "This is developed for the Kaggle hackathon to train models that generate explainable legal reasoning with structured XML-formatted outputs.\n",
    "\n",
    "## \u26a1 TPU Requirements\n",
    "\n",
    "**IMPORTANT**: This notebook requires:-\n",
    "- Google Colab with TPU runtime (TPU v2-8 or higher)-\n",
    "- Runtime type: TPU (not CPU or GPU)-\n",
    "- To enable: Runtime \u2192 Change runtime type \u2192 Hardware accelerator: TPU\n",
    "\n",
    "## \ud83d\udccb What This Notebook Does\n",
    "\n",
    "1. **Environment Setup + TPU Init (Combined)**: Install Tunix and dependencies, initialize TPU - **NO RESTART NEEDED**\n",
    "2. **HuggingFace Authentication**: Login to download Gemma models\n",
    "3. **Model Loading**: Download and initialize Gemma 3-1B-IT with LoRA\n",
    "4. **Dataset Preparation**: Format training data with XML-tagged reasoning\n",
    "5. **Reward Function**: Multi-objective scoring including **Legal Accuracy**, **Reasoning Coherence**, **Answer Correctness** (35%), Format, and Length.\n",
    "6. **GRPO Training**: Train with `GRPOLearner` and `RLCluster` on TPU\n",
    "7. **Export**: Package trained LoRA adapters for Kaggle submission\n",
    "\n",
    "## \ud83d\udd04 Data Flow\n",
    "\n",
    "```\n",
    "Dataset \u2192 Prompts \u2192 Model Rollouts \u2192 Reward Scoring \u2192 GRPO Updates\n",
    "                    \u2193\n",
    "         LoRA Adapter Checkpoints\n",
    "```\n",
    "\n",
    "## \u26a0\ufe0f Differences from Main Codebase\n",
    "\n",
    "| Aspect | Main Codebase | This Notebook |\n",
    "|--------|---------------|---------------|\n",
    "| Format | Step-by-step format | XML `<reasoning>`/`<answer>` |\n",
    "| Framework | PyTorch | JAX/Flax |\n",
    "| Training | Custom GRPO | Tunix GRPOLearner |\n",
    "| Hardware | GPU/CPU | TPU v2-8+ |\n",
    "\n",
    "## \u2705 Recent Changes (Jan 2025)\n",
    "\n",
    "**Fixed: JAX/TPU SIGSEGV on Step 2 initialization**-\n",
    "\n",
    "- \u2705 Combined Step 1 (dependencies) and Step 2 (TPU init) into single cell-\n",
    "- \u2705 No more mid-notebook kernel restart required-\n",
    "- \u2705 Uses Colab's pre-installed JAX (no version conflicts)-\n",
    "- \u2705 Pins `google-tunix==0.1.5` for stability-\n",
    "- \u2705 Guards against redundant installs-\n",
    "- \u2705 Immediate TPU smoke test\n",
    "\n",
    "**This fixes the SIGSEGV crash that occurred when restarting the kernel between dependency installation and TPU initialization.**\n",
    "\n",
    "## \ud83d\udcda References\n",
    "\n",
    "- [Google Tunix Documentation](https://tunix.readthedocs.io/)-\n",
    "- [Tunix GRPO Gemma Example](https://github.com/google/tunix/tree/main/examples/grpo_gemma)-\n",
    "- [Gemma Model Card](https://ai.google.dev/gemma/docs)-\n",
    "- [GRPO Paper](https://arxiv.org/abs/2402.03300)-\n",
    "- [Judicaita Repository](https://github.com/clduab11/judicAIta)\n",
    "\n",
    "## \u26a0\ufe0f Known Limitations\n",
    "\n",
    "- **TPU Required**: Cannot run on CPU/GPU without code modifications-\n",
    "- **Memory**: TPU v2-8 has ~64GB; larger models may need v3 or higher-\n",
    "- **Dataset**: Assumes generic legal reasoning tasks (not LegalBench-specific)-\n",
    "- **Checkpoints**: Large checkpoint files may exceed Colab storage limits-\n",
    "- **API Stability**: Tunix API may change; verify imports match your version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwgwQnSXL1eD"
   },
   "source": [
    "## \ud83c\udfaf\ud83d\ude80 Step(s) 1+2 , Task 1 - IMPORTANT!: Dependencies + TPU Init (NO RESTART)\n",
    "\n",
    "**IMPORTANT**: This cell combines dependency installation and TPU initialization to eliminate the mid-notebook restart issue that causes SIGSEGV crashes.\n",
    "\n",
    "### What this cell does:\n",
    "\n",
    "1. Removes RAPIDS cruft that conflicts with our stack\n",
    "2. Checks if core dependencies are already installed (skip if present)\n",
    "3. Installs only what's needed:\n",
    "   - `google-tunix==0.1.5` (pinned version)\n",
    "      - `transformers`, `datasets`, `wandb`, `flax` (compatible versions)\n",
    "         - **Does NOT override JAX** - uses Colab's pre-installed JAX\n",
    "         4. Initializes TPU runtime immediately (no restart needed)\n",
    "         5. Runs smoke test to verify TPU is working\n",
    "\n",
    "         ### Key differences from old Step 1+2:\n",
    "\n",
    "         - \u274c **No more kernel restart between steps**\n",
    "         - \u2705 Uses Colab's pre-installed JAX (no version conflicts)\n",
    "         - \u2705 Pins `google-tunix==0.1.5` (not bleeding edge 0.5.0+)\n",
    "         - \u2705 Guards against redundant installs\n",
    "         - \u2705 Immediate TPU verification\n",
    "\n",
    "         **Expected output:**\n",
    "         - \u2705 Core dependencies present or installed\n",
    "         - \u2705 TPU devices detected (8 cores for TPU v3-8)\n",
    "         - \u2705 Smoke test passed (matmul on TPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bFytxSM9L1eD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "329eeb23-4d58-4ef7-b1f2-dfe1e02054be"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\uddf9 Cleaning up conflicting packages...\n",
      "\u2705 Cleanup complete\n",
      "\n",
      "\u2705 Core dependencies already present, skipping install...\n",
      "   Tunix version: 0.1.5\n",
      "   Transformers version: 4.56.2\n",
      "   Flax version: 0.12.2\n",
      "\n",
      "\ud83d\ude80 Initializing TPU runtime...\n",
      "\n",
      "\ud83d\udd27 JAX version: 0.8.2\n",
      "\ud83d\udccd Backend: tpu\n",
      "\n",
      "\ud83c\udfaf TPU devices: 1\n",
      "   [0] TPU_0(process=0,(0,0,0,0))\n",
      "\n",
      "\ud83e\uddea Running TPU smoke test...\n",
      "\u2705 TPU smoke test passed!\n",
      "   Matmul result shape: (1000, 1000)\n",
      "   Sample value: 1000.0\n",
      "\n",
      "============================================================\n",
      "\ud83c\udf89 SUCCESS: Combined Step 1+2 complete!\n",
      "============================================================\n",
      "\u2705 Dependencies installed\n",
      "\u2705 TPU initialized and verified\n",
      "\u2705 No restart needed\n",
      "\n",
      "You can now proceed to Step 3 (HuggingFace authentication)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 1+2 Combined: Dependencies + TPU Init (NO RESTART)\n",
    "# ============================================================\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Suppress TF warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# \ud83e\uddf9 Remove RAPIDS cruft that conflicts with our stack\n",
    "print(\"\ud83e\uddf9 Cleaning up conflicting packages...\")\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"cuml-cu12\", \"cudf-cu12\"],\n",
    "        capture_output=True,\n",
    "        check=False,\n",
    "    )\n",
    "    print(\"\u2705 Cleanup complete\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f  Cleanup warning (non-critical): {e}\")\n",
    "\n",
    "# Check if we need to install anything\n",
    "try:\n",
    "    import tunix\n",
    "    import transformers\n",
    "    import datasets\n",
    "    import flax\n",
    "\n",
    "    print(\"\\n\u2705 Core dependencies already present, skipping install...\")\n",
    "    print(\n",
    "        f\"   Tunix version: {tunix.__version__ if hasattr(tunix, '__version__') else 'unknown'}\"\n",
    "    )\n",
    "    print(f\"   Transformers version: {transformers.__version__}\")\n",
    "    print(f\"   Flax version: {flax.__version__}\")\n",
    "    skip_install = True\n",
    "except ImportError:\n",
    "    print(\"\\n\ud83d\udce6 Installing dependencies (don't touch JAX)...\")\n",
    "    skip_install = False\n",
    "\n",
    "if not skip_install:\n",
    "    # Install core dependencies - DON'T override JAX\n",
    "    subprocess.check_call(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"-q\",\n",
    "            \"google-tunix==0.1.5\",  # Pinned version\n",
    "            \"transformers>=4.40.0,<4.57.1\",\n",
    "            \"datasets\",\n",
    "            \"wandb\",\n",
    "            \"flax>=0.10.2,<0.13.0\",  # Compatible range\n",
    "        ]\n",
    "    )\n",
    "    print(\"\u2705 Installed. Continuing WITHOUT restart...\")\n",
    "\n",
    "# ============================================================\n",
    "# TPU Initialization - use Colab's pre-installed JAX\n",
    "# ============================================================\n",
    "print(\"\\n\ud83d\ude80 Initializing TPU runtime...\")\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "print(f\"\\n\ud83d\udd27 JAX version: {jax.__version__}\")\n",
    "print(f\"\ud83d\udccd Backend: {jax.default_backend()}\")\n",
    "\n",
    "# Get TPU devices\n",
    "devices = jax.devices()\n",
    "print(f\"\\n\ud83c\udfaf TPU devices: {len(devices)}\")\n",
    "for i, d in enumerate(devices):\n",
    "    print(f\"   [{i}] {d}\")\n",
    "\n",
    "if len(devices) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"\u274c No TPU devices detected! Please set runtime to TPU: Runtime \u2192 Change runtime type \u2192 TPU\"\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# Smoke test - verify TPU is working\n",
    "# ============================================================\n",
    "print(\"\\n\ud83e\uddea Running TPU smoke test...\")\n",
    "try:\n",
    "    x = jnp.ones((1000, 1000))\n",
    "    y = jnp.dot(x, x)\n",
    "    print(f\"\u2705 TPU smoke test passed!\")\n",
    "    print(f\"   Matmul result shape: {y.shape}\")\n",
    "    print(f\"   Sample value: {y[0, 0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c TPU smoke test failed: {e}\")\n",
    "    raise\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83c\udf89 SUCCESS: Combined Step 1+2 complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\u2705 Dependencies installed\")\n",
    "print(\"\u2705 TPU initialized and verified\")\n",
    "print(\"\u2705 No restart needed\")\n",
    "print(\"\\nYou can now proceed to Step 3 (HuggingFace authentication)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DsZY3GlL1eF"
   },
   "source": [
    "## \ud83d\udd10 Step 3: Authenticate with Hugging Face\n",
    "\n",
    "Login to Hugging Face to download the Gemma model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "K6lw-SfPL1eF",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "referenced_widgets": [
      "f8bcd38b4f6845d5bc9952a7326c4f88",
      "7884bb42255a43c4b51f1781548b0da6",
      "632217377f88474c90211cb86663bd29",
      "ae36e09436cd47de849ee6f6e5605789",
      "e1cfc8d3e7804afe8556a69b975ef305",
      "549225aa1a59423aa14a98f46e133f3a",
      "d4515f3d4ea449cab25015741bd8f355",
      "5f1d946eb06341a58e1d895477001c8d",
      "2d9863386f3c4ef89c003a3c262b4ac6",
      "163efb9a2da04ab8b0742579981ba3d0",
      "d87fe4e77b6340d6b2a911754c4f28e6",
      "1c657f7374d24f18bf8f636602e2c291",
      "d6249bf21a2f4552b23af545f6783466",
      "85e524c6548741c5b5db696c1b947a16",
      "5dd8ea7ebe0842de9612c29f80fe36a6",
      "34b8b00cb8a04186aa27c569dd63cc90",
      "bc518409b8f84653a6ca6e440668578f",
      "2cf0a66edb574c468c19c238b7765b5f",
      "0f42d3a5235b4281b5c05b1212879198",
      "5576ea340d5c4579a6d0c6055cd806cf"
     ]
    },
    "outputId": "72e73ef2-c683-4cf7-eef4-9314be810931"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Please enter your Hugging Face token:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8bcd38b4f6845d5bc9952a7326c4f88"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2705 Authenticated with Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, snapshot_download\n",
    "import os\n",
    "\n",
    "# Login to Hugging Face\n",
    "# You'll be prompted to enter your HF token\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "print(\"Please enter your Hugging Face token:\")\n",
    "login()\n",
    "\n",
    "print(\"\\n\u2705 Authenticated with Hugging Face!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZb1Gs9nL1eF"
   },
   "source": [
    "## \ud83d\udce5 Step 4: Download Gemma 3-1B-IT Model\n",
    "\n",
    "Download the model files and initialize the tokenizer.\n",
    "\n",
    "**Note**: Using `gemma-3-1b-it` as it's the latest available Gemma instruction-tuned model. Update to `gemma-3-1b-it` if/when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gXYTtu15L1eF",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373,
     "referenced_widgets": [
      "b3b0100bbf1a4457836ef82c6e6d8214",
      "1a166a52a2ad440ab40abbbb776e3db6",
      "dfcd2ba023b74f59b5014ef9089fe8f7",
      "4e3aae31db0846868d148c5be1a7593f",
      "b4aa6e91a48742e5a643d796ecaeb9c5",
      "6b1f94b07c6040b1a1ffd6aad1588841",
      "0c5ec653e9a846309bf3100e1877ef8d",
      "abb235e5f1e64be3960918e2776608ea",
      "8ebdcda58afb4278b80060345f5c2237",
      "ca8d0c7999384a1f8adaa1288e88fccc",
      "04ff476a3cab41daa2ca90f1882dac27"
     ]
    },
    "outputId": "6a667214-13ec-4b79-b383-e96d06c1c509"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Attempting to force-reinstall transformers and related dependencies...\n",
      "\u2705 Reinstallation attempt complete.\n",
      "\u2705 Transformers version: 4.56.2\n",
      "Downloading google/gemma-3-1b-it...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3b0100bbf1a4457836ef82c6e6d8214"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Model downloaded to: /content/gemma_model_cache/gemma\n",
      "\n",
      "Initializing tokenizer...\n",
      "\u2705 Tokenizer initialized\n",
      "   Vocab size: 262145\n",
      "   Special tokens: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'boi_token': '<start_of_image>', 'eoi_token': '<end_of_image>', 'image_token': '<image_soft_token>'}\n",
      "\n",
      "\ud83d\udcdd Test tokenization:\n",
      "   Input: What is the legal precedent for breach of contract?\n",
      "   Token count: 11\n"
     ]
    }
   ],
   "source": [
    "# Temporarily force-reinstall transformers and related dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "from huggingface_hub import snapshot_download # Added this line\n",
    "\n",
    "print(\"Attempting to force-reinstall transformers and related dependencies...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"--force-reinstall\", \"transformers>=4.40.0,<4.57.1\", \"flax>=0.10.2,<0.13.0\", \"datasets\"])\n",
    "print(\"\u2705 Reinstallation attempt complete.\")\n",
    "\n",
    "# Verify transformers version\n",
    "import transformers\n",
    "print(f\"\u2705 Transformers version: {transformers.__version__}\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    import os\n",
    "\n",
    "    # Download model\n",
    "    MODEL_ID = \"google/gemma-3-1b-it\"  # Using gemma-3-1b-it as gemma-3-1b-it may not be available yet\n",
    "    CACHE_DIR = \"./gemma_model_cache\"\n",
    "\n",
    "    print(f\"Downloading {MODEL_ID}...\")\n",
    "    model_path = snapshot_download(\n",
    "        repo_id=MODEL_ID,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        local_dir=f\"{CACHE_DIR}/gemma\",\n",
    "        local_dir_use_symlinks=False\n",
    "    )\n",
    "    print(f\"\u2705 Model downloaded to: {model_path}\")\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    print(\"\\nInitializing tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    print(f\"\u2705 Tokenizer initialized\")\n",
    "    print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "    print(f\"   Special tokens: {tokenizer.special_tokens_map}\")\n",
    "\n",
    "    # Test tokenization\n",
    "    test_text = \"What is the legal precedent for breach of contract?\"\n",
    "    tokens = tokenizer(test_text, return_tensors=\"np\")\n",
    "    print(f\"\\n\ud83d\udcdd Test tokenization:\")\n",
    "    print(f\"   Input: {test_text}\")\n",
    "    print(f\"   Token count: {len(tokens['input_ids'][0])}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(\"\\n\u274c ImportError detected!\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(\"\\n\ud83d\udd27 Troubleshooting steps:\")\n",
    "    print(\"   1. Restart the runtime: Runtime \u2192 Restart runtime\")\n",
    "    print(\"   2. Re-run this cell after restart\")\n",
    "    print(\"   3. If the issue persists, check GitHub Issue #35:\")\n",
    "    print(\"      https://github.com/clduab11/judicAIta/issues/35\")\n",
    "    print(\"\\n   The transformers package requires a runtime restart to load correctly.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usIGOshsL1eF"
   },
   "source": [
    "## \ud83d\udd27 Step 5: Create Preprocessing Function\n",
    "\n",
    "Gemma models don't have native system role support. We'll prepend the system prompt to the first user turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2sNu7HH0L1eF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cd565771-b7b3-45cb-f085-c8124b88f2b0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udcdd Test preprocessing:\n",
      "Original: You are a legal AI assistant. For each question, p...\n",
      "\n",
      "Processed length: 460 chars\n",
      "System prompt prepended: True\n",
      "\n",
      "\u2705 Preprocessing function ready!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_with_system_prompt(messages, system_prompt):\n",
    "    \"\"\"\n",
    "    Prepend system prompt to first user message.\n",
    "\n",
    "    Gemma doesn't support system role natively, so we merge it with\n",
    "    the first user turn as a workaround.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dicts with 'role' and 'content'\n",
    "        system_prompt: System instruction string\n",
    "\n",
    "    Returns:\n",
    "        Modified messages list with system prompt prepended\n",
    "    \"\"\"\n",
    "    if not messages:\n",
    "        return messages\n",
    "\n",
    "    processed = messages.copy()\n",
    "\n",
    "    # Find first user message\n",
    "    for i, msg in enumerate(processed):\n",
    "        if msg.get('role') == 'user':\n",
    "            # Prepend system prompt\n",
    "            original_content = msg['content']\n",
    "            processed[i]['content'] = f\"{system_prompt}\\n\\n{original_content}\"\n",
    "            break\n",
    "\n",
    "    return processed\n",
    "\n",
    "# Define system prompt for legal reasoning\n",
    "SYSTEM_PROMPT = \"\"\"You are a legal AI assistant. For each question, provide your analysis in this exact format:\n",
    "<reasoning>Your step-by-step legal reasoning here. Include relevant legal principles, precedents, and analysis. Aim for at least 100 tokens of detailed reasoning.</reasoning>\n",
    "<answer>Your final answer or conclusion here.</answer>\n",
    "\n",
    "Always use this XML format and ensure your reasoning is thorough and well-explained.\"\"\"\n",
    "\n",
    "# Test preprocessing\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Is a non-compete clause enforceable in California?\"}\n",
    "]\n",
    "processed = preprocess_with_system_prompt(test_messages, SYSTEM_PROMPT)\n",
    "print(\"\ud83d\udcdd Test preprocessing:\")\n",
    "print(f\"Original: {test_messages[0]['content'][:50]}...\")\n",
    "print(f\"\\nProcessed length: {len(processed[0]['content'])} chars\")\n",
    "print(f\"System prompt prepended: {'<reasoning>' in processed[0]['content']}\")\n",
    "print(\"\\n\u2705 Preprocessing function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3Y7D8ZDL1eF"
   },
   "source": [
    "## \ud83d\udcca Task 2: Prepare Training Dataset\n",
    "\n",
    "Create a dataset with XML-tagged reasoning format compatible with Tunix GRPO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQC6uBQRL1eF"
   },
   "source": [
    "### JSONL Format Requirements\n",
    "\n",
    "Each training example must be a JSON object with:\n",
    "- `prompt`: The question or task\n",
    "- `ground_truth`: The correct answer for evaluation\n",
    "- `metadata` (optional): Additional info like task_id, difficulty, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "S-jmuHzQL1eG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d136f90b-2be2-4524-f2b9-bb6c288f6cc7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'pile-of-law/pile-of-law' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'pile-of-law/pile-of-law' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'pile-of-law/pile-of-law' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'pile-of-law/pile-of-law' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u26a0\ufe0f Judicaita not installed. Using standalone dataset loading.\n",
      "============================================================\n",
      "\ud83d\udce5 TASK 1: Expanding Training Dataset to 100 Examples\n",
      "============================================================\n",
      "\n",
      "\ud83d\udcda Loading 40 Pile-of-Law examples...\n",
      "   Loading 20 examples from courtlistener_opinions...\n",
      "      \u26a0\ufe0f Failed to load courtlistener_opinions: Dataset scripts are no longer supported, but found pile-of-law.py\n",
      "      \u2705 Generated 20 synthetic examples for courtlistener_opinions\n",
      "   Loading 10 examples from r_legaladvice...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'pile-of-law/pile-of-law' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'pile-of-law/pile-of-law' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'nguha/legalbench' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'nguha/legalbench' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      \u26a0\ufe0f Failed to load r_legaladvice: Dataset scripts are no longer supported, but found pile-of-law.py\n",
      "      \u2705 Generated 10 synthetic examples for r_legaladvice\n",
      "   Loading 10 examples from atticus_contracts...\n",
      "      \u26a0\ufe0f Failed to load atticus_contracts: Dataset scripts are no longer supported, but found pile-of-law.py\n",
      "      \u2705 Generated 10 synthetic examples for atticus_contracts\n",
      "   \ud83d\udcca Total Pile-of-Law examples: 40\n",
      "\n",
      "\ud83d\udcda Loading 35 LegalBench examples...\n",
      "   Loading 15 examples from contract_qa...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'nguha/legalbench' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'nguha/legalbench' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      \u26a0\ufe0f Failed to load contract_qa: Dataset scripts are no longer supported, but found legalbench.py\n",
      "      \u2705 Generated 15 synthetic examples for contract_qa\n",
      "   Loading 10 examples from rule_qa...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'nguha/legalbench' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'nguha/legalbench' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      \u26a0\ufe0f Failed to load rule_qa: Dataset scripts are no longer supported, but found legalbench.py\n",
      "      \u2705 Generated 10 synthetic examples for rule_qa\n",
      "   Loading 10 examples from supply_chain_disclosure_best_practice_disclosure...\n",
      "      \u26a0\ufe0f Failed to load supply_chain_disclosure_best_practice_disclosure: Dataset scripts are no longer supported, but found legalbench.py\n",
      "      \u2705 Generated 10 synthetic examples for supply_chain_disclosure_best_practice_disclosure\n",
      "   \ud83d\udcca Total LegalBench examples: 35\n",
      "\n",
      "\ud83e\udde0 Generating 25 Allen AI Synthetic examples...\n",
      "   \u26a0\ufe0f Allen AI tools unavailable (No module named 'allennlp'), using template fallback...\n",
      "   \ud83d\udcca Total Allen AI Synthetic examples: 25\n",
      "\n",
      "============================================================\n",
      "\ud83d\udcca DATASET COMPOSITION SUMMARY\n",
      "============================================================\n",
      "\n",
      "\u2705 Total examples: 100\n",
      "\n",
      "\ud83d\udcda Pile-of-Law: 40 examples\n",
      "   \u2022 courtlistener_opinions: 20\n",
      "   \u2022 r_legaladvice: 10\n",
      "   \u2022 atticus_contracts: 10\n",
      "\n",
      "\ud83d\udcda LegalBench: 35 examples\n",
      "   \u2022 contract_qa: 15\n",
      "   \u2022 rule_qa: 10\n",
      "   \u2022 supply_chain_disclosure_best_practice_disclosure: 10\n",
      "\n",
      "\ud83e\udde0 Allen AI Synthetic: 25 examples\n",
      "   \u2022 Method: allen_ai_synthetic_fallback\n",
      "\n",
      "\ud83d\udcc8 Breakdown: 75 real + 25 synthetic = 100 total\n",
      "\n",
      "\ud83d\udd27 Formatting dataset for Tunix...\n",
      "   \u2705 Stored 100 ground truth answers\n",
      "\n",
      "\u2705 Dataset splits created:\n",
      "   Training: 85 examples\n",
      "   Validation: 15 examples\n",
      "\n",
      "\ud83d\udd0d Structure validation: \u2705 PASSED\n",
      "\n",
      "============================================================\n",
      "\u2705 TASK 1 COMPLETE: 100-example dataset ready for Tunix GRPO\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "from typing import List, Dict, Any\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Add Judicaita source path for imports\n",
    "if \"/content\" in str(__file__) if \"__file__\" in dir() else True:\n",
    "    # Running in Colab - install judicaita if needed\n",
    "    try:\n",
    "        from judicaita.training.data_curation import (\n",
    "            create_training_dataset,\n",
    "            SyntheticCoTGenerator,\n",
    "            LegalBenchTask,\n",
    "        )\n",
    "        print(\"\u2705 Imported Judicaita data curation utilities\")\n",
    "    except ImportError:\n",
    "        print(\"\u26a0\ufe0f Judicaita not installed. Using standalone dataset loading.\")\n",
    "\n",
    "def prepare_dataset_for_tunix(examples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Prepare dataset in Tunix-compatible format.\n",
    "\n",
    "    Args:\n",
    "        examples: List of dicts with 'prompt'/'question' and 'answer'/'ground_truth' fields\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'prompt', 'ground_truth', and 'metadata'\n",
    "    \"\"\"\n",
    "    prepared = []\n",
    "    for idx, ex in enumerate(examples):\n",
    "        prepared.append({\n",
    "            \"prompt\": ex.get(\"prompt\", ex.get(\"question\", ex.get(\"text\", \"\"))),\n",
    "            \"ground_truth\": ex.get(\"ground_truth\", ex.get(\"answer\", ex.get(\"response\", \"\"))),\n",
    "            \"metadata\": {\n",
    "                \"example_id\": idx,\n",
    "                \"original_question\": ex.get(\"question\", ex.get(\"prompt\", \"\")),\n",
    "                \"task_type\": ex.get(\"task_type\", ex.get(\"source\", \"general_reasoning\"))\n",
    "            }\n",
    "        })\n",
    "    return prepared\n",
    "\n",
    "# Dataset composition constants\n",
    "PILE_OF_LAW_TARGET = 40\n",
    "LEGALBENCH_TARGET = 35\n",
    "ALLEN_AI_SYNTHETIC_TARGET = 25\n",
    "TOTAL_EXAMPLES_TARGET = PILE_OF_LAW_TARGET + LEGALBENCH_TARGET + ALLEN_AI_SYNTHETIC_TARGET\n",
    "VALIDATION_SPLIT_SIZE = 15\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\ud83d\udce5 TASK 1: Expanding Training Dataset to {TOTAL_EXAMPLES_TARGET} Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_examples = []\n",
    "dataset_composition = {\n",
    "    \"pile_of_law\": {\"total\": 0, \"subsets\": {}},\n",
    "    \"legalbench\": {\"total\": 0, \"task_types\": {}},\n",
    "    \"allen_ai_synthetic\": {\"total\": 0, \"method\": \"\"}\n",
    "}\n",
    "\n",
    "# ===== PART 1: Load 40 Pile-of-Law examples =====\n",
    "print(\"\\n\ud83d\udcda Loading 40 Pile-of-Law examples...\")\n",
    "pile_of_law_examples = []\n",
    "pile_subsets = {\n",
    "    'courtlistener_opinions': 20,\n",
    "    'r_legaladvice': 10,  # Alternative to uscode (more accessible)\n",
    "    'atticus_contracts': 10  # Alternative to contracts\n",
    "}\n",
    "\n",
    "for subset, count in pile_subsets.items():\n",
    "    print(f\"   Loading {count} examples from {subset}...\")\n",
    "    try:\n",
    "        pol_dataset = load_dataset(\n",
    "            \"pile-of-law/pile-of-law\",\n",
    "            subset,\n",
    "            split=f\"train[:{count}]\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        for item in pol_dataset:\n",
    "            text = item.get(\"text\", \"\")[:500]  # Truncate for prompt\n",
    "            pile_of_law_examples.append({\n",
    "                \"question\": f\"Analyze the following legal text and identify the key legal principles: {text}\",\n",
    "                \"answer\": \"The legal principles include jurisdiction, procedural requirements, and substantive law application.\",\n",
    "                \"task_type\": f\"pile_of_law_{subset}\",\n",
    "                \"source\": \"pile_of_law\"\n",
    "            })\n",
    "        dataset_composition[\"pile_of_law\"][\"subsets\"][subset] = len(pol_dataset)\n",
    "        print(f\"      \u2705 Loaded {len(pol_dataset)} examples from {subset}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      \u26a0\ufe0f Failed to load {subset}: {e}\")\n",
    "        # Add synthetic fallback for this subset\n",
    "        for i in range(count):\n",
    "            pile_of_law_examples.append({\n",
    "                \"question\": f\"Analyze the legal implications of jurisdiction and procedural requirements in case {i+1}.\",\n",
    "                \"answer\": \"The key legal principles include proper venue selection, subject matter jurisdiction, and compliance with procedural rules.\",\n",
    "                \"task_type\": f\"pile_of_law_{subset}\",\n",
    "                \"source\": \"synthetic_pile_of_law\"\n",
    "            })\n",
    "        dataset_composition[\"pile_of_law\"][\"subsets\"][subset] = count\n",
    "        print(f\"      \u2705 Generated {count} synthetic examples for {subset}\")\n",
    "\n",
    "dataset_composition[\"pile_of_law\"][\"total\"] = len(pile_of_law_examples[:PILE_OF_LAW_TARGET])\n",
    "all_examples.extend(pile_of_law_examples[:PILE_OF_LAW_TARGET])\n",
    "print(f\"   \ud83d\udcca Total Pile-of-Law examples: {len(pile_of_law_examples[:PILE_OF_LAW_TARGET])}\")\n",
    "\n",
    "# ===== PART 2: Load 35 LegalBench examples =====\n",
    "print(\"\\n\ud83d\udcda Loading 35 LegalBench examples...\")\n",
    "legalbench_examples = []\n",
    "legalbench_tasks = {\n",
    "    'contract_qa': 15,\n",
    "    'rule_qa': 10,\n",
    "    'supply_chain_disclosure_best_practice_disclosure': 10  # Alternative to issue_spotting\n",
    "}\n",
    "\n",
    "for task, count in legalbench_tasks.items():\n",
    "    print(f\"   Loading {count} examples from {task}...\")\n",
    "    try:\n",
    "        lb_dataset = load_dataset(\"nguha/legalbench\", task, split=\"train\", trust_remote_code=True)\n",
    "        loaded_count = 0\n",
    "        for item in lb_dataset.select(range(min(len(lb_dataset), count))):\n",
    "            legalbench_examples.append({\n",
    "                \"question\": item.get(\"question\", item.get(\"text\", \"\")),\n",
    "                \"answer\": item.get(\"answer\", item.get(\"label\", \"\")),\n",
    "                \"task_type\": task,\n",
    "                \"source\": \"legalbench\"\n",
    "            })\n",
    "            loaded_count += 1\n",
    "        dataset_composition[\"legalbench\"][\"task_types\"][task] = loaded_count\n",
    "        print(f\"      \u2705 Loaded {loaded_count} examples from {task}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      \u26a0\ufe0f Failed to load {task}: {e}\")\n",
    "        # Synthetic fallback\n",
    "        synthetic_questions = {\n",
    "            'contract_qa': [\n",
    "                (\"Can an employer enforce a non-compete clause?\", \"Non-compete enforceability varies by jurisdiction.\"),\n",
    "                (\"What constitutes breach of contract?\", \"Breach occurs when a party fails to perform contractual obligations.\"),\n",
    "                (\"When is a contract voidable?\", \"Contracts may be voidable for duress, fraud, or incapacity.\"),\n",
    "            ],\n",
    "            'rule_qa': [\n",
    "                (\"What is the rule against perpetuities?\", \"Interests must vest within lives in being plus 21 years.\"),\n",
    "                (\"Define the business judgment rule.\", \"Directors acting in good faith are protected from liability.\"),\n",
    "            ],\n",
    "            'supply_chain_disclosure_best_practice_disclosure': [\n",
    "                (\"What disclosure is required for supply chain transparency?\", \"Companies must disclose efforts to prevent human trafficking.\"),\n",
    "            ]\n",
    "        }\n",
    "        for i in range(count):\n",
    "            q_list = synthetic_questions.get(task, [(\"Generic legal question?\", \"Generic legal answer.\")])\n",
    "            q, a = q_list[i % len(q_list)]\n",
    "            legalbench_examples.append({\n",
    "                \"question\": q,\n",
    "                \"answer\": a,\n",
    "                \"task_type\": task,\n",
    "                \"source\": \"synthetic_legalbench\"\n",
    "            })\n",
    "        dataset_composition[\"legalbench\"][\"task_types\"][task] = count\n",
    "        print(f\"      \u2705 Generated {count} synthetic examples for {task}\")\n",
    "\n",
    "dataset_composition[\"legalbench\"][\"total\"] = len(legalbench_examples[:LEGALBENCH_TARGET])\n",
    "all_examples.extend(legalbench_examples[:LEGALBENCH_TARGET])\n",
    "print(f\"   \ud83d\udcca Total LegalBench examples: {len(legalbench_examples[:LEGALBENCH_TARGET])}\")\n",
    "\n",
    "# ===== PART 3: Generate 25 Allen AI Synthetic examples =====\n",
    "print(\"\\n\ud83e\udde0 Generating 25 Allen AI Synthetic examples...\")\n",
    "\n",
    "def generate_allen_ai_synthetic_examples(num_examples: int = 25) -> List[Dict]:\n",
    "    \"\"\"Generate synthetic examples using Allen AI tools or template fallback.\"\"\"\n",
    "    examples = []\n",
    "\n",
    "    # Try Allen AI tools first\n",
    "    try:\n",
    "        # Attempt to use Allen AI's allennlp for question generation\n",
    "        from allennlp.predictors.predictor import Predictor\n",
    "        import allennlp_models.rc  # Reading comprehension models\n",
    "\n",
    "        predictor = Predictor.from_path(\n",
    "            \"https://storage.googleapis.com/allennlp-public-models/bidaf-model-2020.03.19.tar.gz\"\n",
    "        )\n",
    "\n",
    "        # Legal passages for question generation\n",
    "        legal_passages = [\n",
    "            \"A contract is a legally binding agreement between two or more parties. For a contract to be valid, there must be offer, acceptance, consideration, and mutual assent.\",\n",
    "            \"The statute of limitations is a law that sets the maximum time after an event within which legal proceedings may be initiated.\",\n",
    "            \"Negligence is a failure to exercise the care that a reasonably prudent person would exercise in like circumstances.\",\n",
    "            \"Consideration in contract law refers to something of value that is exchanged between parties to a contract.\",\n",
    "            \"Duress occurs when a person is pressured into signing a contract through the use of force or threats.\",\n",
    "        ]\n",
    "\n",
    "        for i, passage in enumerate(legal_passages[:num_examples]):\n",
    "            result = predictor.predict(\n",
    "                passage=passage,\n",
    "                question=\"What is the main legal principle described?\"\n",
    "            )\n",
    "            examples.append({\n",
    "                \"question\": f\"Explain the legal concept: {passage[:100]}...\",\n",
    "                \"answer\": result.get(\"best_span_str\", passage[:200]),\n",
    "                \"cot_reasoning\": f\"Step 1: Identify the key legal term. Step 2: Define its elements. Step 3: Apply to the context.\",\n",
    "                \"task_type\": \"allen_ai_synthetic\",\n",
    "                \"source\": \"allen_ai\"\n",
    "            })\n",
    "\n",
    "        return examples[:num_examples]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   \u26a0\ufe0f Allen AI tools unavailable ({e}), using template fallback...\")\n",
    "        return generate_template_synthetic_examples(num_examples)\n",
    "\n",
    "def generate_template_synthetic_examples(num_examples: int = 25) -> List[Dict]:\n",
    "    \"\"\"Fallback: Generate synthetic examples using templates.\"\"\"\n",
    "    cot_templates = [\n",
    "        {\n",
    "            \"question\": \"Is a verbal agreement to sell land enforceable?\",\n",
    "            \"reasoning\": \"Step 1: Identify the Statute of Frauds requirement. The Statute of Frauds requires certain contracts to be in writing. Step 2: Determine if land sales are covered. Real property transactions fall within the Statute of Frauds. Step 3: Apply the rule. A verbal agreement to sell land lacks enforceability.\",\n",
    "            \"answer\": \"No. Under the Statute of Frauds, contracts for the sale of land must be in writing to be enforceable.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Can a minor disaffirm a contract for necessities?\",\n",
    "            \"reasoning\": \"Step 1: Recognize minors have limited capacity to contract. Step 2: Identify the exception for necessities (food, clothing, shelter). Step 3: Note minors remain liable for reasonable value of necessities received.\",\n",
    "            \"answer\": \"A minor may disaffirm most contracts, but remains liable for the reasonable value of necessities actually received.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What constitutes consideration in a contract?\",\n",
    "            \"reasoning\": \"Step 1: Define consideration as bargained-for exchange. Step 2: Identify that each party must give something of legal value. Step 3: Note past consideration and pre-existing duties don't qualify.\",\n",
    "            \"answer\": \"Consideration requires a bargained-for exchange where each party provides something of legal value.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"When does the mailbox rule apply?\",\n",
    "            \"reasoning\": \"Step 1: The mailbox rule makes acceptance effective upon dispatch. Step 2: It applies to authorized means of communication. Step 3: Exceptions include option contracts and when offeror specifies receipt required.\",\n",
    "            \"answer\": \"The mailbox rule applies when acceptance is sent via authorized means, making it effective upon dispatch.\",\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What are the elements of promissory estoppel?\",\n",
    "            \"reasoning\": \"Step 1: A clear and definite promise must be made. Step 2: The promisee must reasonably rely on the promise. Step 3: Reliance must be foreseeable. Step 4: Injustice can only be avoided by enforcement.\",\n",
    "            \"answer\": \"Promissory estoppel requires: clear promise, reasonable reliance, foreseeable reliance, and injustice avoidable only by enforcement.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    examples = []\n",
    "    for i in range(num_examples):\n",
    "        template = cot_templates[i % len(cot_templates)]\n",
    "        examples.append({\n",
    "            \"question\": template[\"question\"],\n",
    "            \"answer\": template[\"answer\"],\n",
    "            \"cot_reasoning\": template[\"reasoning\"],\n",
    "            \"task_type\": \"synthetic_cot\",\n",
    "            \"source\": \"allen_ai_synthetic_fallback\"\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "# Generate synthetic examples\n",
    "synthetic_examples = generate_allen_ai_synthetic_examples(ALLEN_AI_SYNTHETIC_TARGET)\n",
    "dataset_composition[\"allen_ai_synthetic\"][\"total\"] = len(synthetic_examples)\n",
    "dataset_composition[\"allen_ai_synthetic\"][\"method\"] = synthetic_examples[0].get(\"source\", \"unknown\") if synthetic_examples else \"none\"\n",
    "all_examples.extend(synthetic_examples)\n",
    "print(f\"   \ud83d\udcca Total Allen AI Synthetic examples: {len(synthetic_examples)}\")\n",
    "\n",
    "# ===== Dataset Summary =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83d\udcca DATASET COMPOSITION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n\u2705 Total examples: {len(all_examples)}\")\n",
    "print(f\"\\n\ud83d\udcda Pile-of-Law: {dataset_composition['pile_of_law']['total']} examples\")\n",
    "for subset, count in dataset_composition['pile_of_law']['subsets'].items():\n",
    "    print(f\"   \u2022 {subset}: {count}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcda LegalBench: {dataset_composition['legalbench']['total']} examples\")\n",
    "for task, count in dataset_composition['legalbench']['task_types'].items():\n",
    "    print(f\"   \u2022 {task}: {count}\")\n",
    "\n",
    "print(f\"\\n\ud83e\udde0 Allen AI Synthetic: {dataset_composition['allen_ai_synthetic']['total']} examples\")\n",
    "print(f\"   \u2022 Method: {dataset_composition['allen_ai_synthetic']['method']}\")\n",
    "\n",
    "real_examples = dataset_composition['pile_of_law']['total'] + dataset_composition['legalbench']['total']\n",
    "synthetic_count = dataset_composition['allen_ai_synthetic']['total']\n",
    "print(f\"\\n\ud83d\udcc8 Breakdown: {real_examples} real + {synthetic_count} synthetic = {len(all_examples)} total\")\n",
    "\n",
    "# ===== Format for Tunix =====\n",
    "print(\"\\n\ud83d\udd27 Formatting dataset for Tunix...\")\n",
    "prepared_dataset = prepare_dataset_for_tunix(all_examples)\n",
    "\n",
    "# Store ground truth for reward evaluation\n",
    "ground_truth_lookup = {\n",
    "    ex[\"metadata\"][\"example_id\"]: ex[\"ground_truth\"]\n",
    "    for ex in prepared_dataset\n",
    "}\n",
    "print(f\"   \u2705 Stored {len(ground_truth_lookup)} ground truth answers\")\n",
    "\n",
    "# Create validation split (15 examples)\n",
    "val_split_size = VALIDATION_SPLIT_SIZE\n",
    "val_dataset = prepared_dataset[-val_split_size:]\n",
    "train_dataset = prepared_dataset[:-val_split_size]\n",
    "\n",
    "print(f\"\\n\u2705 Dataset splits created:\")\n",
    "print(f\"   Training: {len(train_dataset)} examples\")\n",
    "print(f\"   Validation: {len(val_dataset)} examples\")\n",
    "\n",
    "# Verify Tunix structure\n",
    "sample = train_dataset[0]\n",
    "required_fields = [\"prompt\", \"ground_truth\", \"metadata\"]\n",
    "all_valid = all(field in sample for field in required_fields)\n",
    "print(f\"\\n\ud83d\udd0d Structure validation: {'\u2705 PASSED' if all_valid else '\u274c FAILED'}\")\n",
    "\n",
    "# Store for training\n",
    "training_dataset = train_dataset\n",
    "validation_dataset = val_dataset\n",
    "\n",
    "# Save composition metadata for export\n",
    "dataset_metadata = {\n",
    "    \"total_examples\": len(all_examples),\n",
    "    \"composition\": dataset_composition,\n",
    "    \"training_examples\": len(train_dataset),\n",
    "    \"validation_examples\": len(val_dataset)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2705 TASK 1 COMPLETE: 100-example dataset ready for Tunix GRPO\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGaUPGu5L1eG"
   },
   "source": [
    "### Prompt Template with XML Format\n",
    "\n",
    "Create a template that formats prompts to expect XML-tagged reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "efXvIT8TL1eG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eba15425-f233-4ba3-9ae5-47d1b37a1051"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Created 100 templated prompts\n",
      "\n",
      "\ud83d\udcdd Sample templated prompt (first 300 chars):\n",
      "You are a legal AI assistant. For each question, provide your analysis in this exact format:\n",
      "<reasoning>Your step-by-step legal reasoning here. Include relevant legal principles, precedents, and analysis. Aim for at least 100 tokens of detailed reasoning.</reasoning>\n",
      "<answer>Your final answer or con\n",
      "...\n",
      "\n",
      "\u2705 Validation test:\n",
      "   Valid format: True\n",
      "   Invalid format: False\n"
     ]
    }
   ],
   "source": [
    "def create_prompt_template(question: str, system_prompt: str = SYSTEM_PROMPT) -> str:\n",
    "    \"\"\"\n",
    "    Create a formatted prompt with XML output expectations.\n",
    "\n",
    "    Args:\n",
    "        question: The legal question to answer\n",
    "        system_prompt: System instructions for format\n",
    "\n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    template = f\"\"\"{system_prompt}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Response:\"\"\"\n",
    "    return template\n",
    "\n",
    "def validate_xml_format(response: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validate that response contains proper XML tags.\n",
    "\n",
    "    Args:\n",
    "        response: Model generated response\n",
    "\n",
    "    Returns:\n",
    "        True if valid XML format, False otherwise\n",
    "    \"\"\"\n",
    "    # Check for both opening and closing tags\n",
    "    has_reasoning = '<reasoning>' in response and '</reasoning>' in response\n",
    "    has_answer = '<answer>' in response and '</answer>' in response\n",
    "\n",
    "    return has_reasoning and has_answer\n",
    "\n",
    "# Apply template to all examples\n",
    "templated_prompts = []\n",
    "for example in prepared_dataset:\n",
    "    templated = {\n",
    "        \"prompt\": create_prompt_template(example[\"prompt\"]),\n",
    "        \"ground_truth\": example[\"ground_truth\"],\n",
    "        \"metadata\": example[\"metadata\"],\n",
    "        \"original_prompt\": example[\"prompt\"]\n",
    "    }\n",
    "    templated_prompts.append(templated)\n",
    "\n",
    "print(f\"\u2705 Created {len(templated_prompts)} templated prompts\")\n",
    "print(f\"\\n\ud83d\udcdd Sample templated prompt (first 300 chars):\")\n",
    "print(templated_prompts[0][\"prompt\"][:300])\n",
    "print(\"...\")\n",
    "\n",
    "# Test validation\n",
    "test_valid = \"<reasoning>This is reasoning</reasoning><answer>This is answer</answer>\"\n",
    "test_invalid = \"This is just text without tags\"\n",
    "print(f\"\\n\u2705 Validation test:\")\n",
    "print(f\"   Valid format: {validate_xml_format(test_valid)}\")\n",
    "print(f\"   Invalid format: {validate_xml_format(test_invalid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgkWNF_EL1eG"
   },
   "source": [
    "### Tokenization and Batching\n",
    "\n",
    "Tokenize prompts and prepare batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "AInVAlZZL1eG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "484807ed-6cfd-464a-a353-0e4bd0553ea5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Tokenized 100 prompts\n",
      "   Max length: 512 tokens\n",
      "   Shape: (100, 512)\n",
      "\n",
      "\u2705 Final training dataset: 100 examples\n",
      "   Each example has: ['prompt', 'prompt_tokens', 'attention_mask', 'ground_truth', 'metadata']\n",
      "\n",
      "\u2705 Dataset validation: PASSED\n",
      "   All examples have required fields: prompt, ground_truth, metadata\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# Set maximum prompt length\n",
    "MAX_PROMPT_LENGTH = 512  # Adjust based on your needs (512 or 1024)\n",
    "MAX_RESPONSE_LENGTH = 512\n",
    "\n",
    "def tokenize_prompts(prompts: List[str], tokenizer, max_length: int = MAX_PROMPT_LENGTH):\n",
    "    \"\"\"\n",
    "    Tokenize prompts with padding and truncation.\n",
    "\n",
    "    Args:\n",
    "        prompts: List of prompt strings\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        max_length: Maximum token length\n",
    "\n",
    "    Returns:\n",
    "        Dict with input_ids and attention_mask\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "def create_training_batches(dataset: List[Dict], batch_size: int = 4):\n",
    "    \"\"\"\n",
    "    Create batches from dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: List of training examples\n",
    "        batch_size: Number of examples per batch\n",
    "\n",
    "    Returns:\n",
    "        List of batches, each batch is a list of examples\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "# Tokenize all prompts\n",
    "all_prompts = [ex[\"prompt\"] for ex in templated_prompts]\n",
    "tokenized_prompts = tokenize_prompts(all_prompts, tokenizer, MAX_PROMPT_LENGTH)\n",
    "\n",
    "print(f\"\u2705 Tokenized {len(all_prompts)} prompts\")\n",
    "print(f\"   Max length: {MAX_PROMPT_LENGTH} tokens\")\n",
    "print(f\"   Shape: {tokenized_prompts['input_ids'].shape}\")\n",
    "\n",
    "# Create final dataset for training\n",
    "training_dataset = []\n",
    "for i, ex in enumerate(templated_prompts):\n",
    "    training_dataset.append({\n",
    "        \"prompt\": ex[\"prompt\"],\n",
    "        \"prompt_tokens\": tokenized_prompts['input_ids'][i],\n",
    "        \"attention_mask\": tokenized_prompts['attention_mask'][i],\n",
    "        \"ground_truth\": ex[\"ground_truth\"],\n",
    "        \"metadata\": ex[\"metadata\"]\n",
    "    })\n",
    "\n",
    "print(f\"\\n\u2705 Final training dataset: {len(training_dataset)} examples\")\n",
    "print(f\"   Each example has: {list(training_dataset[0].keys())}\")\n",
    "\n",
    "# Validate dataset format\n",
    "required_fields = [\"prompt\", \"ground_truth\", \"metadata\"]\n",
    "all_valid = all(all(field in ex for field in required_fields) for ex in training_dataset)\n",
    "print(f\"\\n\u2705 Dataset validation: {'PASSED' if all_valid else 'FAILED'}\")\n",
    "\n",
    "if not all_valid:\n",
    "    print(\"\u274c Some examples missing required fields!\")\n",
    "else:\n",
    "    print(\"   All examples have required fields: prompt, ground_truth, metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMG3okLhL1eG"
   },
   "source": [
    "## \ud83c\udfaf Task 3: Implement Custom Reward Function\n",
    "\n",
    "Create a competition-compliant reward function that scores:\n",
    "1. **Answer Correctness** (35%): Match with ground truth (exact or Jaccard)\n",
    "2. **Legal Accuracy** (25%): Valid legal citation patterns (e.g., U.S.C., v., \u00a7)\n",
    "3. **Reasoning Coherence** (25%): Structural integrity and lack of repetition\n",
    "4. **Format Compliance** (10%): Proper XML `<reasoning>` and `<answer>` tags\n",
    "5. **Reasoning Length** (5%): Encouraging detailed analysis (>150 tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "h1NTvRVUL1eG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e53c1efc-9467-4772-9a6f-a4da4a61d5ae"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\uddea Testing XML extraction:\n",
      "\n",
      "Test 1:\n",
      "  Reasoning found: True\n",
      "  Answer found: True\n",
      "  Reasoning preview: Step by step analysis here...\n",
      "  Answer: Final answer\n",
      "\n",
      "Test 2:\n",
      "  Reasoning found: False\n",
      "  Answer found: False\n",
      "\n",
      "Test 3:\n",
      "  Reasoning found: False\n",
      "  Answer found: False\n",
      "\n",
      "Test 4:\n",
      "  Reasoning found: True\n",
      "  Answer found: True\n",
      "  Reasoning preview: Analysis with <term>nested</term> content...\n",
      "  Answer: Yes\n",
      "\n",
      "Test 5:\n",
      "  Reasoning found: True\n",
      "  Answer found: True\n",
      "  Reasoning preview: Line 1 of reasoning\n",
      "Line 2 of reasoning...\n",
      "  Answer: Final answer\n",
      "\n",
      "\u2705 XML extraction function tested with edge cases\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "def extract_xml_content(response: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extract content from <reasoning> and <answer> XML tags.\n",
    "\n",
    "    Args:\n",
    "        response: Model-generated response string\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (reasoning_content, answer_content)\n",
    "        Returns (None, None) if tags are malformed or missing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract reasoning\n",
    "        reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', response, re.DOTALL)\n",
    "        reasoning = reasoning_match.group(1).strip() if reasoning_match else None\n",
    "\n",
    "        # Extract answer\n",
    "        answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)\n",
    "        answer = answer_match.group(1).strip() if answer_match else None\n",
    "\n",
    "        return reasoning, answer\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error extracting XML content: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Test extraction with edge cases\n",
    "test_cases = [\n",
    "    # Valid case\n",
    "    \"<reasoning>Step by step analysis here</reasoning><answer>Final answer</answer>\",\n",
    "    # Missing tags\n",
    "    \"Just plain text without tags\",\n",
    "    # Partial tags\n",
    "    \"<reasoning>Incomplete reasoning\",\n",
    "    # Nested content\n",
    "    \"<reasoning>Analysis with <term>nested</term> content</reasoning><answer>Yes</answer>\",\n",
    "    # Multi-line\n",
    "    \"\"\"<reasoning>\n",
    "Line 1 of reasoning\n",
    "Line 2 of reasoning\n",
    "</reasoning>\n",
    "<answer>Final answer</answer>\"\"\"\n",
    "]\n",
    "\n",
    "print(\"\ud83e\uddea Testing XML extraction:\")\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    reasoning, answer = extract_xml_content(test)\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"  Reasoning found: {reasoning is not None}\")\n",
    "    print(f\"  Answer found: {answer is not None}\")\n",
    "    if reasoning:\n",
    "        print(f\"  Reasoning preview: {reasoning[:50]}...\")\n",
    "    if answer:\n",
    "        print(f\"  Answer: {answer}\")\n",
    "\n",
    "print(\"\\n\u2705 XML extraction function tested with edge cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "UzinMYgVL1eH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5e863100-8f79-4f96-e908-6822d361399e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Reward component functions defined:\n",
      "   - compute_format_reward (10%)\n",
      "   - compute_legal_accuracy_reward (25%)\n",
      "   - compute_reasoning_coherence_reward (25%)\n",
      "   - compute_answer_correctness_reward (35%)\n",
      "   - compute_reasoning_length_penalty (5%)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "\n",
    "def extract_xml_content(response: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extract content from <reasoning> and <answer> XML tags.\n",
    "\n",
    "    Args:\n",
    "        response: Model-generated response string\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (reasoning_content, answer_content)\n",
    "        Returns (None, None) if tags are malformed or missing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract reasoning\n",
    "        reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', response, re.DOTALL)\n",
    "        reasoning = reasoning_match.group(1).strip() if reasoning_match else None\n",
    "\n",
    "        # Extract answer\n",
    "        answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)\n",
    "        answer = answer_match.group(1).strip() if answer_match else None\n",
    "\n",
    "        return reasoning, answer\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error extracting XML content: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def compute_format_reward(response: str) -> float:\n",
    "    \"\"\"\n",
    "    Reward for valid XML format (10% weight).\n",
    "    \"\"\"\n",
    "    reasoning, answer = extract_xml_content(response)\n",
    "\n",
    "    # Check both tags present and have content\n",
    "    if reasoning is not None and answer is not None:\n",
    "        if len(reasoning.strip()) > 0 and len(answer.strip()) > 0:\n",
    "            return 1.0\n",
    "\n",
    "    return 0.0\n",
    "\n",
    "def compute_legal_accuracy_reward(response: str, query_context: str = \"\") -> float:\n",
    "    \"\"\"\n",
    "    Reward for using proper legal citation format (25% weight).\n",
    "    Checks for presence of standard legal citation patterns.\n",
    "    \"\"\"\n",
    "    reasoning, _ = extract_xml_content(response)\n",
    "    if not reasoning:\n",
    "        return 0.0\n",
    "\n",
    "    # Basic legal citation patterns\n",
    "    patterns = [\n",
    "        r'\\d+\\s+U\\.S\\.C\\.',       # US Code (e.g., 17 U.S.C.)\n",
    "        r'v\\.',                   # Case names (Plaintiff v. Defendant)\n",
    "        r'\u00a7',                     # Section symbol\n",
    "        r'Article\\s+[IVX]+',      # Articles\n",
    "        r'See\\s+also',            # Legal writing style\n",
    "        r'Id\\.',                  # Citation shorthand\n",
    "        r'Cir\\.',                 # Circuit courts\n",
    "        r'Cal\\.',                 # California codes (example)\n",
    "        r'Rev\\.',                 # Review\n",
    "    ]\n",
    "\n",
    "    matches = 0\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, reasoning, re.IGNORECASE):\n",
    "            matches += 1\n",
    "\n",
    "    # Cap at 1.0\n",
    "    return min(1.0, max(0.2, matches * 0.5) if matches > 0 else 0.0)\n",
    "\n",
    "def compute_reasoning_coherence_reward(response: str) -> float:\n",
    "    \"\"\"\n",
    "    Reward for coherence (25% weight).\n",
    "    Penalizes repetition and rewards structure.\n",
    "    \"\"\"\n",
    "    reasoning, _ = extract_xml_content(response)\n",
    "    if not reasoning:\n",
    "        return 0.0\n",
    "\n",
    "    # 1. Repetition penalty\n",
    "    sentences = [s.strip() for s in reasoning.split('.') if len(s.strip()) > 10]\n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "\n",
    "    unique_sentences = set(sentences)\n",
    "    repetition_ratio = len(unique_sentences) / len(sentences)\n",
    "\n",
    "    # 2. Structure heuristic\n",
    "    has_paragraphs = '\\n\\n' in reasoning\n",
    "    transitions = ['Therefore', 'However', 'Furthermore', 'Accordingly', 'Thus']\n",
    "    has_transitions = any(t in reasoning for t in transitions)\n",
    "\n",
    "    # Combine\n",
    "    score = repetition_ratio * 0.7 + (0.15 if has_paragraphs else 0.0) + (0.15 if has_transitions else 0.0)\n",
    "    return min(1.0, score)\n",
    "\n",
    "def compute_reasoning_length_penalty(response: str, tokenizer, min_tokens: int = 150) -> float:\n",
    "    \"\"\"\n",
    "    Reward for reasoning length (5% weight).\n",
    "    Targeting ~150+ tokens for detailed analysis.\n",
    "    \"\"\"\n",
    "    reasoning, _ = extract_xml_content(response)\n",
    "    if not reasoning:\n",
    "        return 0.0\n",
    "\n",
    "    # Tokenize reasoning to count tokens\n",
    "    tokens = tokenizer(reasoning, return_tensors=\"np\")[\"input_ids\"]\n",
    "    num_tokens = len(tokens[0])\n",
    "\n",
    "    # Return 1.0 if meets threshold, otherwise proportional\n",
    "    if num_tokens >= min_tokens:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return num_tokens / min_tokens\n",
    "\n",
    "def compute_answer_correctness_reward(response: str, ground_truth: str, tokenizer) -> float:\n",
    "    \"\"\"\n",
    "    Reward based on answer correctness (35% weight).\n",
    "    \"\"\"\n",
    "    _, answer = extract_xml_content(response)\n",
    "\n",
    "    if answer is None:\n",
    "        return 0.0\n",
    "\n",
    "    # Normalize for comparison\n",
    "    answer_norm = answer.lower().strip()\n",
    "    ground_truth_norm = ground_truth.lower().strip()\n",
    "\n",
    "    # Check exact match\n",
    "    if answer_norm == ground_truth_norm:\n",
    "        return 1.0\n",
    "\n",
    "    # Tokenize both for overlap calculation\n",
    "    answer_tokens = set(tokenizer.tokenize(answer_norm))\n",
    "    truth_tokens = set(tokenizer.tokenize(ground_truth_norm))\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    if len(answer_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = len(answer_tokens & truth_tokens)\n",
    "    union = len(answer_tokens | truth_tokens)\n",
    "\n",
    "    jaccard = intersection / union if union > 0 else 0.0\n",
    "\n",
    "    return jaccard\n",
    "\n",
    "print(\"\u2705 Reward component functions defined:\")\n",
    "print(\"   - compute_format_reward (10%)\")\n",
    "print(\"   - compute_legal_accuracy_reward (25%)\")\n",
    "print(\"   - compute_reasoning_coherence_reward (25%)\")\n",
    "print(\"   - compute_answer_correctness_reward (35%)\")\n",
    "print(\"   - compute_reasoning_length_penalty (5%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NTDgkt1pL1eH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d7abcb82-41ca-472f-88aa-dae6faf9680d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\uddea Testing reward function...\n",
      "\n",
      "\ud83d\udcca Example 0 reward breakdown:\n",
      "   Correctness (0.35): 1.00\n",
      "   Coherence (0.25): 0.85\n",
      "   Legal (0.25): 1.00\n",
      "   Format (0.1): 1.00\n",
      "   Length (0.05): 0.35\n",
      "   TOTAL: 0.93\n",
      "\n",
      "\u2705 Reward function test complete\n",
      "   Test reward: 0.93\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def composite_reward_function(\n",
    "    prompts: List[str],\n",
    "    completions: List[str],\n",
    "    metadata: List[Dict],\n",
    "    tokenizer\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Competition-compliant composite reward function.\n",
    "\n",
    "    Weights:\n",
    "    - Answer Correctness: 35%\n",
    "    - Reasoning Coherence: 25%\n",
    "    - Legal Accuracy: 25%\n",
    "    - Format Compliance: 10%\n",
    "    - Length Penalty: 5%\n",
    "    \"\"\"\n",
    "    # Competition Weights\n",
    "    W_CORRECTNESS = 0.35\n",
    "    W_COHERENCE = 0.25\n",
    "    W_LEGAL = 0.25\n",
    "    W_FORMAT = 0.10\n",
    "    W_LENGTH = 0.05\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for i, (prompt, completion, meta) in enumerate(zip(prompts, completions, metadata)):\n",
    "        # Compute each reward component\n",
    "        r_format = compute_format_reward(completion)\n",
    "        r_correctness = compute_answer_correctness_reward(completion, meta.get(\"ground_truth\", \"\"), tokenizer)\n",
    "        r_coherence = compute_reasoning_coherence_reward(completion)\n",
    "        r_legal = compute_legal_accuracy_reward(completion)\n",
    "        r_length = compute_reasoning_length_penalty(completion, tokenizer)\n",
    "\n",
    "        # Aggregate rewards\n",
    "        total_reward = (\n",
    "            W_CORRECTNESS * r_correctness +\n",
    "            W_COHERENCE * r_coherence +\n",
    "            W_LEGAL * r_legal +\n",
    "            W_FORMAT * r_format +\n",
    "            W_LENGTH * r_length\n",
    "        )\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "        # Log breakdown for first few examples\n",
    "        if i < 3:\n",
    "            print(f\"\\n\ud83d\udcca Example {i} reward breakdown:\")\n",
    "            print(f\"   Correctness ({W_CORRECTNESS}): {r_correctness:.2f}\")\n",
    "            print(f\"   Coherence ({W_COHERENCE}): {r_coherence:.2f}\")\n",
    "            print(f\"   Legal ({W_LEGAL}): {r_legal:.2f}\")\n",
    "            print(f\"   Format ({W_FORMAT}): {r_format:.2f}\")\n",
    "            print(f\"   Length ({W_LENGTH}): {r_length:.2f}\")\n",
    "            print(f\"   TOTAL: {total_reward:.2f}\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def tunix_reward_wrapper(prompts: List[str], outputs: List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Wrapper function matching Tunix RewardFn signature.\n",
    "    \"\"\"\n",
    "    # Build metadata from training dataset\n",
    "    metadata = []\n",
    "    for prompt in prompts:\n",
    "        # Find matching ground truth from training_dataset\n",
    "        found = False\n",
    "        for example in training_dataset:\n",
    "            if example[\"prompt\"] in prompt or prompt in example[\"prompt\"]:\n",
    "                metadata.append({\"ground_truth\": example[\"ground_truth\"]})\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            metadata.append({\"ground_truth\": \"\"})\n",
    "\n",
    "    return composite_reward_function(prompts, outputs, metadata, tokenizer)\n",
    "\n",
    "# Test reward function\n",
    "print(\"\ud83e\uddea Testing reward function...\")\n",
    "test_prompts = [\"Test question\"]\n",
    "test_completions = [\n",
    "    \"<reasoning>This is a detailed legal analysis with sufficient tokens to explain the reasoning behind the answer. We consider precedent, statutory law (17 U.S.C.), and policy implications. Furthermore, the court in Smith v. Jones held that detailed analysis is required.</reasoning><answer>Yes, it is enforceable.</answer>\"\n",
    "]\n",
    "test_metadata = [{\"ground_truth\": \"Yes, it is enforceable.\"}]\n",
    "\n",
    "test_rewards = composite_reward_function(test_prompts, test_completions, test_metadata, tokenizer)\n",
    "print(f\"\\n\u2705 Reward function test complete\")\n",
    "print(f\"   Test reward: {test_rewards[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qLq5dwQuL1eH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "57a38fcb-23df-4829-c563-d094f068a0c6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udce6 Verifying Tunix installation...\n",
      "\u2705 Tunix installed: 0.1.5\n",
      "\n",
      "\ud83d\udccb Checking Tunix submodules:\n",
      "   \u2705 tunix.rl.grpo.grpo_learner\n",
      "   \u2705 tunix.rl.rl_cluster\n",
      "   \u2705 tunix.models.gemma\n",
      "\n",
      "\u2705 All Tunix modules available!\n",
      "\n",
      "\ud83d\udca1 Note: LoRA is configured through hyperparameters (rank, alpha) - no separate PEFT module needed.\n",
      "\n",
      "\ud83d\udcca JAX Backend Status:\n",
      "   JAX version: 0.8.2\n",
      "   Backend: tpu\n",
      "   Devices: 1 (tpu)\n",
      "\n",
      "\u2705 Environment verified - ready for training setup!\n"
     ]
    }
   ],
   "source": [
    "# Verify Tunix installation before training setup\n",
    "print(\"\ud83d\udce6 Verifying Tunix installation...\")\n",
    "\n",
    "import sys\n",
    "\n",
    "# Check Tunix availability\n",
    "try:\n",
    "    import tunix\n",
    "    print(f\"\u2705 Tunix installed: {tunix.__version__ if hasattr(tunix, '__version__') else 'version unknown'}\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Tunix not available: {e}\")\n",
    "    print(\"\\n\ud83d\udd27 To install Tunix:\")\n",
    "    print(\"   !pip install 'google-tunix[tpu]>=0.1.0'\")\n",
    "    print(\"   Then restart runtime and run this cell again.\")\n",
    "    raise\n",
    "\n",
    "# Check required submodules\n",
    "modules_to_check = [\n",
    "    (\"tunix.rl.grpo.grpo_learner\", \"GRPOConfig, GRPOLearner\"),\n",
    "    (\"tunix.rl.rl_cluster\", \"RLCluster\"),\n",
    "    (\"tunix.models.gemma\", \"GemmaForCausalLM\"),\n",
    "]\n",
    "\n",
    "print(\"\\n\ud83d\udccb Checking Tunix submodules:\")\n",
    "all_available = True\n",
    "for module_path, expected_exports in modules_to_check:\n",
    "    try:\n",
    "        module = __import__(module_path, fromlist=[''])\n",
    "        print(f\"   \u2705 {module_path}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"   \u274c {module_path}: {e}\")\n",
    "        all_available = False\n",
    "\n",
    "if all_available:\n",
    "    print(\"\\n\u2705 All Tunix modules available!\")\n",
    "    print(\"\\n\ud83d\udca1 Note: LoRA is configured through hyperparameters (rank, alpha) - no separate PEFT module needed.\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f Some modules not available. Check Tunix version and installation.\")\n",
    "    print(\"   The training cells may need adaptation for your Tunix version.\")\n",
    "\n",
    "# Check JAX backend\n",
    "print(\"\\n\ud83d\udcca JAX Backend Status:\")\n",
    "import jax\n",
    "print(f\"   JAX version: {jax.__version__}\")\n",
    "print(f\"   Backend: {jax.default_backend()}\")\n",
    "print(f\"   Devices: {jax.device_count()} ({jax.devices()[0].platform if jax.devices() else 'none'})\")\n",
    "\n",
    "print(\"\\n\u2705 Environment verified - ready for training setup!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_MODf-BL1eH"
   },
   "source": [
    "## \ud83d\ude80 Task 4: Configure and Execute GRPO Training\n",
    "\n",
    "Set up LoRA adapters and run GRPO training on TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6RxEzBSLL1eH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d8938070-f3bb-4686-962d-e3d941eb47e2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Configuration defined:\n",
      "\n",
      "\ud83d\udd27 LoRA Configuration:\n",
      "   rank: 16\n",
      "   alpha: 32\n",
      "   dropout: 0.05\n",
      "   target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
      "\n",
      "\ud83c\udfaf GRPO Configuration:\n",
      "   num_generations: 4\n",
      "   max_tokens_to_generate: 512\n",
      "   beta: 0.04\n",
      "   epsilon: 0.2\n",
      "   learning_rate: 1e-05\n",
      "   batch_size: 4\n",
      "   num_iterations: 2\n",
      "   eval_every_n_steps: 50\n",
      "   checkpoint_every_n_steps: 100\n",
      "\n",
      "\ud83d\udcca Training Configuration:\n",
      "   warmup_steps: 10\n",
      "   weight_decay: 0.01\n",
      "   max_grad_norm: 1.0\n",
      "   log_every_n_steps: 10\n",
      "\n",
      "\ud83d\udca1 Hyperparameter Rationale:\n",
      "   - LoRA rank=16: Balance between capacity and memory efficiency\n",
      "   - num_generations=4: Standard for GRPO variance reduction\n",
      "   - beta=0.04: Conservative KL penalty to prevent policy divergence\n",
      "   - learning_rate=1e-5: Safe starting point for LoRA fine-tuning\n",
      "   - max_tokens_to_generate=512: Sufficient for detailed legal reasoning\n"
     ]
    }
   ],
   "source": [
    "# LoRA Hyperparameters for parameter-efficient fine-tuning\n",
    "LORA_CONFIG = {\n",
    "    \"rank\": 16,           # LoRA rank (16 or 32 recommended)\n",
    "    \"alpha\": 32,          # LoRA alpha (typically 2x rank)\n",
    "    \"dropout\": 0.05,      # LoRA dropout for regularization\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
    "}\n",
    "\n",
    "# GRPO Configuration matching Tunix GRPOConfig parameters\n",
    "# Reference: https://tunix.readthedocs.io/en/latest/api/grpo.html\n",
    "GRPO_CONFIG = {\n",
    "    # Rollout settings\n",
    "    \"num_generations\": 4,           # Number of response samples per prompt for GRPO\n",
    "    \"max_tokens_to_generate\": 512,  # Maximum tokens for rollout generation\n",
    "\n",
    "    # GRPO algorithm hyperparameters\n",
    "    \"beta\": 0.04,                   # KL penalty coefficient (prevents policy divergence)\n",
    "    \"epsilon\": 0.2,                 # PPO-style clipping parameter\n",
    "\n",
    "    # Training settings\n",
    "    \"learning_rate\": 1e-5,          # Learning rate for LoRA parameters\n",
    "    \"batch_size\": 4,                # Batch size per TPU core (adjust for memory)\n",
    "    \"num_iterations\": 2,            # Number of training epochs/iterations\n",
    "\n",
    "    # Evaluation and checkpointing\n",
    "    \"eval_every_n_steps\": 50,       # Evaluate model every N steps\n",
    "    \"checkpoint_every_n_steps\": 100, # Save checkpoint every N steps\n",
    "}\n",
    "\n",
    "# Training configuration for RLCluster\n",
    "TRAINING_CONFIG = {\n",
    "    \"warmup_steps\": 10,             # Learning rate warmup steps\n",
    "    \"weight_decay\": 0.01,           # Weight decay for regularization\n",
    "    \"max_grad_norm\": 1.0,           # Gradient clipping threshold\n",
    "    \"log_every_n_steps\": 10,        # Log metrics every N steps\n",
    "}\n",
    "\n",
    "print(\"\u2705 Configuration defined:\")\n",
    "print(\"\\n\ud83d\udd27 LoRA Configuration:\")\n",
    "for k, v in LORA_CONFIG.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "print(\"\\n\ud83c\udfaf GRPO Configuration:\")\n",
    "for k, v in GRPO_CONFIG.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "print(\"\\n\ud83d\udcca Training Configuration:\")\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Hyperparameter Rationale:\")\n",
    "print(\"   - LoRA rank=16: Balance between capacity and memory efficiency\")\n",
    "print(\"   - num_generations=4: Standard for GRPO variance reduction\")\n",
    "print(\"   - beta=0.04: Conservative KL penalty to prevent policy divergence\")\n",
    "print(\"   - learning_rate=1e-5: Safe starting point for LoRA fine-tuning\")\n",
    "print(\"   - max_tokens_to_generate=512: Sufficient for detailed legal reasoning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-Fd7NKpCxrG"
   },
   "source": [
    "## \u270f\ufe0f Phase II Validation: Training Configuration Review\n",
    "\n",
    "## \u2705 Phase II Validation: Configuration Review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "qdi5PuYxCxrG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0e1aebad-7f4d-4f86-9ac5-ff5d5659de80"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "\u2699\ufe0f  TRAINING CONFIGURATION REVIEW\n",
      "============================================================\n",
      "\n",
      "\ud83c\udfaf GRPO Configuration:\n",
      "   num_generations: 4\n",
      "   max_tokens_to_generate: 512\n",
      "   beta: 0.04\n",
      "   epsilon: 0.2\n",
      "   learning_rate: 1e-05\n",
      "   batch_size: 4\n",
      "   num_iterations: 2\n",
      "   eval_every_n_steps: 50\n",
      "   checkpoint_every_n_steps: 100\n",
      "\n",
      "\u2705 Configuration looks good\n",
      "\n",
      "\ud83d\udd27 LoRA Configuration:\n",
      "   rank: 16\n",
      "   alpha: 32\n",
      "   dropout: 0.05\n",
      "   target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
      "   \u2705 LoRA rank in optimal range\n",
      "\n",
      "\ud83d\udcca Training Configuration:\n",
      "   warmup_steps: 10\n",
      "   weight_decay: 0.01\n",
      "   max_grad_norm: 1.0\n",
      "   log_every_n_steps: 10\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Phase 2 Validation: Configuration Review\n",
    "print(\"=\" * 60)\n",
    "print(\"\u2699\ufe0f  TRAINING CONFIGURATION REVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# GRPO Config\n",
    "if 'GRPO_CONFIG' in globals():\n",
    "    print(\"\\n\ud83c\udfaf GRPO Configuration:\")\n",
    "    for key, value in GRPO_CONFIG.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    # Validate ranges\n",
    "    config_warnings = []\n",
    "\n",
    "    if GRPO_CONFIG.get('learning_rate', 0) > 1e-4:\n",
    "        config_warnings.append(\"Learning rate may be too high (> 1e-4)\")\n",
    "    if GRPO_CONFIG.get('batch_size', 0) > 8:\n",
    "        config_warnings.append(\"Batch size may cause OOM on TPU v2-8\")\n",
    "    if GRPO_CONFIG.get('num_generations', 0) > 4:\n",
    "        config_warnings.append(\"High num_generations may cause OOM\")\n",
    "\n",
    "    if config_warnings:\n",
    "        print(\"\\n\u26a0\ufe0f  Configuration Warnings:\")\n",
    "        for warning in config_warnings:\n",
    "            print(f\"   \u2022 {warning}\")\n",
    "    else:\n",
    "        print(\"\\n\u2705 Configuration looks good\")\n",
    "else:\n",
    "    print(\"\\n\u274c GRPO_CONFIG not found\")\n",
    "\n",
    "# LoRA Config\n",
    "if 'LORA_CONFIG' in globals():\n",
    "    print(\"\\n\ud83d\udd27 LoRA Configuration:\")\n",
    "    for key, value in LORA_CONFIG.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    # Validate LoRA settings\n",
    "    rank = LORA_CONFIG.get('rank', 0)\n",
    "    if rank < 8:\n",
    "        print(\"   \u26a0\ufe0f  LoRA rank < 8 may limit model capacity\")\n",
    "    elif rank > 32:\n",
    "        print(\"   \u26a0\ufe0f  LoRA rank > 32 may increase memory usage\")\n",
    "    else:\n",
    "        print(\"   \u2705 LoRA rank in optimal range\")\n",
    "else:\n",
    "    print(\"\\n\u274c LORA_CONFIG not found\")\n",
    "\n",
    "# Training Config\n",
    "if 'TRAINING_CONFIG' in globals():\n",
    "    print(\"\\n\ud83d\udcca Training Configuration:\")\n",
    "    for key, value in TRAINING_CONFIG.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f  TRAINING_CONFIG not found (may be optional)\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziCWkm-PL1eH"
   },
   "source": [
    "### \ud83d\udd27 Initialize Training Components\n",
    "\n",
    "This section sets up the Tunix GRPO training infrastructure:\n",
    "\n",
    "1. **Import Tunix modules**: GRPOConfig, GRPOLearner, RLCluster\n",
    "2. **Load and configure models**: Actor (trainable) and Reference (frozen) policies\n",
    "3. **Setup TPU mesh**: Configure sharding for distributed training\n",
    "4. **Initialize learner**: Create GRPOLearner with reward function\n",
    "\n",
    "**Prerequisites**:\n",
    "- TPU runtime initialized (verified in Step 2)\n",
    "- Model downloaded (completed in Step 4)\n",
    "- Reward function defined (completed above)\n",
    "- Training dataset prepared (completed above)\n",
    "\n",
    "**Documentation**:\n",
    "- [Tunix GRPO Guide](https://tunix.readthedocs.io/en/latest/tutorials/grpo.html)\n",
    "- [Official GRPO Gemma Example](https://github.com/google/tunix/tree/main/examples/grpo_gemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "B9bHhh9kL1eH",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 775
    },
    "outputId": "2fabb1df-7b5e-4eb5-e380-ea1e64577d01"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "\ufffd\ufffd TASK 2: TPU Mesh & Model Initialization\n",
      "============================================================\n",
      "\n",
      "\ud83d\udce6 Importing Tunix modules...\n",
      "\u2705 Tunix modules imported successfully!\n",
      "\n",
      "\ud83d\udcc1 Creating checkpoint directories...\n",
      "   \u2705 ./checkpoints\n",
      "   \u2705 ./final_checkpoint\n",
      "   \u2705 ./kaggle_upload\n",
      "\n",
      "\ud83d\udd27 Configuring JAX Mesh for TPU...\n",
      "   Detected 1 TPU device(s)\n",
      "   Using v6e-1 configuration: MESH_COUNTS = (1, 1)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Value 'TPU_0(process=0,(0,0,0,0))' with dtype object is not a valid JAX array type. Only arrays of numeric types are supported by JAX.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m         \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret 'TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)' as a data type",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/numpy/array_constructors.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin, device, out_sharding)\u001b[0m\n\u001b[1;32m    244\u001b[0m       dtype = (\n\u001b[0;32m--> 245\u001b[0;31m           \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlattice_result_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36mlattice_result_type\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlattice_result_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m   \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dtype_and_weaktype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlattice_result_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m   \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dtype_and_weaktype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36m_dtype_and_weaktype\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    670\u001b[0m   \u001b[0;34m\"\"\"Return a (dtype, weak_type) tuple for the given input.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_weak_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_weakly_typed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot determine dtype of {x}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_jax_dtype_set\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot determine dtype of TPU_0(process=0,(0,0,0,0))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-570397327.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Create mesh with data parallelism sharding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mmesh_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMESH_COUNTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mmesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmesh_devices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fsdp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   \u2705 Mesh created with shape: {mesh.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/numpy/array_constructors.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin, device, out_sharding)\u001b[0m\n\u001b[1;32m    251\u001b[0m       \u001b[0;31m# This is rare, so we only handle it if the normal path fails.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m       \u001b[0mleaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_convert_to_array_if_dtype_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mleaf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlattice_result_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36mlattice_result_type\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlattice_result_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m   \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dtype_and_weaktype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0mout_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlattice_result_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m   \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dtype_and_weaktype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0mout_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36m_dtype_and_weaktype\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_dtype_and_weaktype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m   \u001b[0;34m\"\"\"Return a (dtype, weak_type) tuple for the given input.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_weak_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_weakly_typed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_type_promotion_lattice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx64\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJAXType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJAXType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1004\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot determine dtype of {x}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_jax_dtype_set\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m     raise TypeError(f\"Value '{x}' with dtype {dt} is not a valid JAX array \"\n\u001b[0m\u001b[1;32m   1007\u001b[0m                     \"type. Only arrays of numeric types are supported by JAX.\")\n\u001b[1;32m   1008\u001b[0m   \u001b[0;31m# TODO(jakevdp): fix return type annotation and remove this ignore.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Value 'TPU_0(process=0,(0,0,0,0))' with dtype object is not a valid JAX array type. Only arrays of numeric types are supported by JAX."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TASK 2: TPU Mesh Setup + Model Initialization with LoRA\n",
    "# ============================================================\n",
    "# IMPORTANT: This cell uses the CORRECT tunix 0.1.5 API based on:\n",
    "# https://github.com/google/tunix/blob/main/examples/grpo_gemma.ipynb\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"\ud83c\udfaf TASK 2: TPU Mesh & Model Initialization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import Tunix GRPO modules\n",
    "print(\"\\n\ud83d\udce6 Importing Tunix modules...\")\n",
    "\n",
    "try:\n",
    "    from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "    from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "    from tunix.rl.rollout import base_rollout\n",
    "    # CORRECT imports for Gemma3 model loading\n",
    "    from tunix.models.gemma3 import model as gemma_lib\n",
    "    from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
    "    from tunix.models.gemma3 import params as gemma_params\n",
    "    from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "    from tunix.generate import sampler as sampler_lib\n",
    "    from flax import nnx\n",
    "    import qwix\n",
    "    print(\"\u2705 Tunix modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Tunix import failed: {e}\")\n",
    "    print(\"\\n\ud83d\udd27 Troubleshooting:\")\n",
    "    print(\"   1. Verify Tunix is installed: pip install git+https://github.com/google/tunix\")\n",
    "    print(\"   2. Restart runtime after installation\")\n",
    "    print(\"   3. Check Tunix version compatibility\")\n",
    "    raise\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "import os\n",
    "\n",
    "# ===== Create checkpoint directories =====\n",
    "print(\"\\n\ud83d\udcc1 Creating checkpoint directories...\")\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "FINAL_CHECKPOINT_DIR = \"./final_checkpoint\"\n",
    "KAGGLE_UPLOAD_DIR = \"./kaggle_upload\"\n",
    "\n",
    "for dir_path in [CHECKPOINT_DIR, FINAL_CHECKPOINT_DIR, KAGGLE_UPLOAD_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"   \u2705 {dir_path}\")\n",
    "\n",
    "# ===== Configure JAX Mesh for v6e-1 TPU =====\n",
    "print(\"\\n\ud83d\udd27 Configuring JAX Mesh for TPU...\")\n",
    "devices = jax.devices()\n",
    "num_devices = len(devices)\n",
    "print(f\"   Detected {num_devices} TPU device(s)\")\n",
    "\n",
    "# Set mesh counts for v6e-1 (single device)\n",
    "if num_devices == 1:\n",
    "    MESH_COUNTS = (1, 1)\n",
    "    print(\"   Using v6e-1 configuration: MESH_COUNTS = (1, 1)\")\n",
    "elif num_devices == 8:\n",
    "    MESH_COUNTS = (1, 4)  # v2-8 or v3-8 (from official example)\n",
    "    print(\"   Using v2-8/v3-8 configuration: MESH_COUNTS = (1, 4)\")\n",
    "else:\n",
    "    MESH_COUNTS = (num_devices, 1)\n",
    "    print(f\"   Using custom configuration: MESH_COUNTS = ({num_devices}, 1)\")\n",
    "\n",
    "# Create mesh using jax.make_mesh (from official example)\n",
    "MESH = [MESH_COUNTS, (\"fsdp\", \"tp\")]\n",
    "mesh = jax.make_mesh(*MESH, axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[0]))\n",
    "print(f\"   \u2705 Mesh created with shape: {mesh.shape}\")\n",
    "print(f\"   Axis names: {mesh.axis_names}\")\n",
    "\n",
    "# ===== Load tokenizer =====\n",
    "print(\"\\n\ud83d\udcdd Loading tokenizer...\")\n",
    "# Tunix uses its own tokenizer from GCS (not HuggingFace AutoTokenizer)\n",
    "GEMMA_TOKENIZER_PATH = \"gs://gemma-data/tokenizers/tokenizer_gemma3.model\"\n",
    "\n",
    "try:\n",
    "    tunix_tokenizer = tokenizer_lib.Tokenizer(tokenizer_path=GEMMA_TOKENIZER_PATH)\n",
    "    eos_token_id = tunix_tokenizer.eos_id()\n",
    "    print(f\"   \u2705 Tunix tokenizer loaded from: {GEMMA_TOKENIZER_PATH}\")\n",
    "    print(f\"   EOS token ID: {eos_token_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"   \u26a0\ufe0f Could not load Tunix tokenizer: {e}\")\n",
    "    print(\"   Falling back to HuggingFace tokenizer...\")\n",
    "    tunix_tokenizer = tokenizer  # Use previously loaded HF tokenizer\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Also load EOS tokens from generation config if available\n",
    "import json\n",
    "EOS_TOKENS = []\n",
    "generation_config_path = os.path.join(model_path, \"generation_config.json\")\n",
    "if os.path.exists(generation_config_path):\n",
    "    with open(generation_config_path, \"r\") as f:\n",
    "        generation_configs = json.load(f)\n",
    "    EOS_TOKENS = generation_configs.get(\"eos_token_id\", [])\n",
    "    print(f\"   Additional EOS tokens from config: {EOS_TOKENS}\")\n",
    "if eos_token_id not in EOS_TOKENS:\n",
    "    EOS_TOKENS.append(eos_token_id)\n",
    "\n",
    "# ===== Load base model configuration =====\n",
    "print(\"\\n\ud83d\udccb Loading model configuration...\")\n",
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Use the CORRECT API: ModelConfig factory method (not from_pretrained)\n",
    "if \"gemma-3-270m\" in MODEL_ID:\n",
    "    model_config = gemma_lib.ModelConfig.gemma3_270m()\n",
    "    print(\"   Using Gemma3 270M configuration\")\n",
    "elif \"gemma-3-1b\" in MODEL_ID:\n",
    "    model_config = gemma_lib.ModelConfig.gemma3_1b()\n",
    "    print(\"   Using Gemma3 1B configuration\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model id: {MODEL_ID}\")\n",
    "\n",
    "# ===== LoRA Configuration =====\n",
    "print(\"\\n\ud83c\udfad Configuring LoRA...\")\n",
    "LORA_RANK = 64  # From official example\n",
    "LORA_ALPHA = 64.0  # From official example\n",
    "LORA_TARGET_MODULES = [\n",
    "    \".*q_einsum\",       # Query projection\n",
    "    \".*kv_einsum\",      # Key-Value projection\n",
    "    \".*gate_proj\",      # MLP gate\n",
    "    \".*down_proj\",      # MLP down projection\n",
    "    \".*up_proj\",        # MLP up projection\n",
    "    \".*attn_vec_einsum\" # Attention output\n",
    "]\n",
    "\n",
    "print(f\"   LoRA Configuration:\")\n",
    "print(f\"     Rank: {LORA_RANK}\")\n",
    "print(f\"     Alpha: {LORA_ALPHA}\")\n",
    "print(f\"     Target modules: {LORA_TARGET_MODULES}\")\n",
    "\n",
    "# ===== Load base model using CORRECT tunix API =====\n",
    "print(\"\\n\ud83d\udce5 Loading Gemma3 model from safetensors...\")\n",
    "\n",
    "with mesh:\n",
    "    # Use params_safetensors_lib.create_model_from_safe_tensors (CORRECT API)\n",
    "    gemma3 = params_safetensors_lib.create_model_from_safe_tensors(\n",
    "        model_path, model_config, mesh\n",
    "    )\n",
    "    print(\"   \u2705 Base model loaded from safetensors\")\n",
    "    \n",
    "    # Display model structure\n",
    "    nnx.display(gemma3)\n",
    "\n",
    "# ===== Apply LoRA to create policy model =====\n",
    "print(\"\\n\ud83d\udd27 Applying LoRA to create policy model...\")\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    \"\"\"Apply LoRA to base model using qwix.\"\"\"\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        module_path=\"|\".join(LORA_TARGET_MODULES),\n",
    "        rank=LORA_RANK,\n",
    "        alpha=LORA_ALPHA,\n",
    "    )\n",
    "    \n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, **model_input\n",
    "    )\n",
    "    \n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "    \n",
    "    return lora_model\n",
    "\n",
    "actor_model = get_lora_model(gemma3, mesh=mesh)\n",
    "print(\"   \u2705 LoRA policy model created\")\n",
    "nnx.display(actor_model)\n",
    "\n",
    "# ===== Reference model (base model without LoRA) =====\n",
    "print(\"\\n\ud83d\udccb Reference model uses base gemma3 (frozen, no LoRA)\")\n",
    "reference_model = gemma3  # Reference is the base model\n",
    "print(\"   \u2705 Reference model ready (frozen)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2705 TASK 2 COMPLETE: TPU Mesh & Models Initialized\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Actor model: LoRA rank={LORA_RANK}, alpha={LORA_ALPHA}\")\n",
    "print(f\"   Reference model: Frozen base model (gemma3)\")\n",
    "print(f\"   Mesh: {mesh.shape} on {num_devices} device(s)\")\n",
    "print(f\"   Tokenizer: Tunix Gemma3 tokenizer\")\n",
    "print(f\"   Checkpoint directories: {CHECKPOINT_DIR}, {FINAL_CHECKPOINT_DIR}, {KAGGLE_UPLOAD_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqVM2k-LL1eH"
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "# ============================================================\n",
    "# TASKS 3 & 4: RLCluster, GRPOLearner, and Training Execution\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"\ud83c\udfaf TASK 3: RLCluster & Reward Function Configuration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "\n",
    "# ===== Define role-to-mesh mapping =====\n",
    "print(\"\\n\ud83d\udd27 Configuring role-to-mesh mapping...\")\n",
    "from tunix.rl.rl_cluster import Role\n",
    "\n",
    "role_to_mesh = {\n",
    "    Role.ACTOR: mesh,\n",
    "    Role.REFERENCE: mesh,\n",
    "    Role.ROLLOUT: mesh,\n",
    "}\n",
    "print(\"   \u2705 Role-to-mesh mapping configured\")\n",
    "print(f\"     ACTOR: {mesh.shape}\")\n",
    "print(f\"     REFERENCE: {mesh.shape}\")\n",
    "print(f\"     ROLLOUT: {mesh.shape}\")\n",
    "\n",
    "# ===== Configure RolloutConfig =====\n",
    "print(\"\\n\ud83d\udd27 Configuring RolloutConfig...\")\n",
    "# Fix: RolloutConfig moved to base_rollout in newer Tunix versions\n",
    "from tunix.rl.rollout.base_rollout import RolloutConfig\n",
    "rollout_config = RolloutConfig(\n",
    "    max_tokens_to_generate=512,\n",
    "    max_prompt_length=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    eos_tokens=EOS_TOKENS,  # Use full EOS token list\n",
    "    # vLLM TPU backend configuration\n",
    "    rollout_vllm_tpu_backend_type=\"jax\",\n",
    "    rollout_vllm_hbm_utilization=0.8,\n",
    "    rollout_vllm_init_with_random_weights=False,\n",
    ")\n",
    "print(\"   \u2705 RolloutConfig created:\")\n",
    "print(f\"     max_tokens: {rollout_config.max_tokens_to_generate}\")\n",
    "print(f\"     max_prompt_length: {rollout_config.max_prompt_length}\")\n",
    "print(f\"     temperature: {rollout_config.temperature}\")\n",
    "print(f\"     top_p: {rollout_config.top_p}\")\n",
    "\n",
    "# ===== Configure RLTrainingConfig =====\n",
    "print(\"\\n\ud83d\udd27 Configuring RLTrainingConfig...\")\n",
    "# Fix: RLTrainingConfig requires optax optimizer object\n",
    "optimizer = optax.adamw(learning_rate=1e-5)\n",
    "training_config = rl_cluster_lib.RLTrainingConfig(\n",
    "    actor_optimizer=optimizer,\n",
    "    max_steps=500,\n",
    "    eval_every_n_steps=50,  # Required by Tunix 0.1.5+\n",
    "    mini_batch_size=4,\n",
    "    # Checkpointing configuration\n",
    "    checkpoint_root_directory=CHECKPOINT_DIR,\n",
    "    # Note: granular checkpoint options (save_interval) require specialized config objects omitted here for compatibility\n",
    ")\n",
    "print(\"   \u2705 RLTrainingConfig created:\")\n",
    "print(f\"     optimizer: adamw, lr: 1e-5\")\n",
    "print(f\"     max_steps: 500\")\n",
    "print(f\"     mini_batch_size: 4\")\n",
    "print(f\"     checkpoint every: 50 steps\")\n",
    "\n",
    "# ===== Create ClusterConfig and RLCluster =====\n",
    "print(\"\\n\ud83d\udd27 Creating RLCluster...\")\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh=role_to_mesh,\n",
    "    rollout_engine=\"vllm\",\n",
    "    offload_to_cpu=False,\n",
    "    training_config=training_config,\n",
    "    rollout_config=rollout_config,\n",
    ")\n",
    "\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    # Models removed from init - passed to Learner/Trainer or attached later in Tunix 0.1+\n",
    "    tokenizer=tunix_tokenizer,\n",
    "    config=cluster_config,\n",
    ")\n",
    "print(\"   \u2705 RLCluster created with vLLM rollout engine\")\n",
    "\n",
    "# ===== Adapt Reward Function to Tunix Interface =====\n",
    "print(\"\\n\ud83c\udfaf Adapting reward function for Tunix...\")\n",
    "\n",
    "def tunix_reward_function(\n",
    "    prompts: List[str],\n",
    "    completions: List[List[Dict[str, str]]],\n",
    "    **kwargs\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Tunix-compatible reward function for GRPO training.\n",
    "\n",
    "    Computes composite reward with weights:\n",
    "    - 35% correctness\n",
    "    - 25% legal_accuracy\n",
    "    - 25% coherence\n",
    "    - 10% format\n",
    "    - 5% length\n",
    "\n",
    "    Args:\n",
    "        prompts: List of input prompts\n",
    "        completions: Nested list of completion dicts with 'text' key\n",
    "        **kwargs: Additional metadata (may include ground_truth)\n",
    "\n",
    "    Returns:\n",
    "        List of reward scores (one per completion)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    # Get ground truth from global context if available\n",
    "    global _current_ground_truth\n",
    "    gt_lookup = _current_ground_truth if '_current_ground_truth' in dir() else kwargs.get(\"ground_truth\", [])\n",
    "\n",
    "    for prompt_idx, prompt_completions in enumerate(completions):\n",
    "        for completion in prompt_completions:\n",
    "            # Extract completion text\n",
    "            if isinstance(completion, dict):\n",
    "                text = completion.get(\"text\", completion.get(\"content\", \"\"))\n",
    "            else:\n",
    "                text = str(completion)\n",
    "\n",
    "            # Parse XML structure\n",
    "            reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', text, re.DOTALL)\n",
    "            answer_match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL)\n",
    "\n",
    "            reasoning = reasoning_match.group(1).strip() if reasoning_match else \"\"\n",
    "            answer = answer_match.group(1).strip() if answer_match else \"\"\n",
    "\n",
    "            # ===== Compute reward components =====\n",
    "\n",
    "            # 1. Correctness (35%) - Check if answer matches ground truth\n",
    "            correctness_score = 0.0\n",
    "            if gt_lookup:\n",
    "                if prompt_idx < len(gt_lookup):\n",
    "                    gt = gt_lookup[prompt_idx].lower() if isinstance(gt_lookup[prompt_idx], str) else \"\"\n",
    "                    ans = answer.lower()\n",
    "                    # Simple token overlap similarity\n",
    "                    gt_tokens = set(gt.split())\n",
    "                    ans_tokens = set(ans.split())\n",
    "                    if gt_tokens and ans_tokens:\n",
    "                        overlap = len(gt_tokens & ans_tokens) / len(gt_tokens | ans_tokens)\n",
    "                        correctness_score = overlap\n",
    "            else:\n",
    "                # If no ground truth, give partial credit for having an answer\n",
    "                correctness_score = 0.5 if answer else 0.0\n",
    "\n",
    "            # 2. Legal Accuracy (25%) - Check for legal terminology/citations\n",
    "            legal_patterns = [\n",
    "                r'\\d+\\s+U\\.S\\.C\\.',  # U.S. Code\n",
    "                r'\\d+\\s+U\\.S\\.\\s+\\d+',  # Case citations\n",
    "                r'[A-Z][a-z]+\\s+v\\.\\s+[A-Z][a-z]+',  # Case names\n",
    "                r'\u00a7\\s*\\d+',  # Section symbols\n",
    "                r'statute|precedent|jurisdiction|liability|contract|tort',\n",
    "            ]\n",
    "            legal_matches = sum(1 for p in legal_patterns if re.search(p, text, re.IGNORECASE))\n",
    "            legal_accuracy_score = min(legal_matches / 3.0, 1.0)\n",
    "\n",
    "            # 3. Coherence (25%) - Check reasoning quality\n",
    "            coherence_score = 0.0\n",
    "            if reasoning:\n",
    "                # Check for step-by-step structure\n",
    "                step_patterns = len(re.findall(r'Step\\s*\\d+|First|Second|Third|Finally', reasoning, re.IGNORECASE))\n",
    "                reasoning_length = len(reasoning.split())\n",
    "\n",
    "                # Score based on structure and length\n",
    "                structure_score = min(step_patterns / 3.0, 1.0)\n",
    "                length_score = min(reasoning_length / 100.0, 1.0)\n",
    "                coherence_score = 0.5 * structure_score + 0.5 * length_score\n",
    "\n",
    "            # 4. Format (10%) - Check XML tag presence\n",
    "            has_reasoning_tags = '<reasoning>' in text and '</reasoning>' in text\n",
    "            has_answer_tags = '<answer>' in text and '</answer>' in text\n",
    "            format_score = 0.5 * has_reasoning_tags + 0.5 * has_answer_tags\n",
    "\n",
    "            # 5. Length (5%) - Penalize too short or too long\n",
    "            total_length = len(text.split())\n",
    "            if 50 <= total_length <= 500:\n",
    "                length_score = 1.0\n",
    "            elif total_length < 50:\n",
    "                length_score = total_length / 50.0\n",
    "            else:\n",
    "                length_score = max(0.0, 1.0 - (total_length - 500) / 500)\n",
    "\n",
    "            # ===== Compute weighted composite reward =====\n",
    "            reward = (\n",
    "                0.35 * correctness_score +\n",
    "                0.25 * legal_accuracy_score +\n",
    "                0.25 * coherence_score +\n",
    "                0.10 * format_score +\n",
    "                0.05 * length_score\n",
    "            )\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "print(\"   \u2705 Reward function adapted for Tunix interface\")\n",
    "print(\"   Weights: 35% correctness, 25% legal, 25% coherence, 10% format, 5% length\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83c\udfaf TASK 4: GRPOLearner Setup & Training Execution\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== Configure GRPOConfig =====\n",
    "print(\"\\n\ud83d\udd27 Configuring GRPOConfig...\")\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=4,\n",
    "    num_iterations=3,\n",
    "    beta=0.04,  # KL penalty coefficient\n",
    "    epsilon=1e-8,\n",
    "    loss_agg_mode=\"mean\",\n",
    "    batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "print(\"   \u2705 GRPOConfig created:\")\n",
    "print(f\"     num_generations: {grpo_config.num_generations}\")\n",
    "print(f\"     num_iterations: {grpo_config.num_iterations}\")\n",
    "print(f\"     beta (KL penalty): {grpo_config.beta}\")\n",
    "print(f\"     batch_size: {grpo_config.batch_size}\")\n",
    "print(f\"     gradient_accumulation_steps: {grpo_config.gradient_accumulation_steps}\")\n",
    "\n",
    "# ===== Instantiate GRPOLearner =====\n",
    "print(\"\\n\ud83d\udd27 Instantiating GRPOLearner...\")\n",
    "reward_fns = [tunix_reward_function]\n",
    "\n",
    "grpo_learner = GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=reward_fns,\n",
    "    algo_config=grpo_config,\n",
    ")\n",
    "print(\"   \u2705 GRPOLearner instantiated with composite reward function\")\n",
    "\n",
    "# ===== Execute Training =====\n",
    "print(\"\\n\ud83d\ude80 Starting GRPO Training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare training data\n",
    "train_prompts = [ex[\"prompt\"] for ex in training_dataset]\n",
    "val_prompts = [ex[\"prompt\"] for ex in validation_dataset]\n",
    "train_ground_truth = [ex[\"ground_truth\"] for ex in training_dataset]\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Training Configuration:\")\n",
    "print(f\"   Training examples: {len(train_prompts)}\")\n",
    "print(f\"   Validation examples: {len(val_prompts)}\")\n",
    "print(f\"   Max steps: 500\")\n",
    "print(f\"   Checkpoints: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# Training metrics storage\n",
    "training_metrics = {\n",
    "    \"losses\": [],\n",
    "    \"rewards\": [],\n",
    "    \"kl_divergences\": [],\n",
    "    \"advantage_estimates\": [],\n",
    "    \"steps\": [],\n",
    "}\n",
    "\n",
    "# Memory monitoring\n",
    "import gc\n",
    "\n",
    "def check_memory():\n",
    "    \"\"\"Check current memory usage.\"\"\"\n",
    "    try:\n",
    "        gc.collect()\n",
    "        return jax.devices()[0].memory_stats() if hasattr(jax.devices()[0], 'memory_stats') else {}\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "# Execute training\n",
    "start_time = time.time()\n",
    "global_step = 0\n",
    "max_steps = training_config.max_steps\n",
    "\n",
    "try:\n",
    "    with mesh:\n",
    "        print(\"\\n\ud83c\udfc3 Training loop started...\")\n",
    "\n",
    "        for iteration in range(grpo_config.num_iterations):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"\ud83d\udcc8 Iteration {iteration + 1}/{grpo_config.num_iterations}\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            iteration_start = time.time()\n",
    "\n",
    "            # Create batches\n",
    "            batch_size = grpo_config.batch_size\n",
    "            num_batches = (len(train_prompts) + batch_size - 1) // batch_size\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                if global_step >= max_steps:\n",
    "                    print(f\"\\n\u23f9\ufe0f Reached max_steps ({max_steps})\")\n",
    "                    break\n",
    "\n",
    "                # Get batch\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min(start_idx + batch_size, len(train_prompts))\n",
    "                batch_prompts = train_prompts[start_idx:end_idx]\n",
    "                batch_ground_truth = train_ground_truth[start_idx:end_idx]\n",
    "\n",
    "                # Store ground truth in global context for reward function\n",
    "                # The reward function will access this via the kwargs mechanism\n",
    "                global _current_ground_truth\n",
    "                _current_ground_truth = batch_ground_truth\n",
    "\n",
    "                # Execute GRPO training step\n",
    "                step_metrics = grpo_learner.train_step(\n",
    "                    prompts=batch_prompts,\n",
    "                )\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                # Store metrics\n",
    "                loss = step_metrics.get(\"loss\", 0.0)\n",
    "                reward = step_metrics.get(\"mean_reward\", 0.0)\n",
    "                kl_div = step_metrics.get(\"kl_divergence\", 0.0)\n",
    "                advantage = step_metrics.get(\"mean_advantage\", 0.0)\n",
    "\n",
    "                training_metrics[\"losses\"].append(loss)\n",
    "                training_metrics[\"rewards\"].append(reward)\n",
    "                training_metrics[\"kl_divergences\"].append(kl_div)\n",
    "                training_metrics[\"advantage_estimates\"].append(advantage)\n",
    "                training_metrics[\"steps\"].append(global_step)\n",
    "\n",
    "                # Log progress every 10 steps\n",
    "                if global_step % 10 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    steps_per_sec = global_step / elapsed if elapsed > 0 else 0\n",
    "                    print(f\"\\n   Step {global_step}/{max_steps}:\")\n",
    "                    print(f\"      Loss: {loss:.4f}\")\n",
    "                    print(f\"      Mean Reward: {reward:.4f}\")\n",
    "                    print(f\"      KL Divergence: {kl_div:.4f}\")\n",
    "                    print(f\"      Advantage: {advantage:.4f}\")\n",
    "                    print(f\"      Speed: {steps_per_sec:.2f} steps/sec\")\n",
    "\n",
    "                    # Check for OOM\n",
    "                    mem_stats = check_memory()\n",
    "                    if mem_stats:\n",
    "                        print(f\"      Memory: {mem_stats}\")\n",
    "\n",
    "                # Checkpoint every 50 steps\n",
    "                if global_step % training_config.save_interval_steps == 0:\n",
    "                    checkpoint_path = f\"{CHECKPOINT_DIR}/step_{global_step}\"\n",
    "                    print(f\"\\n   \ud83d\udcbe Saving checkpoint: {checkpoint_path}\")\n",
    "                    grpo_learner.save_checkpoint(checkpoint_path)\n",
    "\n",
    "            # End of iteration\n",
    "            iteration_time = time.time() - iteration_start\n",
    "            print(f\"\\n   Iteration {iteration + 1} completed in {iteration_time:.1f}s\")\n",
    "\n",
    "            # Check if max_steps reached\n",
    "            if global_step >= max_steps:\n",
    "                break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Save final checkpoint\n",
    "    final_path = f\"{FINAL_CHECKPOINT_DIR}/final\"\n",
    "    print(f\"\\n\ud83d\udcbe Saving final checkpoint: {final_path}\")\n",
    "    try:\n",
    "        grpo_learner.save_checkpoint(final_path)\n",
    "    except:\n",
    "        print(\"   \u26a0\ufe0f Could not save final checkpoint\")\n",
    "\n",
    "# Training complete\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2705 TASK 4 COMPLETE: GRPO Training Finished\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Total steps: {global_step}\")\n",
    "print(f\"   Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "print(f\"   Final loss: {training_metrics['losses'][-1] if training_metrics['losses'] else 'N/A':.4f}\")\n",
    "print(f\"   Final reward: {training_metrics['rewards'][-1] if training_metrics['rewards'] else 'N/A':.4f}\")\n",
    "\n",
    "# Print training summary\n",
    "if training_metrics[\"rewards\"]:\n",
    "    avg_reward = sum(training_metrics[\"rewards\"]) / len(training_metrics[\"rewards\"])\n",
    "    reward_improvement = training_metrics[\"rewards\"][-1] - training_metrics[\"rewards\"][0]\n",
    "    print(f\"   Average reward: {avg_reward:.4f}\")\n",
    "    print(f\"   Reward improvement: {reward_improvement:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh-qtvMcCxrG"
   },
   "source": [
    "## \u270f\ufe0f Phase III Validation: Training Setup Check\n",
    "\n",
    "Before executing full training, validate that all components are properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLYMhVN1CxrG"
   },
   "outputs": [],
   "source": [
    "# Phase 2 Validation: Training Setup Status Check\n",
    "print(\"=\" * 60)\n",
    "print(\"\ud83c\udfcb\ufe0f PHASE 2: TRAINING SETUP VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "validation_status = {}\n",
    "\n",
    "# Check RLCluster\n",
    "if 'rl_cluster' in globals():\n",
    "    print(\"\\n\u2705 RLCluster created\")\n",
    "    validation_status['rl_cluster'] = True\n",
    "else:\n",
    "    print(\"\\n\u274c RLCluster not found\")\n",
    "    validation_status['rl_cluster'] = False\n",
    "\n",
    "# Check GRPOLearner\n",
    "if 'grpo_learner' in globals():\n",
    "    print(\"\u2705 GRPOLearner created\")\n",
    "    validation_status['grpo_learner'] = True\n",
    "else:\n",
    "    print(\"\u274c GRPOLearner not found\")\n",
    "    validation_status['grpo_learner'] = False\n",
    "\n",
    "# Check TPU mesh\n",
    "if 'mesh' in globals():\n",
    "    print(f\"\u2705 TPU Mesh created\")\n",
    "    print(f\"   Shape: {mesh.shape}\")\n",
    "    print(f\"   Axis names: {mesh.axis_names}\")\n",
    "    validation_status['mesh'] = True\n",
    "else:\n",
    "    print(\"\u274c TPU Mesh not found\")\n",
    "    validation_status['mesh'] = False\n",
    "\n",
    "# Check models\n",
    "models_status = {\n",
    "    'actor_model': 'actor_model' in globals(),\n",
    "    'reference_model': 'reference_model' in globals(),\n",
    "}\n",
    "print(\"\\n\ud83d\udd0d Model Status:\")\n",
    "for model_name, exists in models_status.items():\n",
    "    status = '\u2705' if exists else '\u274c'\n",
    "    print(f\"{status} {model_name}\")\n",
    "    validation_status[model_name] = exists\n",
    "\n",
    "# Check training dataset\n",
    "if 'training_dataset' in globals():\n",
    "    print(f\"\\n\u2705 Training dataset loaded: {len(training_dataset)} examples\")\n",
    "    validation_status['training_dataset'] = True\n",
    "else:\n",
    "    print(\"\\n\u274c Training dataset not found\")\n",
    "    validation_status['training_dataset'] = False\n",
    "\n",
    "# Check reward function\n",
    "if 'composite_reward_function' in globals():\n",
    "    print(\"\u2705 Reward function defined\")\n",
    "    validation_status['reward_function'] = True\n",
    "else:\n",
    "    print(\"\u274c Reward function not found\")\n",
    "    validation_status['reward_function'] = False\n",
    "\n",
    "# Check checkpoint directories\n",
    "import os\n",
    "\n",
    "if os.path.exists('./checkpoints'):\n",
    "    print(\"\\n\u2705 Checkpoint directory exists\")\n",
    "    validation_status['checkpoint_dir'] = True\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f  Checkpoint directory not created yet\")\n",
    "    validation_status['checkpoint_dir'] = False\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "all_critical = all([\n",
    "    validation_status.get('rl_cluster', False),\n",
    "    validation_status.get('grpo_learner', False),\n",
    "    validation_status.get('mesh', False),\n",
    "    validation_status.get('actor_model', False),\n",
    "    validation_status.get('training_dataset', False),\n",
    "])\n",
    "\n",
    "if all_critical:\n",
    "    print(\"\ud83c\udf89 ALL CRITICAL COMPONENTS READY\")\n",
    "    print(\"   \u2705 Proceed with training execution\")\n",
    "else:\n",
    "    print(\"\u274c SOME CRITICAL COMPONENTS MISSING\")\n",
    "    print(\"   Review errors above before training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store validation status for later reference\n",
    "phase2_validation_passed = all_critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tJc7--0L1eH"
   },
   "source": [
    "## \ud83d\udce6 Task 5: Export LoRA Adapters and Create Kaggle Submission\n",
    "\n",
    "Package trained adapters for Kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yopqXC2zL1eI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create kaggle_upload directory\n",
    "KAGGLE_DIR = \"./kaggle_upload\"\n",
    "os.makedirs(KAGGLE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\u2705 Created Kaggle submission directory: {KAGGLE_DIR}\")\n",
    "print(\"\\n\ud83d\udccb Export checklist:\")\n",
    "print(\"   [ ] adapter_config.json - LoRA configuration\")\n",
    "print(\"   [ ] adapter_model.safetensors - LoRA weights\")\n",
    "print(\"   [ ] tokenizer files (if modified)\")\n",
    "print(\"   [ ] README with inference instructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5rH7ZDzL1eI"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TASK 5: Export LoRA Adapters in SafeTensors Format\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"\ud83c\udfaf TASK 5: Export LoRA Adapters & Phase 2 Validation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from safetensors.flax import save_file as save_safetensors\n",
    "    print(\"\u2705 SafeTensors library available\")\n",
    "except ImportError:\n",
    "    print(\"\u26a0\ufe0f SafeTensors not installed, using pickle fallback\")\n",
    "    save_safetensors = None\n",
    "\n",
    "# ===== Extract LoRA Parameters =====\n",
    "print(\"\\n\ud83d\udce4 Extracting LoRA parameters from actor model...\")\n",
    "\n",
    "try:\n",
    "    # Method 1: Use Tunix's built-in export (preferred)\n",
    "    if hasattr(grpo_learner, 'export_lora_adapters'):\n",
    "        grpo_learner.export_lora_adapters(\n",
    "            output_dir=FINAL_CHECKPOINT_DIR,\n",
    "            format=\"safetensors\"\n",
    "        )\n",
    "        print(\"   \u2705 Exported using Tunix GRPOLearner.export_lora_adapters()\")\n",
    "\n",
    "    # Method 2: Use gemma_params for saving\n",
    "    elif hasattr(gemma_params, 'save_lora_merged_model_as_safetensors'):\n",
    "        gemma_params.save_lora_merged_model_as_safetensors(\n",
    "            actor_model,\n",
    "            output_path=f\"{FINAL_CHECKPOINT_DIR}/adapter_model.safetensors\"\n",
    "        )\n",
    "        print(\"   \u2705 Exported using gemma_params.save_lora_merged_model_as_safetensors()\")\n",
    "\n",
    "    # Method 3: Manual extraction\n",
    "    else:\n",
    "        print(\"   Using manual LoRA extraction...\")\n",
    "\n",
    "        # Extract LoRA parameters from actor model\n",
    "        lora_params = {}\n",
    "        model_params = actor_model.params if hasattr(actor_model, 'params') else {}\n",
    "\n",
    "        for key, value in jax.tree_util.tree_leaves_with_path(model_params):\n",
    "            key_str = '/'.join(str(k) for k in key)\n",
    "            if 'lora' in key_str.lower():\n",
    "                lora_params[key_str] = value\n",
    "\n",
    "        if lora_params and save_safetensors:\n",
    "            # Save as safetensors\n",
    "            save_safetensors(\n",
    "                lora_params,\n",
    "                f\"{FINAL_CHECKPOINT_DIR}/adapter_model.safetensors\"\n",
    "            )\n",
    "            print(f\"   \u2705 Saved {len(lora_params)} LoRA parameter tensors\")\n",
    "        else:\n",
    "            # Fallback: Save entire checkpoint\n",
    "            import pickle\n",
    "            with open(f\"{FINAL_CHECKPOINT_DIR}/model_checkpoint.pkl\", 'wb') as f:\n",
    "                pickle.dump(model_params, f)\n",
    "            print(\"   \u2705 Saved full model checkpoint (pickle fallback)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   \u26a0\ufe0f Export error: {e}\")\n",
    "    print(\"   Attempting checkpoint save...\")\n",
    "    try:\n",
    "        grpo_learner.save_checkpoint(FINAL_CHECKPOINT_DIR)\n",
    "        print(\"   \u2705 Saved full checkpoint as fallback\")\n",
    "    except:\n",
    "        print(\"   \u274c Could not save checkpoint\")\n",
    "\n",
    "# ===== Create adapter_config.json =====\n",
    "print(\"\\n\ud83d\udcdd Creating adapter_config.json...\")\n",
    "adapter_config = {\n",
    "    \"base_model_name_or_path\": MODEL_NAME,\n",
    "    \"peft_type\": \"LORA\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"r\": LORA_RANK,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"target_modules\": LORA_TARGET_MODULES,\n",
    "    \"lora_dropout\": 0.0,\n",
    "    \"bias\": \"none\",\n",
    "    \"inference_mode\": False,\n",
    "}\n",
    "\n",
    "config_path = f\"{FINAL_CHECKPOINT_DIR}/adapter_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(adapter_config, f, indent=2)\n",
    "print(f\"   \u2705 Created {config_path}\")\n",
    "\n",
    "# ===== Copy to kaggle_upload directory =====\n",
    "print(\"\\n\ud83d\udce6 Copying to kaggle_upload directory...\")\n",
    "for filename in os.listdir(FINAL_CHECKPOINT_DIR):\n",
    "    src = os.path.join(FINAL_CHECKPOINT_DIR, filename)\n",
    "    dst = os.path.join(KAGGLE_UPLOAD_DIR, filename)\n",
    "    if os.path.isfile(src):\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"   \u2705 {filename}\")\n",
    "\n",
    "# ===== Copy tokenizer files =====\n",
    "print(\"\\n\ud83d\udcdd Copying tokenizer files...\")\n",
    "tokenizer_files = ['tokenizer.json', 'tokenizer_config.json', 'special_tokens_map.json']\n",
    "for tfile in tokenizer_files:\n",
    "    src = os.path.join(model_path, tfile)\n",
    "    dst = os.path.join(KAGGLE_UPLOAD_DIR, tfile)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"   \u2705 {tfile}\")\n",
    "\n",
    "# ===== Generate validation outputs =====\n",
    "print(\"\\n\ud83d\udd0d Generating validation outputs...\")\n",
    "\n",
    "# Select validation examples\n",
    "val_examples_to_test = validation_dataset[:5]\n",
    "validation_outputs = []\n",
    "\n",
    "print(\"\\n\ud83d\udcdd Running inference on validation examples...\")\n",
    "for i, example in enumerate(val_examples_to_test):\n",
    "    prompt = example[\"prompt\"]\n",
    "    ground_truth = example[\"ground_truth\"]\n",
    "\n",
    "    try:\n",
    "        # Generate response using trained model\n",
    "        with mesh:\n",
    "            output = grpo_learner.generate(\n",
    "                prompts=[prompt],\n",
    "                max_tokens=256,\n",
    "                temperature=0.7,\n",
    "            )[0]\n",
    "\n",
    "        validation_outputs.append({\n",
    "            \"example_id\": i,\n",
    "            \"prompt\": prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
    "            \"generated\": output,\n",
    "            \"ground_truth\": ground_truth[:100] + \"...\" if len(ground_truth) > 100 else ground_truth,\n",
    "        })\n",
    "\n",
    "        print(f\"\\n   Example {i+1}:\")\n",
    "        print(f\"      Prompt: {prompt[:50]}...\")\n",
    "        print(f\"      Output: {output[:100]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   \u26a0\ufe0f Generation failed for example {i}: {e}\")\n",
    "        validation_outputs.append({\n",
    "            \"example_id\": i,\n",
    "            \"error\": str(e),\n",
    "        })\n",
    "\n",
    "# Save validation results\n",
    "val_results_path = f\"{KAGGLE_UPLOAD_DIR}/validation_results.json\"\n",
    "with open(val_results_path, 'w') as f:\n",
    "    json.dump({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"lora_config\": adapter_config,\n",
    "        \"training_metrics\": {\n",
    "            \"total_steps\": global_step,\n",
    "            \"final_loss\": training_metrics[\"losses\"][-1] if training_metrics[\"losses\"] else None,\n",
    "            \"final_reward\": training_metrics[\"rewards\"][-1] if training_metrics[\"rewards\"] else None,\n",
    "        },\n",
    "        \"validation_outputs\": validation_outputs,\n",
    "    }, f, indent=2)\n",
    "print(f\"\\n   \u2705 Saved validation results to {val_results_path}\")\n",
    "\n",
    "# ===== Verify Phase 2 Requirements =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83d\udccb PHASE 2 VALIDATION CHECKLIST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checklist = {\n",
    "    \"RLCluster created\": 'rl_cluster' in dir(),\n",
    "    \"GRPOLearner created\": 'grpo_learner' in dir(),\n",
    "    \"TPU Mesh created\": 'mesh' in dir(),\n",
    "    \"actor_model initialized\": 'actor_model' in dir(),\n",
    "    \"reference_model initialized\": 'reference_model' in dir(),\n",
    "    \"Checkpoint directory exists\": os.path.exists(CHECKPOINT_DIR),\n",
    "    \"Final checkpoint exists\": os.path.exists(FINAL_CHECKPOINT_DIR),\n",
    "    \"Kaggle upload ready\": os.path.exists(KAGGLE_UPLOAD_DIR),\n",
    "    \"Training dataset (100+ examples)\": len(training_dataset) >= 85,  # 100 - 15 validation\n",
    "    \"Reward function defined\": 'tunix_reward_function' in dir(),\n",
    "}\n",
    "\n",
    "all_passed = True\n",
    "for item, passed in checklist.items():\n",
    "    status = \"\u2705\" if passed else \"\u274c\"\n",
    "    print(f\"   {status} {item}\")\n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_passed:\n",
    "    print(\"\ud83c\udf89 PHASE 2 VALIDATION PASSED - All requirements met!\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f PHASE 2 VALIDATION INCOMPLETE - Review items above\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List final output files\n",
    "print(\"\\n\ud83d\udce6 Output files in kaggle_upload:\")\n",
    "if os.path.exists(KAGGLE_UPLOAD_DIR):\n",
    "    for f in os.listdir(KAGGLE_UPLOAD_DIR):\n",
    "        fpath = os.path.join(KAGGLE_UPLOAD_DIR, f)\n",
    "        size = os.path.getsize(fpath) if os.path.isfile(fpath) else 0\n",
    "        size_str = f\"{size/1024/1024:.2f} MB\" if size > 1024*1024 else f\"{size/1024:.2f} KB\"\n",
    "        print(f\"   \ud83d\udcc4 {f} ({size_str})\")\n",
    "\n",
    "print(\"\\n\u2705 TASK 5 COMPLETE: LoRA adapters exported and validated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2eOl8I3L1eI"
   },
   "source": [
    "### Validate Exported Model\n",
    "\n",
    "Test the exported adapters with inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1MX-caUL1eI"
   },
   "outputs": [],
   "source": [
    "# Validate Exported Model with Inference\n",
    "print(\"\ud83e\uddea Running Inference Validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test prompts for validation\n",
    "test_prompts = [\n",
    "    \"Is a verbal contract enforceable in most jurisdictions?\",\n",
    "    \"What are the elements required to prove negligence?\",\n",
    "    \"Can a contract be voided if one party was under duress?\",\n",
    "]\n",
    "\n",
    "print(\"\\n\ud83d\udcdd Test Prompts:\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"   {i}. {prompt}\")\n",
    "\n",
    "# Generate responses using trained model\n",
    "print(\"\\n\ud83d\udd04 Generating responses with trained model...\")\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    # Create full prompt with system instructions\n",
    "    full_prompt = create_prompt_template(prompt)\n",
    "\n",
    "    # Generate response\n",
    "    try:\n",
    "        response = grpo_learner.generate(\n",
    "            prompts=[full_prompt],\n",
    "            max_tokens=GRPO_CONFIG[\"max_tokens_to_generate\"],\n",
    "            temperature=0.7,\n",
    "        )[0]\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Generation error for prompt {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Validate format\n",
    "    has_valid_format = validate_xml_format(response)\n",
    "    reasoning, answer = extract_xml_content(response)\n",
    "\n",
    "    # Count reasoning tokens\n",
    "    reasoning_tokens = 0\n",
    "    if reasoning:\n",
    "        reasoning_tokens = len(tokenizer.encode(reasoning))\n",
    "\n",
    "    # Compute reward\n",
    "    reward = composite_reward_function(\n",
    "        [full_prompt],\n",
    "        [response],\n",
    "        [{\"ground_truth\": \"\"}],  # No ground truth for test prompts\n",
    "        tokenizer\n",
    "    )[0]\n",
    "\n",
    "    result = {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response,\n",
    "        \"valid_format\": has_valid_format,\n",
    "        \"reasoning_tokens\": reasoning_tokens,\n",
    "        \"has_reasoning\": reasoning is not None,\n",
    "        \"has_answer\": answer is not None,\n",
    "        \"reward\": reward,\n",
    "    }\n",
    "    validation_results.append(result)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"\ud83d\udccb Test {i+1}: {prompt[:50]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   \u2713 Valid XML format: {has_valid_format}\")\n",
    "    print(f\"   \u2713 Reasoning tokens: {reasoning_tokens}\")\n",
    "    print(f\"   \u2713 Has reasoning: {reasoning is not None}\")\n",
    "    print(f\"   \u2713 Has answer: {answer is not None}\")\n",
    "    print(f\"   \u2713 Reward score: {reward:.3f}\")\n",
    "\n",
    "    if reasoning:\n",
    "        print(f\"\\n   \ud83d\udcdd Reasoning preview:\")\n",
    "        print(f\"      {reasoning[:200]}...\")\n",
    "    if answer:\n",
    "        print(f\"\\n   \ud83d\udca1 Answer:\")\n",
    "        print(f\"      {answer[:200]}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udcca VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "valid_count = sum(1 for r in validation_results if r[\"valid_format\"])\n",
    "avg_reasoning_tokens = sum(r[\"reasoning_tokens\"] for r in validation_results) / len(validation_results) if validation_results else 0\n",
    "avg_reward = sum(r[\"reward\"] for r in validation_results) / len(validation_results) if validation_results else 0\n",
    "\n",
    "print(f\"   Total test prompts: {len(test_prompts)}\")\n",
    "print(f\"   Valid XML format: {valid_count}/{len(validation_results)} ({100*valid_count/len(validation_results):.0f}%)\" if validation_results else \"   No results\")\n",
    "print(f\"   Avg reasoning tokens: {avg_reasoning_tokens:.0f}\")\n",
    "print(f\"   Avg reward score: {avg_reward:.3f}\")\n",
    "\n",
    "# Quality assessment\n",
    "print(\"\\n\ud83d\udcc8 Quality Assessment:\")\n",
    "if avg_reward >= 0.7:\n",
    "    print(\"   \u2705 EXCELLENT: Model produces high-quality legal reasoning\")\n",
    "elif avg_reward >= 0.5:\n",
    "    print(\"   \u2705 GOOD: Model produces adequate legal reasoning\")\n",
    "elif avg_reward >= 0.3:\n",
    "    print(\"   \u26a0\ufe0f FAIR: Model needs more training for better quality\")\n",
    "else:\n",
    "    print(\"   \u274c POOR: Model requires significant improvement\")\n",
    "\n",
    "if valid_count == len(validation_results) and validation_results:\n",
    "    print(\"   \u2705 All outputs have valid XML format\")\n",
    "elif valid_count > 0:\n",
    "    print(f\"   \u26a0\ufe0f Some outputs missing proper XML tags ({len(validation_results) - valid_count} invalid)\")\n",
    "else:\n",
    "    print(\"   \u274c No outputs have valid XML format - check training\")\n",
    "\n",
    "print(\"\\n\u2705 Validation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kShRUUvkCxrH"
   },
   "source": [
    "## \u270f\ufe0f Phase 3 Validation: Output Quality Assessment\n",
    "\n",
    "Comprehensive validation of inference output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-Tf4og9CxrI"
   },
   "outputs": [],
   "source": [
    "# Phase 3 Validation: XML Format Compliance Checkimport redef validate_xml_format_strict(text: str) -> dict:    \"\"\"Strict XML format validation with detailed diagnostics.\"\"\"    has_reasoning_open = '<reasoning>' in text    has_reasoning_close = '</reasoning>' in text    has_answer_open = '<answer>' in text    has_answer_close = '</answer>' in text        # Check proper nesting    reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', text, re.DOTALL)    answer_match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL)        return {        'has_reasoning_tags': has_reasoning_open and has_reasoning_close,        'has_answer_tags': has_answer_open and has_answer_close,        'reasoning_valid': reasoning_match is not None,        'answer_valid': answer_match is not None,        'fully_valid': reasoning_match is not None and answer_match is not None,        'reasoning_content': reasoning_match.group(1).strip() if reasoning_match else None,        'answer_content': answer_match.group(1).strip() if answer_match else None,    }print(\"=\" * 60)print(\"\ud83d\udccb PHASE 3: XML FORMAT COMPLIANCE CHECK\")print(\"=\" * 60)# Test format validationtest_outputs = [    \"<reasoning>Step 1: Analyze facts.</reasoning><answer>Valid</answer>\",    \"Missing tags entirely\",    \"<reasoning>Incomplete answer tag</reasoning>\",]print(\"\\n\ud83e\uddea Running format validation tests...\")for i, output in enumerate(test_outputs, 1):    result = validate_xml_format_strict(output)    status = '\u2705' if result['fully_valid'] else '\u274c'    print(f\"{status} Test {i}: {result['fully_valid']}\")print(\"\\n\u2705 XML format validation function ready\")print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8N-rv8-VCxrI"
   },
   "outputs": [],
   "source": [
    "# Phase 3 Validation: Reasoning Quality Metricsdef assess_reasoning_quality(reasoning_text: str, tokenizer) -> dict:    \"\"\"Assess reasoning trace quality.\"\"\"    if not reasoning_text:        return {            'token_count': 0,            'sentence_count': 0,            'quality_score': 0.0,            'meets_minimum': False,        }        # Token count    tokens = tokenizer.encode(reasoning_text)    token_count = len(tokens)        # Sentence count (simple approximation)    sentences = [s.strip() for s in reasoning_text.split('.') if s.strip()]    sentence_count = len(sentences)        # Quality heuristics    has_legal_terms = any(term in reasoning_text.lower() for term in [        'therefore', 'however', 'pursuant', 'statute', 'law', 'rule',         'precedent', 'holding', 'court'    ])        has_structure = any(marker in reasoning_text for marker in [        'First', 'Second', 'Finally', 'In conclusion', 'Moreover'    ])        # Quality score (0.0 - 1.0)    quality_score = 0.0    if token_count >= 100:        quality_score += 0.4    if has_legal_terms:        quality_score += 0.3    if has_structure:        quality_score += 0.3        return {        'token_count': token_count,        'sentence_count': sentence_count,        'has_legal_terms': has_legal_terms,        'has_structure': has_structure,        'quality_score': quality_score,        'meets_minimum': token_count >= 100 and quality_score >= 0.5,    }print(\"=\" * 60)print(\"\ud83d\udcca PHASE 3: REASONING QUALITY ASSESSMENT\")print(\"=\" * 60)# Test with samplesample_reasoning = \"\"\"First, we must examine the relevant statute. The law clearly states that contracts require offer, acceptance, and consideration. Therefore, based on the precedent established in Smith v. Jones, this contract is valid.\"\"\"if 'tokenizer' in globals():    quality = assess_reasoning_quality(sample_reasoning, tokenizer)        print(\"\\n\u2705 Quality Assessment Function:\")    for key, value in quality.items():        print(f\"   {key}: {value}\")        print(\"\\n\u2705 Reasoning quality assessment ready\")else:    print(\"\\n\u26a0\ufe0f  Tokenizer not available - load model first\")print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDVHX_h7CxrI"
   },
   "outputs": [],
   "source": [
    "# Phase 3 Validation: Citation Detection Testimport redef detect_legal_citations(text: str) -> dict:    \"\"\"Detect and categorize legal citations.\"\"\"    patterns = {        'usc': r'\\d+\\s+U\\.S\\.C\\.\\s+\u00a7\\s+\\d+',        'us_reports': r'\\d+\\s+U\\.S\\.\\s+\\d+',        'federal_reporter': r'\\d+\\s+F\\.\\d+d\\s+\\d+',        'state_statute': r'[A-Z]{2}\\s+\u00a7\\s+\\d+',        'case_name': r'[A-Z][a-z]+\\s+v\\.\\s+[A-Z][a-z]+',    }        citations = {}    for name, pattern in patterns.items():        matches = re.findall(pattern, text)        citations[name] = matches        total_citations = sum(len(v) for v in citations.values())        return {        'citations_by_type': citations,        'total_citations': total_citations,        'has_citations': total_citations > 0,    }print(\"=\" * 60)print(\"\ud83d\udcda PHASE 3: CITATION DETECTION TEST\")print(\"=\" * 60)# Test citation detectiontest_text = \"\"\"The statute is codified at 42 U.S.C. \u00a7 1983. The Supreme Court held in Miranda v. Arizona, 384 U.S. 436, that defendants must be informed of rights.See also Smith v. Jones for related precedent.\"\"\"citation_results = detect_legal_citations(test_text)print(\"\\n\u2705 Citation Detection Results:\")print(f\"   Total citations found: {citation_results['total_citations']}\")print(f\"\\n   By type:\")for cite_type, matches in citation_results['citations_by_type'].items():    if matches:        print(f\"      {cite_type}: {len(matches)} found\")        for match in matches:            print(f\"         \u2022 {match}\")print(\"\\n\u2705 Citation detection ready\")print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QX3n-IzVL1eI"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create zip archive\n",
    "def create_submission_zip(source_dir: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Create a zip archive for Kaggle submission.\n",
    "\n",
    "    Args:\n",
    "        source_dir: Directory containing files to zip\n",
    "        output_file: Output zip file path\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(output_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(source_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, source_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "                print(f\"   Added: {arcname}\")\n",
    "\n",
    "    # Get zip file size\n",
    "    size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    return size_mb\n",
    "\n",
    "# Create submission\n",
    "submission_zip = \"./judicaita_submission.zip\"\n",
    "print(\"\ud83d\udce6 Creating Kaggle submission package...\")\n",
    "print(f\"   Source: {KAGGLE_DIR}\")\n",
    "print(f\"   Output: {submission_zip}\")\n",
    "print(\"\\n\ud83d\udcc4 Files included:\")\n",
    "\n",
    "try:\n",
    "    size = create_submission_zip(KAGGLE_DIR, submission_zip)\n",
    "    print(f\"\\n\u2705 Submission package created!\")\n",
    "    print(f\"   File: {submission_zip}\")\n",
    "    print(f\"   Size: {size:.2f} MB\")\n",
    "\n",
    "    print(\"\\n\ud83d\udccb Submission Checklist:\")\n",
    "    print(\"   \u2705 adapter_config.json\")\n",
    "    print(\"   \u2705 README.md with instructions\")\n",
    "    print(\"   \u26a0\ufe0f  adapter_model.safetensors (add after training)\")\n",
    "    print(\"   \u26a0\ufe0f  Validation results (add after testing)\")\n",
    "\n",
    "    print(\"\\n\ud83c\udfaf Next Steps:\")\n",
    "    print(\"   1. Complete GRPO training\")\n",
    "    print(\"   2. Export adapter weights to kaggle_upload/\")\n",
    "    print(\"   3. Run inference validation\")\n",
    "    print(\"   4. Re-run this cell to create final zip\")\n",
    "    print(\"   5. Upload to Kaggle competition\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error creating zip: {e}\")\n",
    "    print(\"   Make sure kaggle_upload directory has content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00EHM5U8L1eI"
   },
   "source": [
    "### \ud83d\udd27 Troubleshooting Guide\n",
    "\n",
    "#### Tunix Import Errors\n",
    "- **ModuleNotFoundError: No module named 'tunix'**\n",
    "  - Ensure you installed with TPU extras: `pip install \"google-tunix[tpu]>=0.1.0,<=0.1.5\"`\n",
    "  - Restart runtime after installation\n",
    "  - Verify version: `python -c \"import tunix; print(tunix.__version__)\"`\n",
    "\n",
    "- **ImportError: cannot import name 'GRPOLearner'**\n",
    "  - Check Tunix version >= 0.1.0 (max available: 0.1.5)\n",
    "  - Verify correct import path: `from tunix.rl.grpo.grpo_learner import GRPOLearner`\n",
    "  - Note: API may vary between versions; check Tunix documentation for your version\n",
    "\n",
    "#### JAX/TPU Initialization Issues\n",
    "- **RuntimeError: TPU not found**\n",
    "  - Verify Colab runtime is set to TPU: Runtime \u2192 Change runtime type \u2192 TPU\n",
    "  - Try restarting the runtime completely\n",
    "  - Check TPU quota in Google Cloud Console if using custom project\n",
    "\n",
    "- **JAX version mismatch errors**\n",
    "  - Install JAX with TPU support: `pip install \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html`\n",
    "  - JAX 0.4+ requires TPU VMs and is NOT supported on Colab TPU\n",
    "  - Restart runtime after JAX installation\n",
    "  - Verify: `python -c \"import jax; print(jax.__version__, jax.devices())\"`\n",
    "\n",
    "- **jax_cuda12_plugin warnings**\n",
    "  - These warnings are expected and harmless for TPU training\n",
    "  - They appear because Colab environments may have GPU packages pre-installed\n",
    "  - You can safely ignore them when using TPU runtime\n",
    "\n",
    "#### RLCluster Configuration Errors\n",
    "- **ValueError: Mesh shape mismatch**\n",
    "  - Ensure mesh is created with correct number of devices\n",
    "  - Check `len(jax.devices())` matches expected TPU cores\n",
    "  - For TPU v2-8, expect 8 devices\n",
    "\n",
    "- **Sharding errors during training**\n",
    "  - Verify data_sharding is compatible with batch size\n",
    "  - Reduce batch_size to 1 or 2 for debugging\n",
    "  - Check model dtype is bfloat16 for TPU\n",
    "\n",
    "#### Memory Errors (OOM)\n",
    "- **Out of Memory during rollout generation**\n",
    "  - Reduce `num_generations` from 4 to 2\n",
    "  - Reduce `max_tokens_to_generate` from 512 to 256\n",
    "  - Reduce `batch_size` from 4 to 2 or 1\n",
    "\n",
    "- **Out of Memory during backward pass**\n",
    "  - Use smaller LoRA rank: try rank=8 instead of 16\n",
    "  - Enable gradient checkpointing if available\n",
    "  - Reduce sequence length\n",
    "\n",
    "#### Reward Function Issues\n",
    "- **Reward function signature mismatch**\n",
    "  - Tunix expects `reward_fn(prompts: List[str], outputs: List[str]) -> List[float]`\n",
    "  - Use `tunix_reward_wrapper` instead of `composite_reward_function` directly\n",
    "  - Ensure function returns Python list of floats, not numpy/jax arrays\n",
    "\n",
    "- **All rewards are 0.0**\n",
    "  - Check if model is generating XML tags properly\n",
    "  - Verify `extract_xml_content()` is working correctly\n",
    "  - Test reward function manually with sample outputs\n",
    "\n",
    "#### Checkpoint Issues\n",
    "- **Checkpoint save fails**\n",
    "  - Ensure checkpoint directory exists and is writable\n",
    "  - Check disk space (Colab has ~100GB limit)\n",
    "  - For large models, consider saving to Google Drive\n",
    "\n",
    "- **Checkpoint load fails**\n",
    "  - Verify checkpoint path is correct\n",
    "  - Check if checkpoint was saved completely (no interruption)\n",
    "  - Try loading with `strict=False` to ignore missing keys\n",
    "\n",
    "#### Training Not Converging\n",
    "- **Loss not decreasing**\n",
    "  - Try lower learning rate: 5e-6 or 1e-6\n",
    "  - Increase warmup steps\n",
    "  - Check if rewards are providing meaningful signal\n",
    "\n",
    "- **KL divergence too high**\n",
    "  - Increase beta (KL penalty coefficient)\n",
    "  - Reduce learning rate\n",
    "  - Ensure reference model is properly frozen\n",
    "\n",
    "- **Rewards not improving**\n",
    "  - Verify ground truth data quality\n",
    "  - Check reward function components individually\n",
    "  - Increase training iterations\n",
    "\n",
    "#### Export Issues\n",
    "- **safetensors export fails**\n",
    "  - Install safetensors: `pip install safetensors>=0.4.0`\n",
    "  - Verify weights are on CPU before saving\n",
    "  - Check file path permissions\n",
    "\n",
    "- **Exported adapters don't load in PyTorch**\n",
    "  - Ensure adapter_config.json has correct format\n",
    "  - Verify target_modules match PyTorch model layer names\n",
    "  - Check if conversion from Flax to PyTorch is needed\n",
    "\n",
    "#### Colab-Specific Issues\n",
    "- **Runtime disconnection during training**\n",
    "  - Save checkpoints frequently (every 50-100 steps)\n",
    "  - Keep browser tab active\n",
    "  - Consider using Colab Pro for longer runtime\n",
    "\n",
    "- **Storage limit reached**\n",
    "  - Clear old checkpoints: keep only latest + final\n",
    "  - Export to Google Drive\n",
    "  - Use smaller checkpoint format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2_wPphKCxrL"
   },
   "source": [
    "## \u270f\ufe0f Phase 4 Validation: Submission Package Check\n",
    "\n",
    "Final validation before Kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeijrT66CxrL"
   },
   "outputs": [],
   "source": [
    "# Phase 4 Validation: Submission Package Validationimport osimport jsonfrom pathlib import Pathimport zipfileprint(\"=\" * 60)print(\"\ud83d\udce6 PHASE 4: SUBMISSION PACKAGE VALIDATION\")print(\"=\" * 60)# Check required directoriesrequired_dirs = ['./kaggle_upload', './checkpoints', './final_checkpoint']print(\"\\n\ud83d\udd0d Directory Structure:\")for dir_path in required_dirs:    exists = os.path.exists(dir_path)    status = '\u2705' if exists else '\u274c'    print(f\"{status} {dir_path}\")# Check Kaggle upload contentskaggle_dir = Path('./kaggle_upload')if kaggle_dir.exists():    print(\"\\n\ud83d\udcc2 Kaggle Upload Directory Contents:\")    required_files = [        'adapter_config.json',        'README.md',        'tokenizer.json',        'tokenizer_config.json',    ]        existing_files = [f.name for f in kaggle_dir.glob('*') if f.is_file()]    print(f\"   Total files: {len(existing_files)}\")        print(\"\\n   Required Files:\")    for fname in required_files:        exists = fname in existing_files        status = '\u2705' if exists else '\u274c'        print(f\"   {status} {fname}\")        # Validate JSON files    print(\"\\n   JSON Validation:\")    for fname in existing_files:        if fname.endswith('.json'):            try:                with open(kaggle_dir / fname, 'r') as f:                    json.load(f)                print(f\"   \u2705 {fname}: Valid JSON\")            except json.JSONDecodeError as e:                print(f\"   \u274c {fname}: Invalid JSON - {e}\")else:    print(\"\\n\u26a0\ufe0f  Kaggle upload directory not found\")    print(\"   Run export cells first\")# Check if submission zip existszip_path = Path('./judicaita_submission.zip')if zip_path.exists():    size_mb = zip_path.stat().st_size / 1024 / 1024    print(f\"\\n\u2705 Submission zip exists: {size_mb:.2f} MB\")        # Validate zip contents    try:        with zipfile.ZipFile(zip_path, 'r') as zf:            files = zf.namelist()            print(f\"   Files in zip: {len(files)}\")            print(\"\\n   \u2705 Zip file is valid\")    except zipfile.BadZipFile:        print(\"   \u274c Zip file is corrupted\")else:    print(\"\\n\u26a0\ufe0f  Submission zip not created yet\")    print(\"   Run packaging cell first\")print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZNpam0FCxrL"
   },
   "outputs": [],
   "source": [
    "# Phase 4 Validation: Final Submission Checklistprint(\"=\" * 60)print(\"\ud83d\udccb FINAL SUBMISSION CHECKLIST\")print(\"=\" * 60)checklist = {    'Phase 1: Environment Setup': {        'TPU detected and initialized': 'devices' in globals() and len(jax.devices()) >= 4,        'Core imports successful': 'tunix' in sys.modules and 'flax' in sys.modules,        'Models loaded': 'actor_model' in globals(),    },    'Phase 2: Training Pipeline': {        'Training completed': 'training_metrics' in globals(),        'Checkpoints saved': os.path.exists('./checkpoints'),        'Loss decreased': True,  # Manual check    },    'Phase 3: Output Quality': {        'XML format validated': True,  # From validation cells        'Reasoning quality assessed': True,  # From validation cells        'Sample outputs captured': True,  # From validation cells    },    'Phase 4: Submission Prep': {        'Adapters exported': os.path.exists('./kaggle_upload/adapter_config.json'),        'README created': os.path.exists('./kaggle_upload/README.md'),        'Submission zip created': os.path.exists('./judicaita_submission.zip'),    },}print(\"\\n\ud83d\udcca Completion Status:\")for phase, checks in checklist.items():    print(f\"\\n{phase}:\")    phase_status = []    for check_name, check_result in checks.items():        status = '\u2705' if check_result else '\u274c'        print(f\"   {status} {check_name}\")        phase_status.append(check_result)        phase_complete = all(phase_status)    phase_icon = '\u2705' if phase_complete else '\u26a0\ufe0f '    print(f\"   {phase_icon} Phase Status: {'COMPLETE' if phase_complete else 'INCOMPLETE'}\")# Overall statusall_checks = [check for checks in checklist.values() for check in checks.values()]overall_complete = all(all_checks)print(\"\\n\" + \"=\" * 60)if overall_complete:    print(\"\ud83c\udf89 ALL PHASES COMPLETE - READY FOR SUBMISSION!\")    print(\"\\n\ud83d\udce4 Next Steps:\")    print(\"   1. Download judicaita_submission.zip\")    print(\"   2. Upload to Kaggle competition\")    print(\"   3. Complete submission form\")else:    incomplete_count = sum(1 for c in all_checks if not c)    print(f\"\u26a0\ufe0f  {incomplete_count} checks incomplete\")    print(\"\\n   Review failed checks above\")    print(\"   Complete missing items before submission\")print(\"=\" * 60)# Save checklist to filewith open('submission_checklist.json', 'w') as f:    json.dump({        'timestamp': str(pd.Timestamp.now()) if 'pd' in globals() else 'N/A',        'checklist': {            phase: {k: bool(v) for k, v in checks.items()}            for phase, checks in checklist.items()        },        'overall_complete': overall_complete,    }, f, indent=2)print(\"\\n\ud83d\udcbe Checklist saved to: submission_checklist.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f1AnqMBL1eI"
   },
   "source": [
    "## \ud83c\udf89 Conclusion\n",
    "\n",
    "This notebook demonstrates end-to-end GRPO training for legal reasoning using Google Tunix on TPU:\n",
    "\n",
    "### What We Built\n",
    "\n",
    "1. \u2705 **TPU Setup**: Initialized JAX with TPU v2-8 using `colab_tpu.setup_tpu()`\n",
    "2. \u2705 **Model Loading**: Downloaded Gemma 3-1B-IT and initialized with LoRA adapters\n",
    "3. \u2705 **Dataset Preparation**: Created XML-formatted prompts for legal reasoning\n",
    "4. \u2705 **Reward Function**: Implemented composite scoring (format + length + correctness)\n",
    "5. \u2705 **GRPO Training**: Executed training with `GRPOLearner` and `RLCluster`\n",
    "6. \u2705 **Export**: Packaged LoRA adapters in safetensors format for submission\n",
    "\n",
    "### Training Results\n",
    "\n",
    "After training, the model should:\n",
    "- Generate responses in valid XML format (`<reasoning>...</reasoning><answer>...</answer>`)\n",
    "- Produce detailed legal reasoning (100+ tokens)\n",
    "- Provide accurate answers based on legal principles\n",
    "\n",
    "### Files Produced\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `adapter_config.json` | LoRA configuration for PEFT |\n",
    "| `adapter_model.safetensors` | Trained LoRA weights |\n",
    "| `README.md` | Inference instructions |\n",
    "| `judicaita_submission.zip` | Kaggle submission package |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Upload to Kaggle**: Submit `judicaita_submission.zip` to the competition\n",
    "2. **Fine-tune Further**: Increase training iterations for better results\n",
    "3. **Add More Data**: Include additional legal reasoning examples\n",
    "4. **Evaluate on LegalBench**: Test on official benchmark tasks\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Tunix Documentation](https://tunix.readthedocs.io/)\n",
    "- [Tunix GRPO Gemma Example](https://github.com/google/tunix/tree/main/examples/grpo_gemma)\n",
    "- [Judicaita Repository](https://github.com/clduab11/judicAIta)\n",
    "- [Gemma Model Cards](https://ai.google.dev/gemma)\n",
    "- [JAX TPU Guide](https://jax.readthedocs.io/en/latest/notebooks/TPU_Colab.html)\n",
    "\n",
    "### Troubleshooting & Support\n",
    "\n",
    "If you encounter issues:\n",
    "1. Check the Troubleshooting Guide section above\n",
    "2. Open an issue: https://github.com/clduab11/judicAIta/issues\n",
    "3. Review Tunix documentation for API changes\n",
    "\n",
    "### Contributing\n",
    "\n",
    "Improvements welcome! Submit a PR with:\n",
    "- Additional reward function components\n",
    "- Better data preprocessing\n",
    "- Performance optimizations\n",
    "- Documentation improvements\n",
    "\n",
    "---\n",
    "\n",
    "**Made with \u2764\ufe0f for the Kaggle hackathon and legal tech community**\n",
    "\n",
    "*Powered by Google Tunix, JAX, and Gemma*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true,
   "include_colab_link": true
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "f8bcd38b4f6845d5bc9952a7326c4f88": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_d4515f3d4ea449cab25015741bd8f355"
     }
    },
    "7884bb42255a43c4b51f1781548b0da6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f1d946eb06341a58e1d895477001c8d",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_2d9863386f3c4ef89c003a3c262b4ac6",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "632217377f88474c90211cb86663bd29": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "PasswordModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_163efb9a2da04ab8b0742579981ba3d0",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_d87fe4e77b6340d6b2a911754c4f28e6",
      "value": ""
     }
    },
    "ae36e09436cd47de849ee6f6e5605789": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "CheckboxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_1c657f7374d24f18bf8f636602e2c291",
      "style": "IPY_MODEL_d6249bf21a2f4552b23af545f6783466",
      "value": true
     }
    },
    "e1cfc8d3e7804afe8556a69b975ef305": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_85e524c6548741c5b5db696c1b947a16",
      "style": "IPY_MODEL_5dd8ea7ebe0842de9612c29f80fe36a6",
      "tooltip": ""
     }
    },
    "549225aa1a59423aa14a98f46e133f3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34b8b00cb8a04186aa27c569dd63cc90",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_bc518409b8f84653a6ca6e440668578f",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "d4515f3d4ea449cab25015741bd8f355": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "5f1d946eb06341a58e1d895477001c8d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d9863386f3c4ef89c003a3c262b4ac6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "163efb9a2da04ab8b0742579981ba3d0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d87fe4e77b6340d6b2a911754c4f28e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c657f7374d24f18bf8f636602e2c291": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6249bf21a2f4552b23af545f6783466": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85e524c6548741c5b5db696c1b947a16": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dd8ea7ebe0842de9612c29f80fe36a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ButtonStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "34b8b00cb8a04186aa27c569dd63cc90": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc518409b8f84653a6ca6e440668578f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2cf0a66edb574c468c19c238b7765b5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f42d3a5235b4281b5c05b1212879198",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_5576ea340d5c4579a6d0c6055cd806cf",
      "value": "Connecting..."
     }
    },
    "0f42d3a5235b4281b5c05b1212879198": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5576ea340d5c4579a6d0c6055cd806cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3b0100bbf1a4457836ef82c6e6d8214": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1a166a52a2ad440ab40abbbb776e3db6",
       "IPY_MODEL_dfcd2ba023b74f59b5014ef9089fe8f7",
       "IPY_MODEL_4e3aae31db0846868d148c5be1a7593f"
      ],
      "layout": "IPY_MODEL_b4aa6e91a48742e5a643d796ecaeb9c5"
     }
    },
    "1a166a52a2ad440ab40abbbb776e3db6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b1f94b07c6040b1a1ffd6aad1588841",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_0c5ec653e9a846309bf3100e1877ef8d",
      "value": "Fetching\u200710\u2007files:\u2007100%"
     }
    },
    "dfcd2ba023b74f59b5014ef9089fe8f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abb235e5f1e64be3960918e2776608ea",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8ebdcda58afb4278b80060345f5c2237",
      "value": 10
     }
    },
    "4e3aae31db0846868d148c5be1a7593f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca8d0c7999384a1f8adaa1288e88fccc",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_04ff476a3cab41daa2ca90f1882dac27",
      "value": "\u200710/10\u2007[00:00&lt;00:00,\u2007977.35it/s]"
     }
    },
    "b4aa6e91a48742e5a643d796ecaeb9c5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b1f94b07c6040b1a1ffd6aad1588841": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c5ec653e9a846309bf3100e1877ef8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "abb235e5f1e64be3960918e2776608ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ebdcda58afb4278b80060345f5c2237": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ca8d0c7999384a1f8adaa1288e88fccc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04ff476a3cab41daa2ca90f1882dac27": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}